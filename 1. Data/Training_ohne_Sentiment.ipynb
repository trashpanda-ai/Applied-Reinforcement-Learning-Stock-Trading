{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMjwq6pS-kFz"
   },
   "source": [
    "# Stock NeurIPS2018 Part 2. Train\n",
    "This series is a reproduction of *the process in the paper Practical Deep Reinforcement Learning Approach for Stock Trading*. \n",
    "\n",
    "This is the second part of the NeurIPS2018 series, introducing how to use FinRL to make data into the gym form environment, and train DRL agents on it.\n",
    "\n",
    "Other demos can be found at the repo of [FinRL-Tutorials]((https://github.com/AI4Finance-Foundation/FinRL-Tutorials))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gT-zXutMgqOS"
   },
   "source": [
    "# Part 1. Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T06:03:47.098438200Z",
     "start_time": "2023-08-23T06:03:47.035015800Z"
    },
    "id": "xt1317y2ixSS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import yfinance as yf\n",
    "\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl import config_tickers\n",
    "from finrl.config import INDICATORS\n",
    "\n",
    "import itertools\n",
    "\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
    "from finrl.main import check_and_make_directories\n",
    "from Envs.env_stocktrading import StockTradingEnv\n",
    "\n",
    "check_and_make_directories([TRAINED_MODEL_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWrSrQv3i0Ng"
   },
   "source": [
    "# Part 2. Build A Market Environment in OpenAI Gym-style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiHhM2U-XBMZ"
   },
   "source": [
    "![rl_diagram_transparent_bg.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeneTRdyZDvy"
   },
   "source": [
    "The core element in reinforcement learning are **agent** and **environment**. You can understand RL as the following process: \n",
    "\n",
    "The agent is active in a world, which is the environment. It observe its current condition as a **state**, and is allowed to do certain **actions**. After the agent execute an action, it will arrive at a new state. At the same time, the environment will have feedback to the agent called **reward**, a numerical signal that tells how good or bad the new state is. As the figure above, agent and environment will keep doing this interaction.\n",
    "\n",
    "The goal of agent is to get as much cumulative reward as possible. Reinforcement learning is the method that agent learns to improve its behavior and achieve that goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3H88JXkI93v"
   },
   "source": [
    "To achieve this in Python, we follow the OpenAI gym style to build the stock data into environment.\n",
    "\n",
    "state-action-reward are specified as follows:\n",
    "\n",
    "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes the price data and technical indicators based on the past data. It will learn by interacting with the market environment (usually by replaying historical data).\n",
    "\n",
    "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying. When an action operates multiple shares, a ∈{−k, ..., −1, 0, 1, ..., k}, e.g.. \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
    "\n",
    "\n",
    "**Market environment**: 30 constituent stocks of Dow Jones Industrial Average (DJIA) index. Accessed at the starting date of the testing period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKyZejI0fmp1"
   },
   "source": [
    "## Read data\n",
    "\n",
    "We first read the .csv file of our training data into dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T06:03:53.159497100Z",
     "start_time": "2023-08-23T06:03:52.910244300Z"
    },
    "id": "mFCP1YEhi6oi"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('all_data_with_sentiment_FINAL.csv')\n",
    "\n",
    "# If you are not using the data generated from part 1 of this tutorial, make sure \n",
    "# it has the columns and index in the form that could be make into the environment. \n",
    "# Then you can comment and skip the following two lines.\n",
    "#train = train.set_index(train.columns[0])\n",
    "#train.index.names = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  date   tic     open     high      low    close     volume  \\\n",
      "0  2021-11-24 14:00:00  AAPL  159.380  159.703  158.862  159.085  3513469.0   \n",
      "1  2021-11-24 14:00:00   PFE   47.758   47.831   47.607   47.735   686904.0   \n",
      "2  2021-11-24 14:00:00    MO   38.575   38.598   38.350   38.398   295661.0   \n",
      "3  2021-11-24 14:00:00  META  340.290  341.370  339.000  339.175   695874.0   \n",
      "4  2021-11-24 14:00:00    PG  141.959  142.221  141.802  142.113   230089.0   \n",
      "\n",
      "   day  macd     boll_ub  ...  dx_30  close_30_sma  close_60_sma     vix  \\\n",
      "0  5.0   0.0  160.308182  ...  100.0       159.085       159.085  85.622   \n",
      "1  5.0   0.0  158.835572  ...  100.0        47.735        47.735  85.622   \n",
      "2  5.0   0.0  160.308182  ...  100.0        38.398        38.398  85.622   \n",
      "3  5.0   0.0  160.308182  ...  100.0       339.175       339.175  85.622   \n",
      "4  5.0   0.0  142.570497  ...  100.0       142.113       142.113  85.622   \n",
      "\n",
      "     change  stocktwitsPosts  stocktwitsLikes  stocktwitsImpressions  \\\n",
      "0 -0.591471            102.0            132.0               361612.0   \n",
      "1 -1.960851             14.0              6.0                19312.0   \n",
      "2 -2.130297              0.0              0.0                    0.0   \n",
      "3  3.019949             34.0             18.0                86157.0   \n",
      "4 -0.227013              0.0              0.0                    0.0   \n",
      "\n",
      "   stocktwitsSentiment    random  \n",
      "0             0.532856  0.296856  \n",
      "1             0.476562  0.258228  \n",
      "2             0.313871  0.877339  \n",
      "3             0.520818  0.786928  \n",
      "4             0.307690  0.369543  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Add a new column 'random' with random values between 0 and 1\n",
    "train['random'] = np.random.rand(len(train))\n",
    "\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "SENTIMENT = ['stocktwitsPosts','stocktwitsLikes','stocktwitsImpressions','stocktwitsSentiment', 'random']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to C:\\Users\\potap\\FinARL\\automatic-stock-trading-applied-reinforcement-learning-sommer-term-2023\\Generated Sentiment Data in FinRL\\stock_data\\all_data_with_index.csv\n",
      "                      date   tic     open     high      low    close  \\\n",
      "index                                                                  \n",
      "0      2021-11-24 14:00:00  AAPL  159.380  159.703  158.862  159.085   \n",
      "0      2021-11-24 14:00:00   NKE  168.081  168.315  167.650  168.123   \n",
      "0      2021-11-24 14:00:00   PEP  154.583  154.732  154.265  154.514   \n",
      "0      2021-11-24 14:00:00  NVDA  323.761  325.132  321.596  323.210   \n",
      "0      2021-11-24 14:00:00  NFLX  659.920  660.880  656.890  657.750   \n",
      "\n",
      "          volume  day  macd     boll_ub  ...  dx_30  close_30_sma  \\\n",
      "index                                    ...                        \n",
      "0      3513469.0  5.0   0.0  160.308182  ...  100.0       159.085   \n",
      "0       164604.0  5.0   0.0  168.815945  ...  100.0       168.123   \n",
      "0        88332.0  5.0   0.0  158.835572  ...  100.0       154.514   \n",
      "0      1677597.0  5.0   0.0  160.308182  ...  100.0       323.210   \n",
      "0        46157.0  5.0   0.0  660.353330  ...  100.0       657.750   \n",
      "\n",
      "       close_60_sma     vix    change  stocktwitsPosts  stocktwitsLikes  \\\n",
      "index                                                                     \n",
      "0           159.085  85.622 -0.591471            102.0            132.0   \n",
      "0           168.123  85.622 -1.364129              1.0              0.0   \n",
      "0           154.514  85.622  1.392279              1.0              0.0   \n",
      "0           323.210  85.622  0.983951             71.0             59.0   \n",
      "0           657.750  85.622  1.448540              4.0              0.0   \n",
      "\n",
      "       stocktwitsImpressions  stocktwitsSentiment    random  \n",
      "index                                                        \n",
      "0                   361612.0             0.532856  0.296856  \n",
      "0                     2031.0             0.366283  0.907451  \n",
      "0                     8167.0             0.301477  0.382025  \n",
      "0                   139229.0             0.522796  0.403868  \n",
      "0                      653.0             0.496894  0.565796  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "# Sort the DataFrame by the 'date' column\n",
    "train = train.sort_values(by='date')\n",
    "\n",
    "# Create a mapping of unique dates to their corresponding index\n",
    "date_mapping = {date: idx for idx, date in enumerate(train['date'].unique())}\n",
    "\n",
    "# Map the 'date' column to the date_mapping to get the new index\n",
    "train['index'] = train['date'].map(date_mapping)\n",
    "\n",
    "# Set the new index for the DataFrame\n",
    "train.set_index('index', inplace=True, drop=True)\n",
    "# Specify the path where you want to save the CSV file\n",
    "output_path = r\"C:\\Users\\potap\\FinARL\\automatic-stock-trading-applied-reinforcement-learning-sommer-term-2023\\Generated Sentiment Data in FinRL\\stock_data\\all_data_with_index.csv\"\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "train.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Data saved to {output_path}\")\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T06:03:53.525717700Z",
     "start_time": "2023-08-23T06:03:53.446808900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                      object\n",
       "tic                       object\n",
       "open                     float64\n",
       "high                     float64\n",
       "low                      float64\n",
       "close                    float64\n",
       "volume                   float64\n",
       "day                      float64\n",
       "macd                     float64\n",
       "boll_ub                  float64\n",
       "boll_lb                  float64\n",
       "rsi_30                   float64\n",
       "cci_30                   float64\n",
       "dx_30                    float64\n",
       "close_30_sma             float64\n",
       "close_60_sma             float64\n",
       "vix                      float64\n",
       "change                   float64\n",
       "stocktwitsPosts          float64\n",
       "stocktwitsLikes          float64\n",
       "stocktwitsImpressions    float64\n",
       "stocktwitsSentiment      float64\n",
       "random                   float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.fillna(0)\n",
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yw95ZMicgEyi"
   },
   "source": [
    "## Construct the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WZ6-9q2gq9S"
   },
   "source": [
    "Calculate and specify the parameters we need for constructing the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#SENTIMENT = ['stocktwitsPosts','stocktwitsLikes','stocktwitsImpressions','stocktwitsSentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#SENTIMENT = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "INDICATORS.append('vix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T06:06:01.972279700Z",
     "start_time": "2023-08-23T06:06:01.956637300Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7T3DZPoaIm8k",
    "outputId": "4817e063-400a-416e-f8f2-4b1c4d9c8408"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 30, State Space: 541\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 4*stock_dimension + len(INDICATORS)*stock_dimension +  len(SENTIMENT)*stock_dimension\n",
    "#state_space = 379\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T06:06:02.969957700Z",
     "start_time": "2023-08-23T06:06:02.953981700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stocktwitsPosts',\n",
       " 'stocktwitsLikes',\n",
       " 'stocktwitsImpressions',\n",
       " 'stocktwitsSentiment',\n",
       " 'random']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SENTIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['macd',\n",
       " 'boll_ub',\n",
       " 'boll_lb',\n",
       " 'rsi_30',\n",
       " 'cci_30',\n",
       " 'dx_30',\n",
       " 'close_30_sma',\n",
       " 'close_60_sma',\n",
       " 'vix']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INDICATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"sentiment_list\" : SENTIMENT,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4\n",
    "}\n",
    "\n",
    "\n",
    "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7We-q73jjaFQ"
   },
   "source": [
    "## Environment for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T06:06:05.541279300Z",
     "start_time": "2023-08-23T06:06:05.494015200Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aS-SHiGRJK-4",
    "outputId": "a733ecdf-d857-40f5-b399-4325c7ead299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ],
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "# Part 3: Train DRL Agents\n",
    "* Here, the DRL algorithms are from **[Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/)**. It's a library that implemented popular DRL algorithms using pytorch, succeeding to its old version: Stable Baselines.\n",
    "* Users are also encouraged to try **[ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL)** and **[Ray RLlib](https://github.com/ray-project/ray)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T06:06:11.154422Z",
     "start_time": "2023-08-23T06:06:11.123593400Z"
    },
    "id": "364PsqckttcQ"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "\n",
    "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
    "if_using_a2c = True\n",
    "if_using_ddpg = True\n",
    "if_using_ppo = True\n",
    "if_using_td3 = True\n",
    "if_using_sac = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDmqOyF9h1iz"
   },
   "source": [
    "## Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uijiWgkuh1jB"
   },
   "source": [
    "### Agent 1: A2C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T06:06:14.020357900Z",
     "start_time": "2023-08-23T06:06:13.958069700Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUCnkn-HIbmj",
    "outputId": "2794a094-a916-448c-ead1-6e20184dde2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to results/a2c\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_a2c = agent.get_model(\"a2c\")\n",
    "\n",
    "if if_using_a2c:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/a2c'\n",
    "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_a2c.set_logger(new_logger_a2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T06:06:32.365749900Z",
     "start_time": "2023-08-23T06:06:15.670698500Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0GVpkWGqH4-D",
    "outputId": "f29cf145-e3b5-4e59-f64d-5921462a8f81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 188       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0.0051    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -3.61e+03 |\n",
      "|    reward             | 27.20757  |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 1.24e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 189        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0.00398    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -2.03e+03  |\n",
      "|    reward             | 103.201004 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 2.88e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 191        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | -0.00647   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -3.79e+03  |\n",
      "|    reward             | -72.435005 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 1.78e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 193       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -0.00216  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | 2.95e+03  |\n",
      "|    reward             | 188.03972 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 1.23e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 192        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0.0069     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 1.68e+03   |\n",
      "|    reward             | -185.11946 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 9.3e+03    |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 191      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.5    |\n",
      "|    explained_variance | -0.00418 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 8.75e+03 |\n",
      "|    reward             | 346.3712 |\n",
      "|    std                | 0.998    |\n",
      "|    value_loss         | 5.99e+04 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 190       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0.0344    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 342       |\n",
      "|    reward             | -12.49053 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 611       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 191       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -0.00758  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -2.89e+03 |\n",
      "|    reward             | 25.009933 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 6.36e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 191      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 23       |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.5    |\n",
      "|    explained_variance | 0.000804 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 2.82e+03 |\n",
      "|    reward             | 69.97375 |\n",
      "|    std                | 0.997    |\n",
      "|    value_loss         | 9e+03    |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 192        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0.0209     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 3.39e+03   |\n",
      "|    reward             | 125.349396 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 1.81e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 192        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 28         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | -0.0112    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 1.39e+03   |\n",
      "|    reward             | -248.81761 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 1.26e+04   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 193      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 31       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.5    |\n",
      "|    explained_variance | -0.00387 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 1.29e+04 |\n",
      "|    reward             | 86.69685 |\n",
      "|    std                | 0.997    |\n",
      "|    value_loss         | 1.06e+05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 193       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 33        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -0.0174   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -1.72e+03 |\n",
      "|    reward             | 93.77996  |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 3.15e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 193       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 36        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -0.0154   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 2.94e+03  |\n",
      "|    reward             | 32.645756 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 1.47e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 193        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 38         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0.00748    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | -5.89e+03  |\n",
      "|    reward             | -44.672146 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 4.2e+04    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 193        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 41         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | -0.00689   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 4.5e+03    |\n",
      "|    reward             | -235.35306 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.57e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 193       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 43        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -0.00239  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 6.7e+03   |\n",
      "|    reward             | 48.044712 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 6.36e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 193       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 46        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -0.000473 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 9.61e+03  |\n",
      "|    reward             | 586.10474 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.53e+05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 192      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 49       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.6    |\n",
      "|    explained_variance | -0.0128  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 990      |\n",
      "|    reward             | 23.91629 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 1.85e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 191       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 52        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | -0.0441   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 68.2      |\n",
      "|    reward             | -39.83408 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 3.27e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 191       |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 54        |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0.0363    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | -2.95e+03 |\n",
      "|    reward             | 117.05595 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 9.69e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 191       |\n",
      "|    iterations         | 2200      |\n",
      "|    time_elapsed       | 57        |\n",
      "|    total_timesteps    | 11000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | -0.0173   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2199      |\n",
      "|    policy_loss        | 9.25      |\n",
      "|    reward             | 144.07942 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.12e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 190        |\n",
      "|    iterations         | 2300       |\n",
      "|    time_elapsed       | 60         |\n",
      "|    total_timesteps    | 11500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0.015      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2299       |\n",
      "|    policy_loss        | -2.72e+03  |\n",
      "|    reward             | -222.33862 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 3.99e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 190       |\n",
      "|    iterations         | 2400      |\n",
      "|    time_elapsed       | 62        |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -0.00783  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2399      |\n",
      "|    policy_loss        | -3.99e+03 |\n",
      "|    reward             | -367.0017 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 7.55e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 191        |\n",
      "|    iterations         | 2500       |\n",
      "|    time_elapsed       | 65         |\n",
      "|    total_timesteps    | 12500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | -0.0817    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2499       |\n",
      "|    policy_loss        | 2.36       |\n",
      "|    reward             | -47.556377 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 1.06e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 191       |\n",
      "|    iterations         | 2600      |\n",
      "|    time_elapsed       | 67        |\n",
      "|    total_timesteps    | 13000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -0.00125  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2599      |\n",
      "|    policy_loss        | -125      |\n",
      "|    reward             | -209.4224 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 4.54e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 191       |\n",
      "|    iterations         | 2700      |\n",
      "|    time_elapsed       | 70        |\n",
      "|    total_timesteps    | 13500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -0.0165   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2699      |\n",
      "|    policy_loss        | -4.63e+03 |\n",
      "|    reward             | -266.8201 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 3.41e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 191        |\n",
      "|    iterations         | 2800       |\n",
      "|    time_elapsed       | 73         |\n",
      "|    total_timesteps    | 14000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0.0648     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2799       |\n",
      "|    policy_loss        | -949       |\n",
      "|    reward             | -30.976358 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 4.71e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 190       |\n",
      "|    iterations         | 2900      |\n",
      "|    time_elapsed       | 75        |\n",
      "|    total_timesteps    | 14500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0.00318   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2899      |\n",
      "|    policy_loss        | -4.17e+03 |\n",
      "|    reward             | -206.1321 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.06e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 190       |\n",
      "|    iterations         | 3000      |\n",
      "|    time_elapsed       | 78        |\n",
      "|    total_timesteps    | 15000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | -0.0172   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2999      |\n",
      "|    policy_loss        | 2.08e+04  |\n",
      "|    reward             | -50.02369 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.21e+05  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 191        |\n",
      "|    iterations         | 3100       |\n",
      "|    time_elapsed       | 81         |\n",
      "|    total_timesteps    | 15500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0.00046    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3099       |\n",
      "|    policy_loss        | -5.54e+03  |\n",
      "|    reward             | -697.50616 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.46e+05   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 191       |\n",
      "|    iterations         | 3200      |\n",
      "|    time_elapsed       | 83        |\n",
      "|    total_timesteps    | 16000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0.0354    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3199      |\n",
      "|    policy_loss        | -187      |\n",
      "|    reward             | 27.202127 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.57e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 191       |\n",
      "|    iterations         | 3300      |\n",
      "|    time_elapsed       | 86        |\n",
      "|    total_timesteps    | 16500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | -0.0401   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3299      |\n",
      "|    policy_loss        | 641       |\n",
      "|    reward             | 39.591606 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.86e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 191       |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 88        |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0.0281    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | 4.23e+03  |\n",
      "|    reward             | -81.87022 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.92e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 191        |\n",
      "|    iterations         | 3500       |\n",
      "|    time_elapsed       | 91         |\n",
      "|    total_timesteps    | 17500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | 0.0213     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3499       |\n",
      "|    policy_loss        | -3.56e+03  |\n",
      "|    reward             | -173.85776 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.53e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 191        |\n",
      "|    iterations         | 3600       |\n",
      "|    time_elapsed       | 93         |\n",
      "|    total_timesteps    | 18000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | 0.0186     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3599       |\n",
      "|    policy_loss        | 7.34e+03   |\n",
      "|    reward             | -184.19821 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.14e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 191       |\n",
      "|    iterations         | 3700      |\n",
      "|    time_elapsed       | 96        |\n",
      "|    total_timesteps    | 18500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | 0.00948   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3699      |\n",
      "|    policy_loss        | 7.46e+03  |\n",
      "|    reward             | 451.88443 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.44e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 191       |\n",
      "|    iterations         | 3800      |\n",
      "|    time_elapsed       | 98        |\n",
      "|    total_timesteps    | 19000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | 0.0275    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3799      |\n",
      "|    policy_loss        | 1.13e+03  |\n",
      "|    reward             | -45.75468 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.96e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 192        |\n",
      "|    iterations         | 3900       |\n",
      "|    time_elapsed       | 101        |\n",
      "|    total_timesteps    | 19500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | -0.0329    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3899       |\n",
      "|    policy_loss        | 2.32e+03   |\n",
      "|    reward             | 0.57520175 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 6.55e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 192       |\n",
      "|    iterations         | 4000      |\n",
      "|    time_elapsed       | 103       |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | 0.00426   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3999      |\n",
      "|    policy_loss        | -1.57e+03 |\n",
      "|    reward             | 37.629333 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 6.8e+03   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 192       |\n",
      "|    iterations         | 4100      |\n",
      "|    time_elapsed       | 106       |\n",
      "|    total_timesteps    | 20500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | -0.00465  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4099      |\n",
      "|    policy_loss        | -8.58e+03 |\n",
      "|    reward             | -157.1616 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 5.76e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 192        |\n",
      "|    iterations         | 4200       |\n",
      "|    time_elapsed       | 108        |\n",
      "|    total_timesteps    | 21000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43        |\n",
      "|    explained_variance | -0.0032    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4199       |\n",
      "|    policy_loss        | -1.48e+04  |\n",
      "|    reward             | -389.48608 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.49e+05   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 192        |\n",
      "|    iterations         | 4300       |\n",
      "|    time_elapsed       | 111        |\n",
      "|    total_timesteps    | 21500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43        |\n",
      "|    explained_variance | 0.0181     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4299       |\n",
      "|    policy_loss        | -1.94e+04  |\n",
      "|    reward             | -297.14304 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.39e+05   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 192        |\n",
      "|    iterations         | 4400       |\n",
      "|    time_elapsed       | 114        |\n",
      "|    total_timesteps    | 22000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43        |\n",
      "|    explained_variance | 0.048      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4399       |\n",
      "|    policy_loss        | 2.98e+03   |\n",
      "|    reward             | -120.48289 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 8.03e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 192       |\n",
      "|    iterations         | 4500      |\n",
      "|    time_elapsed       | 116       |\n",
      "|    total_timesteps    | 22500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | -0.0124   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4499      |\n",
      "|    policy_loss        | 1.32e+03  |\n",
      "|    reward             | 243.36037 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.53e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 192       |\n",
      "|    iterations         | 4600      |\n",
      "|    time_elapsed       | 119       |\n",
      "|    total_timesteps    | 23000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | 0.00161   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4599      |\n",
      "|    policy_loss        | -1.41e+03 |\n",
      "|    reward             | 195.03499 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 3.98e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 193        |\n",
      "|    iterations         | 4700       |\n",
      "|    time_elapsed       | 121        |\n",
      "|    total_timesteps    | 23500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.1      |\n",
      "|    explained_variance | -0.0147    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4699       |\n",
      "|    policy_loss        | 7.51e+03   |\n",
      "|    reward             | -33.172157 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 3.96e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 193        |\n",
      "|    iterations         | 4800       |\n",
      "|    time_elapsed       | 124        |\n",
      "|    total_timesteps    | 24000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.2      |\n",
      "|    explained_variance | 0.000815   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4799       |\n",
      "|    policy_loss        | -7.45e+03  |\n",
      "|    reward             | -282.04376 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 8.11e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 193       |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 126       |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.2     |\n",
      "|    explained_variance | -0.00139  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4899      |\n",
      "|    policy_loss        | -6.02e+03 |\n",
      "|    reward             | 284.8063  |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 9.92e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 192       |\n",
      "|    iterations         | 5000      |\n",
      "|    time_elapsed       | 129       |\n",
      "|    total_timesteps    | 25000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | 0.03      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4999      |\n",
      "|    policy_loss        | 2.19e+03  |\n",
      "|    reward             | 17.380224 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 3.51e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 192        |\n",
      "|    iterations         | 5100       |\n",
      "|    time_elapsed       | 132        |\n",
      "|    total_timesteps    | 25500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.1      |\n",
      "|    explained_variance | 0.0418     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5099       |\n",
      "|    policy_loss        | 101        |\n",
      "|    reward             | -41.751934 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.83e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 192       |\n",
      "|    iterations         | 5200      |\n",
      "|    time_elapsed       | 134       |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | -0.116    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5199      |\n",
      "|    policy_loss        | 5.3e+03   |\n",
      "|    reward             | -279.2241 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.65e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 192        |\n",
      "|    iterations         | 5300       |\n",
      "|    time_elapsed       | 137        |\n",
      "|    total_timesteps    | 26500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.1      |\n",
      "|    explained_variance | 0.00544    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5299       |\n",
      "|    policy_loss        | -2.94e+03  |\n",
      "|    reward             | -229.25636 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 8.36e+03   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 192      |\n",
      "|    iterations         | 5400     |\n",
      "|    time_elapsed       | 139      |\n",
      "|    total_timesteps    | 27000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -43      |\n",
      "|    explained_variance | -0.0139  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5399     |\n",
      "|    policy_loss        | 7.91e+03 |\n",
      "|    reward             | 356.5106 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 4.54e+04 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 193        |\n",
      "|    iterations         | 5500       |\n",
      "|    time_elapsed       | 142        |\n",
      "|    total_timesteps    | 27500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43        |\n",
      "|    explained_variance | -0.009     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5499       |\n",
      "|    policy_loss        | -1.07e+03  |\n",
      "|    reward             | -386.02695 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.15e+04   |\n",
      "--------------------------------------\n",
      "day: 3102, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 9529033.55\n",
      "total_reward: 8529033.55\n",
      "total_cost: 137299.36\n",
      "total_trades: 57858\n",
      "Sharpe: 2.931\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 193       |\n",
      "|    iterations         | 5600      |\n",
      "|    time_elapsed       | 144       |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | -0.0453   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5599      |\n",
      "|    policy_loss        | -337      |\n",
      "|    reward             | 35.378544 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.23e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 193        |\n",
      "|    iterations         | 5700       |\n",
      "|    time_elapsed       | 147        |\n",
      "|    total_timesteps    | 28500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43        |\n",
      "|    explained_variance | 0.103      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5699       |\n",
      "|    policy_loss        | -1.57e+03  |\n",
      "|    reward             | -52.512287 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 6.42e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 193       |\n",
      "|    iterations         | 5800      |\n",
      "|    time_elapsed       | 149       |\n",
      "|    total_timesteps    | 29000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | 0.0486    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5799      |\n",
      "|    policy_loss        | 2.24e+03  |\n",
      "|    reward             | -65.67413 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.26e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 193       |\n",
      "|    iterations         | 5900      |\n",
      "|    time_elapsed       | 152       |\n",
      "|    total_timesteps    | 29500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | -0.00507  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5899      |\n",
      "|    policy_loss        | -1.31e+03 |\n",
      "|    reward             | 221.46301 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.25e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 193        |\n",
      "|    iterations         | 6000       |\n",
      "|    time_elapsed       | 154        |\n",
      "|    total_timesteps    | 30000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43        |\n",
      "|    explained_variance | -0.043     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5999       |\n",
      "|    policy_loss        | 6.97e+03   |\n",
      "|    reward             | -45.426632 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.28e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 193       |\n",
      "|    iterations         | 6100      |\n",
      "|    time_elapsed       | 157       |\n",
      "|    total_timesteps    | 30500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | 0.00654   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6099      |\n",
      "|    policy_loss        | -9.94e+03 |\n",
      "|    reward             | 463.0766  |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.99e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 6200      |\n",
      "|    time_elapsed       | 159       |\n",
      "|    total_timesteps    | 31000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | -0.0278   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6199      |\n",
      "|    policy_loss        | -1.8e+04  |\n",
      "|    reward             | -607.5993 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 3.6e+05   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 193       |\n",
      "|    iterations         | 6300      |\n",
      "|    time_elapsed       | 162       |\n",
      "|    total_timesteps    | 31500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | -0.00975  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6299      |\n",
      "|    policy_loss        | -3.46e+03 |\n",
      "|    reward             | 63.608208 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 9.96e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 6400      |\n",
      "|    time_elapsed       | 164       |\n",
      "|    total_timesteps    | 32000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | -0.0548   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6399      |\n",
      "|    policy_loss        | 9.73e+03  |\n",
      "|    reward             | 53.503582 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 5.29e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 6500      |\n",
      "|    time_elapsed       | 167       |\n",
      "|    total_timesteps    | 32500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | 0.0401    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6499      |\n",
      "|    policy_loss        | -1.35e+03 |\n",
      "|    reward             | -430.0216 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.92e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 6600      |\n",
      "|    time_elapsed       | 169       |\n",
      "|    total_timesteps    | 33000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | 0.035     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6599      |\n",
      "|    policy_loss        | -6.58e+03 |\n",
      "|    reward             | 126.17331 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 3.43e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 6700      |\n",
      "|    time_elapsed       | 172       |\n",
      "|    total_timesteps    | 33500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | 0.00452   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6699      |\n",
      "|    policy_loss        | -1.05e+04 |\n",
      "|    reward             | -737.3157 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.62e+05  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 194        |\n",
      "|    iterations         | 6800       |\n",
      "|    time_elapsed       | 174        |\n",
      "|    total_timesteps    | 34000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43        |\n",
      "|    explained_variance | -0.027     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6799       |\n",
      "|    policy_loss        | 1.56e+04   |\n",
      "|    reward             | -244.22348 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.78e+05   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 6900      |\n",
      "|    time_elapsed       | 177       |\n",
      "|    total_timesteps    | 34500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | 0.0557    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6899      |\n",
      "|    policy_loss        | 3.26e+03  |\n",
      "|    reward             | -89.14888 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 9.7e+03   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 7000      |\n",
      "|    time_elapsed       | 179       |\n",
      "|    total_timesteps    | 35000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | 0.0213    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6999      |\n",
      "|    policy_loss        | -2.53e+03 |\n",
      "|    reward             | 63.17618  |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 7.27e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 7100      |\n",
      "|    time_elapsed       | 182       |\n",
      "|    total_timesteps    | 35500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | 0.00662   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7099      |\n",
      "|    policy_loss        | -2.76e+03 |\n",
      "|    reward             | -4.848692 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 8.58e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 194        |\n",
      "|    iterations         | 7200       |\n",
      "|    time_elapsed       | 184        |\n",
      "|    total_timesteps    | 36000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43        |\n",
      "|    explained_variance | 0.012      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7199       |\n",
      "|    policy_loss        | -5.27e+03  |\n",
      "|    reward             | 127.038895 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.77e+04   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 194      |\n",
      "|    iterations         | 7300     |\n",
      "|    time_elapsed       | 187      |\n",
      "|    total_timesteps    | 36500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -43      |\n",
      "|    explained_variance | 0.000179 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7299     |\n",
      "|    policy_loss        | 7.42e+03 |\n",
      "|    reward             | 812.8876 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 8.07e+04 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 7400      |\n",
      "|    time_elapsed       | 190       |\n",
      "|    total_timesteps    | 37000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | -0.00389  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7399      |\n",
      "|    policy_loss        | 9.7e+03   |\n",
      "|    reward             | -522.7178 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 4.42e+05  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 194        |\n",
      "|    iterations         | 7500       |\n",
      "|    time_elapsed       | 192        |\n",
      "|    total_timesteps    | 37500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43        |\n",
      "|    explained_variance | -0.0145    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7499       |\n",
      "|    policy_loss        | 625        |\n",
      "|    reward             | -14.445228 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.71e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 7600      |\n",
      "|    time_elapsed       | 195       |\n",
      "|    total_timesteps    | 38000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | 0.00346   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7599      |\n",
      "|    policy_loss        | 4.74e+03  |\n",
      "|    reward             | -61.57046 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.46e+04  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 194      |\n",
      "|    iterations         | 7700     |\n",
      "|    time_elapsed       | 197      |\n",
      "|    total_timesteps    | 38500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -43      |\n",
      "|    explained_variance | -0.00694 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7699     |\n",
      "|    policy_loss        | -2.3e+03 |\n",
      "|    reward             | 80.73619 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 1.21e+04 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 194        |\n",
      "|    iterations         | 7800       |\n",
      "|    time_elapsed       | 200        |\n",
      "|    total_timesteps    | 39000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43        |\n",
      "|    explained_variance | 0.0179     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7799       |\n",
      "|    policy_loss        | 7.52e+03   |\n",
      "|    reward             | -282.66693 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 4e+04      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 194        |\n",
      "|    iterations         | 7900       |\n",
      "|    time_elapsed       | 202        |\n",
      "|    total_timesteps    | 39500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43        |\n",
      "|    explained_variance | -0.00455   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7899       |\n",
      "|    policy_loss        | -6.92e+03  |\n",
      "|    reward             | -443.49387 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 4.79e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 194        |\n",
      "|    iterations         | 8000       |\n",
      "|    time_elapsed       | 205        |\n",
      "|    total_timesteps    | 40000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43        |\n",
      "|    explained_variance | -0.00655   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7999       |\n",
      "|    policy_loss        | 222        |\n",
      "|    reward             | -1424.3865 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.43e+04   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 194      |\n",
      "|    iterations         | 8100     |\n",
      "|    time_elapsed       | 207      |\n",
      "|    total_timesteps    | 40500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.9    |\n",
      "|    explained_variance | -0.0577  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8099     |\n",
      "|    policy_loss        | 472      |\n",
      "|    reward             | 1.297124 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 1.81e+03 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 194        |\n",
      "|    iterations         | 8200       |\n",
      "|    time_elapsed       | 210        |\n",
      "|    total_timesteps    | 41000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | 0.00953    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8199       |\n",
      "|    policy_loss        | 1.87e+03   |\n",
      "|    reward             | -16.447405 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 5.56e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 8300      |\n",
      "|    time_elapsed       | 212       |\n",
      "|    total_timesteps    | 41500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0.0018    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8299      |\n",
      "|    policy_loss        | 359       |\n",
      "|    reward             | -60.85726 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 8.62e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 194        |\n",
      "|    iterations         | 8400       |\n",
      "|    time_elapsed       | 215        |\n",
      "|    total_timesteps    | 42000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | 0.0117     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8399       |\n",
      "|    policy_loss        | -935       |\n",
      "|    reward             | -140.40723 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.73e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 8500      |\n",
      "|    time_elapsed       | 218       |\n",
      "|    total_timesteps    | 42500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | 0.0139    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8499      |\n",
      "|    policy_loss        | 5.17e+03  |\n",
      "|    reward             | -154.7864 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 4.47e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 194        |\n",
      "|    iterations         | 8600       |\n",
      "|    time_elapsed       | 220        |\n",
      "|    total_timesteps    | 43000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43        |\n",
      "|    explained_variance | -0.00135   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8599       |\n",
      "|    policy_loss        | -1.23e+04  |\n",
      "|    reward             | -530.74005 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.56e+05   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 8700      |\n",
      "|    time_elapsed       | 223       |\n",
      "|    total_timesteps    | 43500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | 0.0475    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8699      |\n",
      "|    policy_loss        | 108       |\n",
      "|    reward             | 33.922615 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 303       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 8800      |\n",
      "|    time_elapsed       | 226       |\n",
      "|    total_timesteps    | 44000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | 0.0145    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8799      |\n",
      "|    policy_loss        | 1.66e+03  |\n",
      "|    reward             | 27.003735 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 3.35e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 8900      |\n",
      "|    time_elapsed       | 228       |\n",
      "|    total_timesteps    | 44500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | -0.0183   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8899      |\n",
      "|    policy_loss        | 3.91e+03  |\n",
      "|    reward             | 28.145464 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.72e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 194        |\n",
      "|    iterations         | 9000       |\n",
      "|    time_elapsed       | 231        |\n",
      "|    total_timesteps    | 45000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43        |\n",
      "|    explained_variance | 0.0152     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8999       |\n",
      "|    policy_loss        | 3.65e+03   |\n",
      "|    reward             | -50.267593 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.46e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 9100      |\n",
      "|    time_elapsed       | 233       |\n",
      "|    total_timesteps    | 45500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | -0.0144   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9099      |\n",
      "|    policy_loss        | -2.76e+03 |\n",
      "|    reward             | 104.68453 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.33e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 9200      |\n",
      "|    time_elapsed       | 236       |\n",
      "|    total_timesteps    | 46000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | -0.0372   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9199      |\n",
      "|    policy_loss        | -4.4e+03  |\n",
      "|    reward             | 182.19498 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.14e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 9300      |\n",
      "|    time_elapsed       | 238       |\n",
      "|    total_timesteps    | 46500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | -0.0131   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9299      |\n",
      "|    policy_loss        | 2.12e+04  |\n",
      "|    reward             | 458.79913 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.76e+05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 194      |\n",
      "|    iterations         | 9400     |\n",
      "|    time_elapsed       | 241      |\n",
      "|    total_timesteps    | 47000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -43      |\n",
      "|    explained_variance | 0.00582  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9399     |\n",
      "|    policy_loss        | 77.9     |\n",
      "|    reward             | 68.07617 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 3.93e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 9500      |\n",
      "|    time_elapsed       | 244       |\n",
      "|    total_timesteps    | 47500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | -0.181    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9499      |\n",
      "|    policy_loss        | 568       |\n",
      "|    reward             | 31.625969 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.35e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 194        |\n",
      "|    iterations         | 9600       |\n",
      "|    time_elapsed       | 246        |\n",
      "|    total_timesteps    | 48000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43        |\n",
      "|    explained_variance | -0.0679    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9599       |\n",
      "|    policy_loss        | -2.35e+03  |\n",
      "|    reward             | -114.55143 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.21e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 194        |\n",
      "|    iterations         | 9700       |\n",
      "|    time_elapsed       | 249        |\n",
      "|    total_timesteps    | 48500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43        |\n",
      "|    explained_variance | -0.00353   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9699       |\n",
      "|    policy_loss        | -3.25e+03  |\n",
      "|    reward             | -215.02225 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.6e+04    |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 9800      |\n",
      "|    time_elapsed       | 251       |\n",
      "|    total_timesteps    | 49000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | -0.00779  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9799      |\n",
      "|    policy_loss        | 4.59e+03  |\n",
      "|    reward             | 354.82797 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 3.53e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 9900      |\n",
      "|    time_elapsed       | 254       |\n",
      "|    total_timesteps    | 49500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0.00498   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9899      |\n",
      "|    policy_loss        | -2.15e+04 |\n",
      "|    reward             | 805.3568  |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 5.15e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 10000     |\n",
      "|    time_elapsed       | 256       |\n",
      "|    total_timesteps    | 50000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0.0217    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9999      |\n",
      "|    policy_loss        | -3.11e+03 |\n",
      "|    reward             | -82.88005 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.38e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 194        |\n",
      "|    iterations         | 10100      |\n",
      "|    time_elapsed       | 259        |\n",
      "|    total_timesteps    | 50500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | -0.0237    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10099      |\n",
      "|    policy_loss        | -1.24e+03  |\n",
      "|    reward             | 103.327194 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 5.62e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 10200     |\n",
      "|    time_elapsed       | 261       |\n",
      "|    total_timesteps    | 51000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0.0327    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10199     |\n",
      "|    policy_loss        | 5.43e+03  |\n",
      "|    reward             | 232.72353 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.86e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 10300     |\n",
      "|    time_elapsed       | 264       |\n",
      "|    total_timesteps    | 51500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0.0226    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10299     |\n",
      "|    policy_loss        | 838       |\n",
      "|    reward             | -38.72634 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.84e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 194        |\n",
      "|    iterations         | 10400      |\n",
      "|    time_elapsed       | 266        |\n",
      "|    total_timesteps    | 52000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | -0.00162   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10399      |\n",
      "|    policy_loss        | -4.35e+03  |\n",
      "|    reward             | -37.632545 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.24e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 194        |\n",
      "|    iterations         | 10500      |\n",
      "|    time_elapsed       | 269        |\n",
      "|    total_timesteps    | 52500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | 0.00488    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10499      |\n",
      "|    policy_loss        | -1.91e+04  |\n",
      "|    reward             | -173.30795 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.68e+05   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 194        |\n",
      "|    iterations         | 10600      |\n",
      "|    time_elapsed       | 271        |\n",
      "|    total_timesteps    | 53000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | -0.00721   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10599      |\n",
      "|    policy_loss        | -3.64e+03  |\n",
      "|    reward             | -127.47503 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.4e+04    |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 10700     |\n",
      "|    time_elapsed       | 274       |\n",
      "|    total_timesteps    | 53500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | -0.00701  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10699     |\n",
      "|    policy_loss        | -310      |\n",
      "|    reward             | -79.86804 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.96e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 195        |\n",
      "|    iterations         | 10800      |\n",
      "|    time_elapsed       | 276        |\n",
      "|    total_timesteps    | 54000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | 0.0173     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10799      |\n",
      "|    policy_loss        | 420        |\n",
      "|    reward             | -33.253128 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.49e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 10900     |\n",
      "|    time_elapsed       | 279       |\n",
      "|    total_timesteps    | 54500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0.0476    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10899     |\n",
      "|    policy_loss        | 2.25e+04  |\n",
      "|    reward             | 132.28055 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.02e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 11000     |\n",
      "|    time_elapsed       | 281       |\n",
      "|    total_timesteps    | 55000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0.00473   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10999     |\n",
      "|    policy_loss        | -1.54e+04 |\n",
      "|    reward             | 133.38864 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.09e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 11100     |\n",
      "|    time_elapsed       | 284       |\n",
      "|    total_timesteps    | 55500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0.00412   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11099     |\n",
      "|    policy_loss        | -3.37e+04 |\n",
      "|    reward             | -686.7224 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 7e+05     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 195      |\n",
      "|    iterations         | 11200    |\n",
      "|    time_elapsed       | 286      |\n",
      "|    total_timesteps    | 56000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.9    |\n",
      "|    explained_variance | 0.0331   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11199    |\n",
      "|    policy_loss        | 2.05e+03 |\n",
      "|    reward             | 48.88998 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 3.38e+03 |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 195         |\n",
      "|    iterations         | 11300       |\n",
      "|    time_elapsed       | 289         |\n",
      "|    total_timesteps    | 56500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.9       |\n",
      "|    explained_variance | 0.0316      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11299       |\n",
      "|    policy_loss        | 2.19e+03    |\n",
      "|    reward             | -13.1036005 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 1.17e+04    |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 195        |\n",
      "|    iterations         | 11400      |\n",
      "|    time_elapsed       | 291        |\n",
      "|    total_timesteps    | 57000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | -0.0138    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11399      |\n",
      "|    policy_loss        | -8.05e+03  |\n",
      "|    reward             | -212.84232 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 6.56e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 11500     |\n",
      "|    time_elapsed       | 294       |\n",
      "|    total_timesteps    | 57500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | 0.00322   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11499     |\n",
      "|    policy_loss        | -8.79e+03 |\n",
      "|    reward             | 364.17905 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 8.39e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 11600     |\n",
      "|    time_elapsed       | 297       |\n",
      "|    total_timesteps    | 58000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | -0.0228   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11599     |\n",
      "|    policy_loss        | 1.15e+04  |\n",
      "|    reward             | 288.89465 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 7.57e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 11700     |\n",
      "|    time_elapsed       | 299       |\n",
      "|    total_timesteps    | 58500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | -0.00232  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11699     |\n",
      "|    policy_loss        | -5.17e+03 |\n",
      "|    reward             | 507.97507 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.06e+05  |\n",
      "-------------------------------------\n",
      "day: 3102, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 9786699.74\n",
      "total_reward: 8786699.74\n",
      "total_cost: 283168.36\n",
      "total_trades: 59442\n",
      "Sharpe: 4.190\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 195        |\n",
      "|    iterations         | 11800      |\n",
      "|    time_elapsed       | 302        |\n",
      "|    total_timesteps    | 59000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43        |\n",
      "|    explained_variance | 0.0169     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11799      |\n",
      "|    policy_loss        | -487       |\n",
      "|    reward             | -11.403251 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.13e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 195        |\n",
      "|    iterations         | 11900      |\n",
      "|    time_elapsed       | 304        |\n",
      "|    total_timesteps    | 59500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43        |\n",
      "|    explained_variance | 0.00169    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11899      |\n",
      "|    policy_loss        | -3.31e+03  |\n",
      "|    reward             | -1.8984034 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 9.24e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 195        |\n",
      "|    iterations         | 12000      |\n",
      "|    time_elapsed       | 307        |\n",
      "|    total_timesteps    | 60000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43        |\n",
      "|    explained_variance | 0.00479    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11999      |\n",
      "|    policy_loss        | 586        |\n",
      "|    reward             | -249.11632 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 8.74e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 195        |\n",
      "|    iterations         | 12100      |\n",
      "|    time_elapsed       | 309        |\n",
      "|    total_timesteps    | 60500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43        |\n",
      "|    explained_variance | 0.0524     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12099      |\n",
      "|    policy_loss        | 1.92e+03   |\n",
      "|    reward             | -125.99621 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.62e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 12200     |\n",
      "|    time_elapsed       | 312       |\n",
      "|    total_timesteps    | 61000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | 0.00149   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12199     |\n",
      "|    policy_loss        | 2.85e+03  |\n",
      "|    reward             | -13.20722 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.61e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 12300     |\n",
      "|    time_elapsed       | 314       |\n",
      "|    total_timesteps    | 61500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | -0.000219 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12299     |\n",
      "|    policy_loss        | -1.17e+04 |\n",
      "|    reward             | -340.0437 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.46e+05  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 195        |\n",
      "|    iterations         | 12400      |\n",
      "|    time_elapsed       | 317        |\n",
      "|    total_timesteps    | 62000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | -0.0191    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12399      |\n",
      "|    policy_loss        | 2.48e+04   |\n",
      "|    reward             | -508.53018 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 4.11e+05   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 12500     |\n",
      "|    time_elapsed       | 319       |\n",
      "|    total_timesteps    | 62500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | 0.0605    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12499     |\n",
      "|    policy_loss        | 2.62e+03  |\n",
      "|    reward             | 209.00568 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 5.33e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 12600     |\n",
      "|    time_elapsed       | 322       |\n",
      "|    total_timesteps    | 63000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | 0.0119    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12599     |\n",
      "|    policy_loss        | -1.59e+03 |\n",
      "|    reward             | 2.3072755 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.07e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 12700     |\n",
      "|    time_elapsed       | 324       |\n",
      "|    total_timesteps    | 63500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0.0184    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12699     |\n",
      "|    policy_loss        | 3.84e+03  |\n",
      "|    reward             | 338.74017 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.63e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 195        |\n",
      "|    iterations         | 12800      |\n",
      "|    time_elapsed       | 327        |\n",
      "|    total_timesteps    | 64000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | -0.014     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12799      |\n",
      "|    policy_loss        | -3.96e+03  |\n",
      "|    reward             | -981.48883 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 4.77e+04   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 195      |\n",
      "|    iterations         | 12900    |\n",
      "|    time_elapsed       | 329      |\n",
      "|    total_timesteps    | 64500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.9    |\n",
      "|    explained_variance | -0.00318 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12899    |\n",
      "|    policy_loss        | 1.97e+03 |\n",
      "|    reward             | 219.3237 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 1.14e+05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 13000     |\n",
      "|    time_elapsed       | 332       |\n",
      "|    total_timesteps    | 65000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | 0.000473  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12999     |\n",
      "|    policy_loss        | -5.36e+04 |\n",
      "|    reward             | 688.66766 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.19e+06  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 195        |\n",
      "|    iterations         | 13100      |\n",
      "|    time_elapsed       | 334        |\n",
      "|    total_timesteps    | 65500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43        |\n",
      "|    explained_variance | 0.0116     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13099      |\n",
      "|    policy_loss        | -3.02e+03  |\n",
      "|    reward             | -104.61409 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 6.79e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 13200     |\n",
      "|    time_elapsed       | 337       |\n",
      "|    total_timesteps    | 66000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | -0.00329  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13199     |\n",
      "|    policy_loss        | 1.32e+04  |\n",
      "|    reward             | 47.478207 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 8.59e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 13300     |\n",
      "|    time_elapsed       | 339       |\n",
      "|    total_timesteps    | 66500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0.0249    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13299     |\n",
      "|    policy_loss        | -5.63e+03 |\n",
      "|    reward             | 625.121   |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 6.91e+04  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 195      |\n",
      "|    iterations         | 13400    |\n",
      "|    time_elapsed       | 342      |\n",
      "|    total_timesteps    | 67000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -43      |\n",
      "|    explained_variance | 0.0236   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13399    |\n",
      "|    policy_loss        | -1.9e+03 |\n",
      "|    reward             | 712.0197 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 2.65e+04 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 13500     |\n",
      "|    time_elapsed       | 344       |\n",
      "|    total_timesteps    | 67500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | 0.00034   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13499     |\n",
      "|    policy_loss        | -1.32e+04 |\n",
      "|    reward             | 87.27713  |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.42e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 13600     |\n",
      "|    time_elapsed       | 347       |\n",
      "|    total_timesteps    | 68000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | -0.00495  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13599     |\n",
      "|    policy_loss        | -3.36e+04 |\n",
      "|    reward             | 807.26074 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 6.9e+05   |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 195        |\n",
      "|    iterations         | 13700      |\n",
      "|    time_elapsed       | 349        |\n",
      "|    total_timesteps    | 68500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.1      |\n",
      "|    explained_variance | 0.00677    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13699      |\n",
      "|    policy_loss        | -1.99e+03  |\n",
      "|    reward             | -94.105675 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 9.75e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 13800     |\n",
      "|    time_elapsed       | 352       |\n",
      "|    total_timesteps    | 69000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | -0.0102   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13799     |\n",
      "|    policy_loss        | -4.01e+03 |\n",
      "|    reward             | -130.602  |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.46e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 13900     |\n",
      "|    time_elapsed       | 355       |\n",
      "|    total_timesteps    | 69500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | -0.0248   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13899     |\n",
      "|    policy_loss        | -4.45e+03 |\n",
      "|    reward             | 33.99316  |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 3.54e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 14000     |\n",
      "|    time_elapsed       | 357       |\n",
      "|    total_timesteps    | 70000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | 0.00358   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13999     |\n",
      "|    policy_loss        | -1.11e+03 |\n",
      "|    reward             | 352.0905  |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.18e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 14100     |\n",
      "|    time_elapsed       | 359       |\n",
      "|    total_timesteps    | 70500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | 0.00625   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14099     |\n",
      "|    policy_loss        | -3.95e+03 |\n",
      "|    reward             | 242.80809 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 4.59e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 14200     |\n",
      "|    time_elapsed       | 362       |\n",
      "|    total_timesteps    | 71000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | 0.00853   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14199     |\n",
      "|    policy_loss        | 5.84e+04  |\n",
      "|    reward             | 456.63318 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.71e+06  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 195        |\n",
      "|    iterations         | 14300      |\n",
      "|    time_elapsed       | 364        |\n",
      "|    total_timesteps    | 71500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.2      |\n",
      "|    explained_variance | -0.105     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14299      |\n",
      "|    policy_loss        | 366        |\n",
      "|    reward             | -20.539877 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 708        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 196       |\n",
      "|    iterations         | 14400     |\n",
      "|    time_elapsed       | 367       |\n",
      "|    total_timesteps    | 72000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.2     |\n",
      "|    explained_variance | 0.0216    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14399     |\n",
      "|    policy_loss        | -7.87e+03 |\n",
      "|    reward             | -304.7061 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 5.4e+04   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 196       |\n",
      "|    iterations         | 14500     |\n",
      "|    time_elapsed       | 369       |\n",
      "|    total_timesteps    | 72500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.2     |\n",
      "|    explained_variance | 0.0178    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14499     |\n",
      "|    policy_loss        | -3.38e+03 |\n",
      "|    reward             | 76.42398  |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 3.15e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 196       |\n",
      "|    iterations         | 14600     |\n",
      "|    time_elapsed       | 372       |\n",
      "|    total_timesteps    | 73000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.2     |\n",
      "|    explained_variance | 0.0327    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14599     |\n",
      "|    policy_loss        | -4.97e+03 |\n",
      "|    reward             | 1.6293386 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 7.08e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 196       |\n",
      "|    iterations         | 14700     |\n",
      "|    time_elapsed       | 374       |\n",
      "|    total_timesteps    | 73500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | 0.00092   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14699     |\n",
      "|    policy_loss        | -1.07e+04 |\n",
      "|    reward             | 388.8176  |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.07e+05  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 196        |\n",
      "|    iterations         | 14800      |\n",
      "|    time_elapsed       | 376        |\n",
      "|    total_timesteps    | 74000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.1      |\n",
      "|    explained_variance | -0.00104   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14799      |\n",
      "|    policy_loss        | -2.52e+04  |\n",
      "|    reward             | -179.45645 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 6.99e+05   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 196       |\n",
      "|    iterations         | 14900     |\n",
      "|    time_elapsed       | 379       |\n",
      "|    total_timesteps    | 74500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.2     |\n",
      "|    explained_variance | -0.17     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14899     |\n",
      "|    policy_loss        | -1.43e+03 |\n",
      "|    reward             | -34.41341 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.13e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 196       |\n",
      "|    iterations         | 15000     |\n",
      "|    time_elapsed       | 381       |\n",
      "|    total_timesteps    | 75000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.2     |\n",
      "|    explained_variance | -0.105    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14999     |\n",
      "|    policy_loss        | 76.1      |\n",
      "|    reward             | 20.407768 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 2.23e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 196       |\n",
      "|    iterations         | 15100     |\n",
      "|    time_elapsed       | 384       |\n",
      "|    total_timesteps    | 75500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.2     |\n",
      "|    explained_variance | 0.124     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15099     |\n",
      "|    policy_loss        | 2.17e+03  |\n",
      "|    reward             | 147.24123 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 5.39e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 196       |\n",
      "|    iterations         | 15200     |\n",
      "|    time_elapsed       | 387       |\n",
      "|    total_timesteps    | 76000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.2     |\n",
      "|    explained_variance | 0.0221    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15199     |\n",
      "|    policy_loss        | 448       |\n",
      "|    reward             | 267.97275 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 7.6e+03   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 196       |\n",
      "|    iterations         | 15300     |\n",
      "|    time_elapsed       | 389       |\n",
      "|    total_timesteps    | 76500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.2     |\n",
      "|    explained_variance | 0.0087    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15299     |\n",
      "|    policy_loss        | -8.13e+03 |\n",
      "|    reward             | 229.75764 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 5.56e+04  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 196      |\n",
      "|    iterations         | 15400    |\n",
      "|    time_elapsed       | 391      |\n",
      "|    total_timesteps    | 77000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -43.1    |\n",
      "|    explained_variance | 0.0386   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15399    |\n",
      "|    policy_loss        | 2.09e+04 |\n",
      "|    reward             | 648.129  |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 3.36e+05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 196      |\n",
      "|    iterations         | 15500    |\n",
      "|    time_elapsed       | 394      |\n",
      "|    total_timesteps    | 77500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -43.2    |\n",
      "|    explained_variance | -0.00403 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15499    |\n",
      "|    policy_loss        | 2.2e+04  |\n",
      "|    reward             | 513.6885 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 4.75e+05 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 196        |\n",
      "|    iterations         | 15600      |\n",
      "|    time_elapsed       | 396        |\n",
      "|    total_timesteps    | 78000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.2      |\n",
      "|    explained_variance | 0.0868     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15599      |\n",
      "|    policy_loss        | -5.29e+03  |\n",
      "|    reward             | -19.732716 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.62e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 196       |\n",
      "|    iterations         | 15700     |\n",
      "|    time_elapsed       | 399       |\n",
      "|    total_timesteps    | 78500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.2     |\n",
      "|    explained_variance | -0.0174   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15699     |\n",
      "|    policy_loss        | -1.31e+03 |\n",
      "|    reward             | 63.535072 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 6.04e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 196        |\n",
      "|    iterations         | 15800      |\n",
      "|    time_elapsed       | 401        |\n",
      "|    total_timesteps    | 79000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.3      |\n",
      "|    explained_variance | 0.0388     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15799      |\n",
      "|    policy_loss        | -1.68e+03  |\n",
      "|    reward             | -11.834608 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 5.74e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 196        |\n",
      "|    iterations         | 15900      |\n",
      "|    time_elapsed       | 404        |\n",
      "|    total_timesteps    | 79500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.3      |\n",
      "|    explained_variance | -0.033     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15899      |\n",
      "|    policy_loss        | 1.77e+04   |\n",
      "|    reward             | -104.30988 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.92e+05   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 196        |\n",
      "|    iterations         | 16000      |\n",
      "|    time_elapsed       | 406        |\n",
      "|    total_timesteps    | 80000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.4      |\n",
      "|    explained_variance | -0.0281    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15999      |\n",
      "|    policy_loss        | 1.04e+04   |\n",
      "|    reward             | -356.75308 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.59e+05   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 196       |\n",
      "|    iterations         | 16100     |\n",
      "|    time_elapsed       | 408       |\n",
      "|    total_timesteps    | 80500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.3     |\n",
      "|    explained_variance | 0.000425  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16099     |\n",
      "|    policy_loss        | -1.63e+04 |\n",
      "|    reward             | -1329.841 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 6.56e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 196       |\n",
      "|    iterations         | 16200     |\n",
      "|    time_elapsed       | 411       |\n",
      "|    total_timesteps    | 81000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.3     |\n",
      "|    explained_variance | -0.0422   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16199     |\n",
      "|    policy_loss        | -511      |\n",
      "|    reward             | 55.703587 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 2.98e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 196       |\n",
      "|    iterations         | 16300     |\n",
      "|    time_elapsed       | 413       |\n",
      "|    total_timesteps    | 81500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.3     |\n",
      "|    explained_variance | -0.0363   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16299     |\n",
      "|    policy_loss        | 1.69e+03  |\n",
      "|    reward             | 236.05403 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 1.92e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 196       |\n",
      "|    iterations         | 16400     |\n",
      "|    time_elapsed       | 416       |\n",
      "|    total_timesteps    | 82000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.3     |\n",
      "|    explained_variance | 0.0182    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16399     |\n",
      "|    policy_loss        | -6.65e+03 |\n",
      "|    reward             | 100.90291 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 3.91e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 197       |\n",
      "|    iterations         | 16500     |\n",
      "|    time_elapsed       | 418       |\n",
      "|    total_timesteps    | 82500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.3     |\n",
      "|    explained_variance | 0.0223    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16499     |\n",
      "|    policy_loss        | -4.21e+03 |\n",
      "|    reward             | 213.10408 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 2.43e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 196        |\n",
      "|    iterations         | 16600      |\n",
      "|    time_elapsed       | 421        |\n",
      "|    total_timesteps    | 83000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.3      |\n",
      "|    explained_variance | 0.0461     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16599      |\n",
      "|    policy_loss        | 7.2e+03    |\n",
      "|    reward             | -450.70468 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.04e+05   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 196        |\n",
      "|    iterations         | 16700      |\n",
      "|    time_elapsed       | 424        |\n",
      "|    total_timesteps    | 83500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.3      |\n",
      "|    explained_variance | -0.0388    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16699      |\n",
      "|    policy_loss        | 2.44e+04   |\n",
      "|    reward             | -1846.9883 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 3.53e+05   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 196       |\n",
      "|    iterations         | 16800     |\n",
      "|    time_elapsed       | 426       |\n",
      "|    total_timesteps    | 84000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.3     |\n",
      "|    explained_variance | 0.0385    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16799     |\n",
      "|    policy_loss        | -241      |\n",
      "|    reward             | -40.72189 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 2.07e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 196       |\n",
      "|    iterations         | 16900     |\n",
      "|    time_elapsed       | 429       |\n",
      "|    total_timesteps    | 84500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.3     |\n",
      "|    explained_variance | -0.0159   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16899     |\n",
      "|    policy_loss        | -2.95e+03 |\n",
      "|    reward             | 82.34926  |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 6.18e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 196      |\n",
      "|    iterations         | 17000    |\n",
      "|    time_elapsed       | 431      |\n",
      "|    total_timesteps    | 85000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -43.3    |\n",
      "|    explained_variance | -0.0115  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16999    |\n",
      "|    policy_loss        | 3.25e+03 |\n",
      "|    reward             | 24.22524 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 1.16e+04 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 196        |\n",
      "|    iterations         | 17100      |\n",
      "|    time_elapsed       | 434        |\n",
      "|    total_timesteps    | 85500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.3      |\n",
      "|    explained_variance | -0.00703   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17099      |\n",
      "|    policy_loss        | 4.17e+03   |\n",
      "|    reward             | -193.79169 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 2.78e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 197       |\n",
      "|    iterations         | 17200     |\n",
      "|    time_elapsed       | 436       |\n",
      "|    total_timesteps    | 86000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.3     |\n",
      "|    explained_variance | 0.0102    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17199     |\n",
      "|    policy_loss        | -8.38e+03 |\n",
      "|    reward             | 30.171488 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 2.04e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 197       |\n",
      "|    iterations         | 17300     |\n",
      "|    time_elapsed       | 438       |\n",
      "|    total_timesteps    | 86500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.4     |\n",
      "|    explained_variance | 0.00973   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17299     |\n",
      "|    policy_loss        | 4.98e+03  |\n",
      "|    reward             | 432.70862 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 6.08e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 197       |\n",
      "|    iterations         | 17400     |\n",
      "|    time_elapsed       | 441       |\n",
      "|    total_timesteps    | 87000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.4     |\n",
      "|    explained_variance | -0.102    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17399     |\n",
      "|    policy_loss        | -678      |\n",
      "|    reward             | 20.646692 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 993       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 197        |\n",
      "|    iterations         | 17500      |\n",
      "|    time_elapsed       | 443        |\n",
      "|    total_timesteps    | 87500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.5      |\n",
      "|    explained_variance | 0.112      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17499      |\n",
      "|    policy_loss        | -587       |\n",
      "|    reward             | -259.02832 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 2.94e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 197       |\n",
      "|    iterations         | 17600     |\n",
      "|    time_elapsed       | 445       |\n",
      "|    total_timesteps    | 88000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.4     |\n",
      "|    explained_variance | -0.103    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17599     |\n",
      "|    policy_loss        | -4.67e+03 |\n",
      "|    reward             | -6.641378 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 1.27e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 197        |\n",
      "|    iterations         | 17700      |\n",
      "|    time_elapsed       | 448        |\n",
      "|    total_timesteps    | 88500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.4      |\n",
      "|    explained_variance | 0.0204     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17699      |\n",
      "|    policy_loss        | 6.45e+03   |\n",
      "|    reward             | -246.25372 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 3.45e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 197       |\n",
      "|    iterations         | 17800     |\n",
      "|    time_elapsed       | 450       |\n",
      "|    total_timesteps    | 89000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.4     |\n",
      "|    explained_variance | 0.00629   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17799     |\n",
      "|    policy_loss        | -7.04e+03 |\n",
      "|    reward             | 115.49137 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 9.71e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 197       |\n",
      "|    iterations         | 17900     |\n",
      "|    time_elapsed       | 452       |\n",
      "|    total_timesteps    | 89500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.4     |\n",
      "|    explained_variance | 0.0738    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17899     |\n",
      "|    policy_loss        | 1.94e+04  |\n",
      "|    reward             | 348.47516 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 2.28e+05  |\n",
      "-------------------------------------\n",
      "day: 3102, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2901213.76\n",
      "total_reward: 1901213.76\n",
      "total_cost: 102708.36\n",
      "total_trades: 52220\n",
      "Sharpe: 4.341\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 197        |\n",
      "|    iterations         | 18000      |\n",
      "|    time_elapsed       | 455        |\n",
      "|    total_timesteps    | 90000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.4      |\n",
      "|    explained_variance | -0.242     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17999      |\n",
      "|    policy_loss        | 997        |\n",
      "|    reward             | -44.113667 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 682        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 197        |\n",
      "|    iterations         | 18100      |\n",
      "|    time_elapsed       | 457        |\n",
      "|    total_timesteps    | 90500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.5      |\n",
      "|    explained_variance | 0.0865     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18099      |\n",
      "|    policy_loss        | 5.37e+03   |\n",
      "|    reward             | 118.705475 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 2.12e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 197       |\n",
      "|    iterations         | 18200     |\n",
      "|    time_elapsed       | 459       |\n",
      "|    total_timesteps    | 91000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.5     |\n",
      "|    explained_variance | 0.0797    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18199     |\n",
      "|    policy_loss        | -1.46e+03 |\n",
      "|    reward             | -79.56511 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 6.52e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 197       |\n",
      "|    iterations         | 18300     |\n",
      "|    time_elapsed       | 462       |\n",
      "|    total_timesteps    | 91500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.5     |\n",
      "|    explained_variance | 0.0189    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18299     |\n",
      "|    policy_loss        | 2.73e+03  |\n",
      "|    reward             | -131.8515 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 7.8e+03   |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 198        |\n",
      "|    iterations         | 18400      |\n",
      "|    time_elapsed       | 464        |\n",
      "|    total_timesteps    | 92000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.5      |\n",
      "|    explained_variance | -0.0275    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18399      |\n",
      "|    policy_loss        | -9.86e+03  |\n",
      "|    reward             | 107.279144 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 6.02e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 198        |\n",
      "|    iterations         | 18500      |\n",
      "|    time_elapsed       | 466        |\n",
      "|    total_timesteps    | 92500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.5      |\n",
      "|    explained_variance | -0.0336    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18499      |\n",
      "|    policy_loss        | -2.47e+03  |\n",
      "|    reward             | -7.8115845 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.55e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 198        |\n",
      "|    iterations         | 18600      |\n",
      "|    time_elapsed       | 469        |\n",
      "|    total_timesteps    | 93000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.5      |\n",
      "|    explained_variance | 0.0127     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18599      |\n",
      "|    policy_loss        | 6.02e+03   |\n",
      "|    reward             | -1020.8695 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 7.55e+04   |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 198         |\n",
      "|    iterations         | 18700       |\n",
      "|    time_elapsed       | 471         |\n",
      "|    total_timesteps    | 93500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -43.6       |\n",
      "|    explained_variance | 0.0645      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18699       |\n",
      "|    policy_loss        | 1.38e+03    |\n",
      "|    reward             | -121.539764 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 3.06e+03    |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 198        |\n",
      "|    iterations         | 18800      |\n",
      "|    time_elapsed       | 473        |\n",
      "|    total_timesteps    | 94000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.6      |\n",
      "|    explained_variance | -0.021     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18799      |\n",
      "|    policy_loss        | -983       |\n",
      "|    reward             | -231.61354 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 2.59e+03   |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 198         |\n",
      "|    iterations         | 18900       |\n",
      "|    time_elapsed       | 476         |\n",
      "|    total_timesteps    | 94500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -43.6       |\n",
      "|    explained_variance | 0.158       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18899       |\n",
      "|    policy_loss        | -3.05e+03   |\n",
      "|    reward             | -109.697235 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 5.06e+03    |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 198        |\n",
      "|    iterations         | 19000      |\n",
      "|    time_elapsed       | 478        |\n",
      "|    total_timesteps    | 95000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.6      |\n",
      "|    explained_variance | 0.00727    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18999      |\n",
      "|    policy_loss        | -5.39e+03  |\n",
      "|    reward             | -19.073547 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 5.94e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 198       |\n",
      "|    iterations         | 19100     |\n",
      "|    time_elapsed       | 480       |\n",
      "|    total_timesteps    | 95500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.6     |\n",
      "|    explained_variance | -0.00648  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19099     |\n",
      "|    policy_loss        | 1.6e+03   |\n",
      "|    reward             | -52.97886 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 9.71e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 198       |\n",
      "|    iterations         | 19200     |\n",
      "|    time_elapsed       | 483       |\n",
      "|    total_timesteps    | 96000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.6     |\n",
      "|    explained_variance | -0.0224   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19199     |\n",
      "|    policy_loss        | 3.21e+03  |\n",
      "|    reward             | -92.39296 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 2.45e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 198        |\n",
      "|    iterations         | 19300      |\n",
      "|    time_elapsed       | 485        |\n",
      "|    total_timesteps    | 96500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.6      |\n",
      "|    explained_variance | 0.166      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19299      |\n",
      "|    policy_loss        | -2.23e+03  |\n",
      "|    reward             | -19.103573 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 3.24e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 198        |\n",
      "|    iterations         | 19400      |\n",
      "|    time_elapsed       | 487        |\n",
      "|    total_timesteps    | 97000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.7      |\n",
      "|    explained_variance | 0.0567     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19399      |\n",
      "|    policy_loss        | -970       |\n",
      "|    reward             | -20.953667 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 2.77e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 198       |\n",
      "|    iterations         | 19500     |\n",
      "|    time_elapsed       | 490       |\n",
      "|    total_timesteps    | 97500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.7     |\n",
      "|    explained_variance | 0.00541   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19499     |\n",
      "|    policy_loss        | 901       |\n",
      "|    reward             | 26.328382 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 1.26e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 198       |\n",
      "|    iterations         | 19600     |\n",
      "|    time_elapsed       | 492       |\n",
      "|    total_timesteps    | 98000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.7     |\n",
      "|    explained_variance | 0.0404    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19599     |\n",
      "|    policy_loss        | 5.03e+03  |\n",
      "|    reward             | 34.205315 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 1.6e+04   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 198      |\n",
      "|    iterations         | 19700    |\n",
      "|    time_elapsed       | 495      |\n",
      "|    total_timesteps    | 98500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -43.7    |\n",
      "|    explained_variance | -0.00535 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19699    |\n",
      "|    policy_loss        | -7.1e+03 |\n",
      "|    reward             | 155.9862 |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 3.81e+04 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 198      |\n",
      "|    iterations         | 19800    |\n",
      "|    time_elapsed       | 497      |\n",
      "|    total_timesteps    | 99000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -43.7    |\n",
      "|    explained_variance | 0.00967  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19799    |\n",
      "|    policy_loss        | 7.64e+03 |\n",
      "|    reward             | 754.89   |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 4.93e+04 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 199        |\n",
      "|    iterations         | 19900      |\n",
      "|    time_elapsed       | 499        |\n",
      "|    total_timesteps    | 99500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.7      |\n",
      "|    explained_variance | 0.0592     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19899      |\n",
      "|    policy_loss        | -3.49e+03  |\n",
      "|    reward             | -12.606813 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 8.68e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 199        |\n",
      "|    iterations         | 20000      |\n",
      "|    time_elapsed       | 502        |\n",
      "|    total_timesteps    | 100000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.7      |\n",
      "|    explained_variance | 0.0258     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19999      |\n",
      "|    policy_loss        | -3.58e+03  |\n",
      "|    reward             | -120.31829 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 1.37e+04   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 199      |\n",
      "|    iterations         | 20100    |\n",
      "|    time_elapsed       | 504      |\n",
      "|    total_timesteps    | 100500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -43.7    |\n",
      "|    explained_variance | 0.024    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20099    |\n",
      "|    policy_loss        | -6.2e+03 |\n",
      "|    reward             | 69.44839 |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 4.65e+04 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 199        |\n",
      "|    iterations         | 20200      |\n",
      "|    time_elapsed       | 507        |\n",
      "|    total_timesteps    | 101000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.7      |\n",
      "|    explained_variance | 0.00359    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 20199      |\n",
      "|    policy_loss        | -4.65e+03  |\n",
      "|    reward             | -446.48907 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 2.38e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 199       |\n",
      "|    iterations         | 20300     |\n",
      "|    time_elapsed       | 509       |\n",
      "|    total_timesteps    | 101500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.7     |\n",
      "|    explained_variance | -0.0179   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20299     |\n",
      "|    policy_loss        | -1.95e+03 |\n",
      "|    reward             | 302.04102 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 7.6e+04   |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 199        |\n",
      "|    iterations         | 20400      |\n",
      "|    time_elapsed       | 511        |\n",
      "|    total_timesteps    | 102000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.7      |\n",
      "|    explained_variance | -0.015     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 20399      |\n",
      "|    policy_loss        | -1.5e+04   |\n",
      "|    reward             | -219.93372 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 1.8e+05    |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 199       |\n",
      "|    iterations         | 20500     |\n",
      "|    time_elapsed       | 513       |\n",
      "|    total_timesteps    | 102500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.7     |\n",
      "|    explained_variance | 0.0451    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20499     |\n",
      "|    policy_loss        | 110       |\n",
      "|    reward             | 63.934624 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 2.97e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 199       |\n",
      "|    iterations         | 20600     |\n",
      "|    time_elapsed       | 516       |\n",
      "|    total_timesteps    | 103000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.7     |\n",
      "|    explained_variance | 0.0212    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20599     |\n",
      "|    policy_loss        | -4.48e+03 |\n",
      "|    reward             | 145.21506 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 2.18e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 199       |\n",
      "|    iterations         | 20700     |\n",
      "|    time_elapsed       | 518       |\n",
      "|    total_timesteps    | 103500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.7     |\n",
      "|    explained_variance | -0.00896  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20699     |\n",
      "|    policy_loss        | -2e+03    |\n",
      "|    reward             | -95.69861 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 4.44e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 199      |\n",
      "|    iterations         | 20800    |\n",
      "|    time_elapsed       | 520      |\n",
      "|    total_timesteps    | 104000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -43.7    |\n",
      "|    explained_variance | 0.00826  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20799    |\n",
      "|    policy_loss        | -27.9    |\n",
      "|    reward             | 59.33872 |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 5.27e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 199       |\n",
      "|    iterations         | 20900     |\n",
      "|    time_elapsed       | 523       |\n",
      "|    total_timesteps    | 104500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.7     |\n",
      "|    explained_variance | 0.00863   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20899     |\n",
      "|    policy_loss        | 1.18e+04  |\n",
      "|    reward             | -348.1104 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 1.11e+05  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 199        |\n",
      "|    iterations         | 21000      |\n",
      "|    time_elapsed       | 525        |\n",
      "|    total_timesteps    | 105000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.8      |\n",
      "|    explained_variance | 0.00115    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 20999      |\n",
      "|    policy_loss        | 301        |\n",
      "|    reward             | -109.35043 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 3.18e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 199        |\n",
      "|    iterations         | 21100      |\n",
      "|    time_elapsed       | 527        |\n",
      "|    total_timesteps    | 105500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.8      |\n",
      "|    explained_variance | -0.00355   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 21099      |\n",
      "|    policy_loss        | -1.55e+04  |\n",
      "|    reward             | -168.53952 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 2.24e+05   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 199      |\n",
      "|    iterations         | 21200    |\n",
      "|    time_elapsed       | 530      |\n",
      "|    total_timesteps    | 106000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -43.8    |\n",
      "|    explained_variance | 0.0749   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21199    |\n",
      "|    policy_loss        | 4.36e+03 |\n",
      "|    reward             | 91.27513 |\n",
      "|    std                | 1.05     |\n",
      "|    value_loss         | 1.07e+04 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 199      |\n",
      "|    iterations         | 21300    |\n",
      "|    time_elapsed       | 532      |\n",
      "|    total_timesteps    | 106500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -43.8    |\n",
      "|    explained_variance | 0.0055   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21299    |\n",
      "|    policy_loss        | 2.15e+03 |\n",
      "|    reward             | 88.3184  |\n",
      "|    std                | 1.05     |\n",
      "|    value_loss         | 4.32e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 200       |\n",
      "|    iterations         | 21400     |\n",
      "|    time_elapsed       | 534       |\n",
      "|    total_timesteps    | 107000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.7     |\n",
      "|    explained_variance | 0.0645    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21399     |\n",
      "|    policy_loss        | -2.29e+03 |\n",
      "|    reward             | 66.31068  |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 5.3e+03   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 200       |\n",
      "|    iterations         | 21500     |\n",
      "|    time_elapsed       | 537       |\n",
      "|    total_timesteps    | 107500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.7     |\n",
      "|    explained_variance | 0.005     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21499     |\n",
      "|    policy_loss        | 1.03e+04  |\n",
      "|    reward             | 100.23035 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 8.73e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 200       |\n",
      "|    iterations         | 21600     |\n",
      "|    time_elapsed       | 539       |\n",
      "|    total_timesteps    | 108000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.8     |\n",
      "|    explained_variance | 0.0236    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21599     |\n",
      "|    policy_loss        | -2.58e+03 |\n",
      "|    reward             | -16.15346 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 2.92e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 200       |\n",
      "|    iterations         | 21700     |\n",
      "|    time_elapsed       | 541       |\n",
      "|    total_timesteps    | 108500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.8     |\n",
      "|    explained_variance | 0.00152   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21699     |\n",
      "|    policy_loss        | 2.62e+03  |\n",
      "|    reward             | 89.559715 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 4.85e+04  |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 200         |\n",
      "|    iterations         | 21800       |\n",
      "|    time_elapsed       | 544         |\n",
      "|    total_timesteps    | 109000      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -43.9       |\n",
      "|    explained_variance | 0.131       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 21799       |\n",
      "|    policy_loss        | 6.86e+03    |\n",
      "|    reward             | -0.28701156 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 2.61e+04    |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 200       |\n",
      "|    iterations         | 21900     |\n",
      "|    time_elapsed       | 546       |\n",
      "|    total_timesteps    | 109500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.9     |\n",
      "|    explained_variance | -0.0204   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21899     |\n",
      "|    policy_loss        | -8.57e+03 |\n",
      "|    reward             | 62.540962 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 3.84e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 200       |\n",
      "|    iterations         | 22000     |\n",
      "|    time_elapsed       | 548       |\n",
      "|    total_timesteps    | 110000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.9     |\n",
      "|    explained_variance | -0.0128   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21999     |\n",
      "|    policy_loss        | 222       |\n",
      "|    reward             | 53.392826 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 1.13e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 200       |\n",
      "|    iterations         | 22100     |\n",
      "|    time_elapsed       | 551       |\n",
      "|    total_timesteps    | 110500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44       |\n",
      "|    explained_variance | -0.0125   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22099     |\n",
      "|    policy_loss        | 1.72e+04  |\n",
      "|    reward             | 46.792126 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 1.55e+05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 200      |\n",
      "|    iterations         | 22200    |\n",
      "|    time_elapsed       | 553      |\n",
      "|    total_timesteps    | 111000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -44      |\n",
      "|    explained_variance | 0.00784  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22199    |\n",
      "|    policy_loss        | 7.69e+03 |\n",
      "|    reward             | 168.728  |\n",
      "|    std                | 1.05     |\n",
      "|    value_loss         | 1.46e+05 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 200        |\n",
      "|    iterations         | 22300      |\n",
      "|    time_elapsed       | 556        |\n",
      "|    total_timesteps    | 111500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44        |\n",
      "|    explained_variance | 0.00416    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 22299      |\n",
      "|    policy_loss        | 3.04e+04   |\n",
      "|    reward             | -18.791208 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 5.58e+05   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 200        |\n",
      "|    iterations         | 22400      |\n",
      "|    time_elapsed       | 558        |\n",
      "|    total_timesteps    | 112000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44        |\n",
      "|    explained_variance | 0.0139     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 22399      |\n",
      "|    policy_loss        | 1.11e+03   |\n",
      "|    reward             | -105.06439 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 782        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 200       |\n",
      "|    iterations         | 22500     |\n",
      "|    time_elapsed       | 560       |\n",
      "|    total_timesteps    | 112500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44       |\n",
      "|    explained_variance | -0.00721  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22499     |\n",
      "|    policy_loss        | -1.64e+03 |\n",
      "|    reward             | 98.66697  |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 2.67e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 200       |\n",
      "|    iterations         | 22600     |\n",
      "|    time_elapsed       | 563       |\n",
      "|    total_timesteps    | 113000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44       |\n",
      "|    explained_variance | 0.00487   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22599     |\n",
      "|    policy_loss        | 1.04e+04  |\n",
      "|    reward             | 183.57062 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 8.46e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 200       |\n",
      "|    iterations         | 22700     |\n",
      "|    time_elapsed       | 565       |\n",
      "|    total_timesteps    | 113500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.9     |\n",
      "|    explained_variance | 0.0169    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22699     |\n",
      "|    policy_loss        | 6.9e+03   |\n",
      "|    reward             | 649.29755 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 3.46e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 200        |\n",
      "|    iterations         | 22800      |\n",
      "|    time_elapsed       | 567        |\n",
      "|    total_timesteps    | 114000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44        |\n",
      "|    explained_variance | 0.017      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 22799      |\n",
      "|    policy_loss        | 1.25e+04   |\n",
      "|    reward             | -164.93105 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 1.25e+05   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 200       |\n",
      "|    iterations         | 22900     |\n",
      "|    time_elapsed       | 570       |\n",
      "|    total_timesteps    | 114500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44       |\n",
      "|    explained_variance | -0.000822 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22899     |\n",
      "|    policy_loss        | -5.45e+03 |\n",
      "|    reward             | 457.1093  |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 1.36e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 200       |\n",
      "|    iterations         | 23000     |\n",
      "|    time_elapsed       | 572       |\n",
      "|    total_timesteps    | 115000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.9     |\n",
      "|    explained_variance | 0.00896   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22999     |\n",
      "|    policy_loss        | -1.17e+03 |\n",
      "|    reward             | -9.217122 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 4.45e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 200       |\n",
      "|    iterations         | 23100     |\n",
      "|    time_elapsed       | 574       |\n",
      "|    total_timesteps    | 115500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.9     |\n",
      "|    explained_variance | 0.00961   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23099     |\n",
      "|    policy_loss        | 57.7      |\n",
      "|    reward             | -82.26627 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 4.42e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 201       |\n",
      "|    iterations         | 23200     |\n",
      "|    time_elapsed       | 576       |\n",
      "|    total_timesteps    | 116000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44       |\n",
      "|    explained_variance | 0.00518   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23199     |\n",
      "|    policy_loss        | -5e+03    |\n",
      "|    reward             | 297.64078 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 3.37e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 201       |\n",
      "|    iterations         | 23300     |\n",
      "|    time_elapsed       | 579       |\n",
      "|    total_timesteps    | 116500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44       |\n",
      "|    explained_variance | 0.0215    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23299     |\n",
      "|    policy_loss        | 1.78e+03  |\n",
      "|    reward             | 3.3295639 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 2.43e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 201       |\n",
      "|    iterations         | 23400     |\n",
      "|    time_elapsed       | 581       |\n",
      "|    total_timesteps    | 117000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44       |\n",
      "|    explained_variance | -0.00521  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23399     |\n",
      "|    policy_loss        | -1.14e+04 |\n",
      "|    reward             | -32.70197 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 7.75e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 201       |\n",
      "|    iterations         | 23500     |\n",
      "|    time_elapsed       | 583       |\n",
      "|    total_timesteps    | 117500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44       |\n",
      "|    explained_variance | -0.00387  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23499     |\n",
      "|    policy_loss        | -2.04e+04 |\n",
      "|    reward             | 51.118385 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 3.33e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 201       |\n",
      "|    iterations         | 23600     |\n",
      "|    time_elapsed       | 586       |\n",
      "|    total_timesteps    | 118000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44       |\n",
      "|    explained_variance | 0.0121    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23599     |\n",
      "|    policy_loss        | 821       |\n",
      "|    reward             | 42.504566 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 1.35e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 201      |\n",
      "|    iterations         | 23700    |\n",
      "|    time_elapsed       | 588      |\n",
      "|    total_timesteps    | 118500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -44      |\n",
      "|    explained_variance | 0.0208   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23699    |\n",
      "|    policy_loss        | 4.91e+03 |\n",
      "|    reward             | 180.5249 |\n",
      "|    std                | 1.06     |\n",
      "|    value_loss         | 1.83e+04 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 201       |\n",
      "|    iterations         | 23800     |\n",
      "|    time_elapsed       | 590       |\n",
      "|    total_timesteps    | 119000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.1     |\n",
      "|    explained_variance | -0.0811   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23799     |\n",
      "|    policy_loss        | -3.95e+03 |\n",
      "|    reward             | 10.655545 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 1.03e+04  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 201      |\n",
      "|    iterations         | 23900    |\n",
      "|    time_elapsed       | 593      |\n",
      "|    total_timesteps    | 119500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -44      |\n",
      "|    explained_variance | -0.088   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23899    |\n",
      "|    policy_loss        | 668      |\n",
      "|    reward             | 1.65238  |\n",
      "|    std                | 1.06     |\n",
      "|    value_loss         | 2.59e+03 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 201        |\n",
      "|    iterations         | 24000      |\n",
      "|    time_elapsed       | 595        |\n",
      "|    total_timesteps    | 120000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.1      |\n",
      "|    explained_variance | -0.0248    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 23999      |\n",
      "|    policy_loss        | -2.62e+03  |\n",
      "|    reward             | -365.32938 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 1.05e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 201        |\n",
      "|    iterations         | 24100      |\n",
      "|    time_elapsed       | 597        |\n",
      "|    total_timesteps    | 120500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.1      |\n",
      "|    explained_variance | -0.00493   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 24099      |\n",
      "|    policy_loss        | 7.26e+03   |\n",
      "|    reward             | -1.7861104 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 8.45e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 201       |\n",
      "|    iterations         | 24200     |\n",
      "|    time_elapsed       | 600       |\n",
      "|    total_timesteps    | 121000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44       |\n",
      "|    explained_variance | 0.00576   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 24199     |\n",
      "|    policy_loss        | -1.94e+04 |\n",
      "|    reward             | 1837.4907 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 3.15e+05  |\n",
      "-------------------------------------\n",
      "day: 3102, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3718587.53\n",
      "total_reward: 2718587.53\n",
      "total_cost: 93813.23\n",
      "total_trades: 41890\n",
      "Sharpe: 3.911\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 201       |\n",
      "|    iterations         | 24300     |\n",
      "|    time_elapsed       | 602       |\n",
      "|    total_timesteps    | 121500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44       |\n",
      "|    explained_variance | 0.00504   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 24299     |\n",
      "|    policy_loss        | 373       |\n",
      "|    reward             | 15.175915 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 2.4e+03   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 201       |\n",
      "|    iterations         | 24400     |\n",
      "|    time_elapsed       | 604       |\n",
      "|    total_timesteps    | 122000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44       |\n",
      "|    explained_variance | -0.00298  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 24399     |\n",
      "|    policy_loss        | 3.34e+03  |\n",
      "|    reward             | -19.73285 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 8.89e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 201       |\n",
      "|    iterations         | 24500     |\n",
      "|    time_elapsed       | 607       |\n",
      "|    total_timesteps    | 122500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44       |\n",
      "|    explained_variance | -0.00591  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 24499     |\n",
      "|    policy_loss        | 3.05e+03  |\n",
      "|    reward             | 350.21872 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 6.71e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 201       |\n",
      "|    iterations         | 24600     |\n",
      "|    time_elapsed       | 609       |\n",
      "|    total_timesteps    | 123000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.1     |\n",
      "|    explained_variance | 0.0449    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 24599     |\n",
      "|    policy_loss        | -6.61e+03 |\n",
      "|    reward             | 668.8727  |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 2.45e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 201       |\n",
      "|    iterations         | 24700     |\n",
      "|    time_elapsed       | 611       |\n",
      "|    total_timesteps    | 123500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44       |\n",
      "|    explained_variance | 0.0033    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 24699     |\n",
      "|    policy_loss        | 3.57e+03  |\n",
      "|    reward             | -65.28042 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 9.91e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 201       |\n",
      "|    iterations         | 24800     |\n",
      "|    time_elapsed       | 614       |\n",
      "|    total_timesteps    | 124000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44       |\n",
      "|    explained_variance | 0.0102    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 24799     |\n",
      "|    policy_loss        | 1.2e+04   |\n",
      "|    reward             | 1080.6514 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 1.13e+05  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 201        |\n",
      "|    iterations         | 24900      |\n",
      "|    time_elapsed       | 616        |\n",
      "|    total_timesteps    | 124500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.1      |\n",
      "|    explained_variance | 0.0187     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 24899      |\n",
      "|    policy_loss        | -1.24e+03  |\n",
      "|    reward             | -135.60823 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 5.94e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 201       |\n",
      "|    iterations         | 25000     |\n",
      "|    time_elapsed       | 618       |\n",
      "|    total_timesteps    | 125000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.2     |\n",
      "|    explained_variance | 0.0331    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 24999     |\n",
      "|    policy_loss        | -2.57e+03 |\n",
      "|    reward             | -68.78598 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 6.93e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 202       |\n",
      "|    iterations         | 25100     |\n",
      "|    time_elapsed       | 621       |\n",
      "|    total_timesteps    | 125500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.2     |\n",
      "|    explained_variance | -0.0106   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 25099     |\n",
      "|    policy_loss        | 5.3e+03   |\n",
      "|    reward             | 177.62924 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 2.14e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 202        |\n",
      "|    iterations         | 25200      |\n",
      "|    time_elapsed       | 623        |\n",
      "|    total_timesteps    | 126000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.2      |\n",
      "|    explained_variance | 0.0165     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 25199      |\n",
      "|    policy_loss        | -7.14e+03  |\n",
      "|    reward             | -200.10469 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 3.66e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 201       |\n",
      "|    iterations         | 25300     |\n",
      "|    time_elapsed       | 626       |\n",
      "|    total_timesteps    | 126500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.2     |\n",
      "|    explained_variance | 0.0284    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 25299     |\n",
      "|    policy_loss        | 8.06e+03  |\n",
      "|    reward             | 275.55823 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 3.67e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 201       |\n",
      "|    iterations         | 25400     |\n",
      "|    time_elapsed       | 628       |\n",
      "|    total_timesteps    | 127000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.2     |\n",
      "|    explained_variance | -0.0114   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 25399     |\n",
      "|    policy_loss        | 1.84e+04  |\n",
      "|    reward             | 102.83598 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 2.01e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 201       |\n",
      "|    iterations         | 25500     |\n",
      "|    time_elapsed       | 631       |\n",
      "|    total_timesteps    | 127500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.3     |\n",
      "|    explained_variance | 0.0446    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 25499     |\n",
      "|    policy_loss        | -1.62e+03 |\n",
      "|    reward             | -50.51059 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 4.09e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 202       |\n",
      "|    iterations         | 25600     |\n",
      "|    time_elapsed       | 633       |\n",
      "|    total_timesteps    | 128000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.3     |\n",
      "|    explained_variance | -0.0288   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 25599     |\n",
      "|    policy_loss        | -4.29e+03 |\n",
      "|    reward             | 196.21808 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 2.13e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 202       |\n",
      "|    iterations         | 25700     |\n",
      "|    time_elapsed       | 635       |\n",
      "|    total_timesteps    | 128500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.2     |\n",
      "|    explained_variance | 0.0253    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 25699     |\n",
      "|    policy_loss        | 3.19e+03  |\n",
      "|    reward             | 52.579998 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 2.18e+04  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 202      |\n",
      "|    iterations         | 25800    |\n",
      "|    time_elapsed       | 638      |\n",
      "|    total_timesteps    | 129000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -44.2    |\n",
      "|    explained_variance | -0.0154  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25799    |\n",
      "|    policy_loss        | 4.74e+03 |\n",
      "|    reward             | 537.127  |\n",
      "|    std                | 1.06     |\n",
      "|    value_loss         | 4.3e+04  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 202       |\n",
      "|    iterations         | 25900     |\n",
      "|    time_elapsed       | 640       |\n",
      "|    total_timesteps    | 129500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.2     |\n",
      "|    explained_variance | 0.0261    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 25899     |\n",
      "|    policy_loss        | 3.62e+04  |\n",
      "|    reward             | 248.31789 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 7.53e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 202       |\n",
      "|    iterations         | 26000     |\n",
      "|    time_elapsed       | 642       |\n",
      "|    total_timesteps    | 130000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.2     |\n",
      "|    explained_variance | -3.39e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 25999     |\n",
      "|    policy_loss        | 4.82e+03  |\n",
      "|    reward             | 649.612   |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 4.49e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 202       |\n",
      "|    iterations         | 26100     |\n",
      "|    time_elapsed       | 645       |\n",
      "|    total_timesteps    | 130500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.2     |\n",
      "|    explained_variance | -0.0718   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 26099     |\n",
      "|    policy_loss        | 448       |\n",
      "|    reward             | 17.527245 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 485       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 202        |\n",
      "|    iterations         | 26200      |\n",
      "|    time_elapsed       | 647        |\n",
      "|    total_timesteps    | 131000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.2      |\n",
      "|    explained_variance | 0.0123     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 26199      |\n",
      "|    policy_loss        | -1.92e+03  |\n",
      "|    reward             | -138.56097 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 4.16e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 202        |\n",
      "|    iterations         | 26300      |\n",
      "|    time_elapsed       | 649        |\n",
      "|    total_timesteps    | 131500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.2      |\n",
      "|    explained_variance | 0.00871    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 26299      |\n",
      "|    policy_loss        | 2.66e+03   |\n",
      "|    reward             | -253.50426 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 9.87e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 202       |\n",
      "|    iterations         | 26400     |\n",
      "|    time_elapsed       | 652       |\n",
      "|    total_timesteps    | 132000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.3     |\n",
      "|    explained_variance | -0.0113   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 26399     |\n",
      "|    policy_loss        | -5.97e+03 |\n",
      "|    reward             | 198.27995 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 3.03e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 202       |\n",
      "|    iterations         | 26500     |\n",
      "|    time_elapsed       | 654       |\n",
      "|    total_timesteps    | 132500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.3     |\n",
      "|    explained_variance | 0.0371    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 26499     |\n",
      "|    policy_loss        | -4.65e+03 |\n",
      "|    reward             | 147.86845 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 7.34e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 202        |\n",
      "|    iterations         | 26600      |\n",
      "|    time_elapsed       | 656        |\n",
      "|    total_timesteps    | 133000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.2      |\n",
      "|    explained_variance | 0.0044     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 26599      |\n",
      "|    policy_loss        | -2.05e+04  |\n",
      "|    reward             | -42.215015 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 3.39e+05   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 202       |\n",
      "|    iterations         | 26700     |\n",
      "|    time_elapsed       | 659       |\n",
      "|    total_timesteps    | 133500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.3     |\n",
      "|    explained_variance | -0.218    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 26699     |\n",
      "|    policy_loss        | -1.15e+03 |\n",
      "|    reward             | 29.681944 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 1.2e+03   |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 202        |\n",
      "|    iterations         | 26800      |\n",
      "|    time_elapsed       | 661        |\n",
      "|    total_timesteps    | 134000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.3      |\n",
      "|    explained_variance | -0.0258    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 26799      |\n",
      "|    policy_loss        | 1.13e+04   |\n",
      "|    reward             | -200.72998 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 7.48e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 202       |\n",
      "|    iterations         | 26900     |\n",
      "|    time_elapsed       | 663       |\n",
      "|    total_timesteps    | 134500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.3     |\n",
      "|    explained_variance | -0.00669  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 26899     |\n",
      "|    policy_loss        | 5.3e+03   |\n",
      "|    reward             | 105.12737 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 3.59e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 202       |\n",
      "|    iterations         | 27000     |\n",
      "|    time_elapsed       | 666       |\n",
      "|    total_timesteps    | 135000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.4     |\n",
      "|    explained_variance | 0.0296    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 26999     |\n",
      "|    policy_loss        | -1.35e+04 |\n",
      "|    reward             | 133.67938 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 1.32e+05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 202      |\n",
      "|    iterations         | 27100    |\n",
      "|    time_elapsed       | 668      |\n",
      "|    total_timesteps    | 135500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -44.4    |\n",
      "|    explained_variance | 0.00901  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27099    |\n",
      "|    policy_loss        | -926     |\n",
      "|    reward             | 232.8738 |\n",
      "|    std                | 1.07     |\n",
      "|    value_loss         | 1.3e+05  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 202       |\n",
      "|    iterations         | 27200     |\n",
      "|    time_elapsed       | 670       |\n",
      "|    total_timesteps    | 136000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.4     |\n",
      "|    explained_variance | 0.0133    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 27199     |\n",
      "|    policy_loss        | -3.91e+04 |\n",
      "|    reward             | -548.5483 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 9.32e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 202       |\n",
      "|    iterations         | 27300     |\n",
      "|    time_elapsed       | 673       |\n",
      "|    total_timesteps    | 136500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.5     |\n",
      "|    explained_variance | 0.0105    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 27299     |\n",
      "|    policy_loss        | -1.68e+04 |\n",
      "|    reward             | -564.4968 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 3.73e+05  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 202        |\n",
      "|    iterations         | 27400      |\n",
      "|    time_elapsed       | 675        |\n",
      "|    total_timesteps    | 137000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.4      |\n",
      "|    explained_variance | -0.0223    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 27399      |\n",
      "|    policy_loss        | 1.32e+03   |\n",
      "|    reward             | -275.75244 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 2.28e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 202        |\n",
      "|    iterations         | 27500      |\n",
      "|    time_elapsed       | 677        |\n",
      "|    total_timesteps    | 137500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.4      |\n",
      "|    explained_variance | -0.126     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 27499      |\n",
      "|    policy_loss        | -825       |\n",
      "|    reward             | -19.866896 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 6.71e+03   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 202      |\n",
      "|    iterations         | 27600    |\n",
      "|    time_elapsed       | 680      |\n",
      "|    total_timesteps    | 138000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -44.4    |\n",
      "|    explained_variance | 0.0243   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27599    |\n",
      "|    policy_loss        | 2.61e+03 |\n",
      "|    reward             | 376.1953 |\n",
      "|    std                | 1.07     |\n",
      "|    value_loss         | 1.44e+04 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 202       |\n",
      "|    iterations         | 27700     |\n",
      "|    time_elapsed       | 682       |\n",
      "|    total_timesteps    | 138500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.4     |\n",
      "|    explained_variance | 0.0191    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 27699     |\n",
      "|    policy_loss        | 2.41e+04  |\n",
      "|    reward             | 76.798485 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 3.31e+05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 203      |\n",
      "|    iterations         | 27800    |\n",
      "|    time_elapsed       | 684      |\n",
      "|    total_timesteps    | 139000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -44.4    |\n",
      "|    explained_variance | -0.0233  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27799    |\n",
      "|    policy_loss        | 9e+03    |\n",
      "|    reward             | 29.87411 |\n",
      "|    std                | 1.07     |\n",
      "|    value_loss         | 8.33e+04 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 203        |\n",
      "|    iterations         | 27900      |\n",
      "|    time_elapsed       | 687        |\n",
      "|    total_timesteps    | 139500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.4      |\n",
      "|    explained_variance | 0.027      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 27899      |\n",
      "|    policy_loss        | -1.06e+04  |\n",
      "|    reward             | -987.93713 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 1.64e+05   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 203       |\n",
      "|    iterations         | 28000     |\n",
      "|    time_elapsed       | 689       |\n",
      "|    total_timesteps    | 140000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.5     |\n",
      "|    explained_variance | -0.922    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 27999     |\n",
      "|    policy_loss        | -1.16e+03 |\n",
      "|    reward             | -99.23008 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 673       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 203       |\n",
      "|    iterations         | 28100     |\n",
      "|    time_elapsed       | 691       |\n",
      "|    total_timesteps    | 140500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.5     |\n",
      "|    explained_variance | 0.0932    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 28099     |\n",
      "|    policy_loss        | -2.54e+03 |\n",
      "|    reward             | -2.521202 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 4.36e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 203      |\n",
      "|    iterations         | 28200    |\n",
      "|    time_elapsed       | 694      |\n",
      "|    total_timesteps    | 141000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -44.5    |\n",
      "|    explained_variance | 0.0139   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28199    |\n",
      "|    policy_loss        | 383      |\n",
      "|    reward             | 350.6993 |\n",
      "|    std                | 1.07     |\n",
      "|    value_loss         | 7.28e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 203       |\n",
      "|    iterations         | 28300     |\n",
      "|    time_elapsed       | 696       |\n",
      "|    total_timesteps    | 141500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.5     |\n",
      "|    explained_variance | -0.0196   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 28299     |\n",
      "|    policy_loss        | -2.62e+03 |\n",
      "|    reward             | 85.82204  |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 1.28e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 203        |\n",
      "|    iterations         | 28400      |\n",
      "|    time_elapsed       | 698        |\n",
      "|    total_timesteps    | 142000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.5      |\n",
      "|    explained_variance | 0.0272     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 28399      |\n",
      "|    policy_loss        | 1.2e+04    |\n",
      "|    reward             | -39.465374 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 1.06e+05   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 203        |\n",
      "|    iterations         | 28500      |\n",
      "|    time_elapsed       | 700        |\n",
      "|    total_timesteps    | 142500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.6      |\n",
      "|    explained_variance | -0.0105    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 28499      |\n",
      "|    policy_loss        | -1.55e+04  |\n",
      "|    reward             | -14.049003 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 2.55e+05   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 203       |\n",
      "|    iterations         | 28600     |\n",
      "|    time_elapsed       | 703       |\n",
      "|    total_timesteps    | 143000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.6     |\n",
      "|    explained_variance | 0.0909    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 28599     |\n",
      "|    policy_loss        | -4.28e+03 |\n",
      "|    reward             | -69.26022 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 9.12e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 203        |\n",
      "|    iterations         | 28700      |\n",
      "|    time_elapsed       | 705        |\n",
      "|    total_timesteps    | 143500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.6      |\n",
      "|    explained_variance | 0.0232     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 28699      |\n",
      "|    policy_loss        | -5.91e+03  |\n",
      "|    reward             | -186.71933 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 1.92e+04   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 203      |\n",
      "|    iterations         | 28800    |\n",
      "|    time_elapsed       | 707      |\n",
      "|    total_timesteps    | 144000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -44.5    |\n",
      "|    explained_variance | -0.0021  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28799    |\n",
      "|    policy_loss        | 1.57e+03 |\n",
      "|    reward             | 72.66225 |\n",
      "|    std                | 1.07     |\n",
      "|    value_loss         | 4.09e+03 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 203        |\n",
      "|    iterations         | 28900      |\n",
      "|    time_elapsed       | 710        |\n",
      "|    total_timesteps    | 144500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.6      |\n",
      "|    explained_variance | 0.0161     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 28899      |\n",
      "|    policy_loss        | -507       |\n",
      "|    reward             | -393.71194 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 1.29e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 203        |\n",
      "|    iterations         | 29000      |\n",
      "|    time_elapsed       | 712        |\n",
      "|    total_timesteps    | 145000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.6      |\n",
      "|    explained_variance | 0.00769    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 28999      |\n",
      "|    policy_loss        | -3.93e+03  |\n",
      "|    reward             | -180.86667 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 2.43e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 203       |\n",
      "|    iterations         | 29100     |\n",
      "|    time_elapsed       | 714       |\n",
      "|    total_timesteps    | 145500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.7     |\n",
      "|    explained_variance | 0.037     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 29099     |\n",
      "|    policy_loss        | -1.13e+04 |\n",
      "|    reward             | 396.2786  |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 9.51e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 203        |\n",
      "|    iterations         | 29200      |\n",
      "|    time_elapsed       | 717        |\n",
      "|    total_timesteps    | 146000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.7      |\n",
      "|    explained_variance | 0.0647     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 29199      |\n",
      "|    policy_loss        | -1.85e+03  |\n",
      "|    reward             | -1.5771863 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 3.07e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 203       |\n",
      "|    iterations         | 29300     |\n",
      "|    time_elapsed       | 719       |\n",
      "|    total_timesteps    | 146500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.6     |\n",
      "|    explained_variance | -0.00684  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 29299     |\n",
      "|    policy_loss        | -1.58e+03 |\n",
      "|    reward             | 17.81525  |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 9.73e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 203       |\n",
      "|    iterations         | 29400     |\n",
      "|    time_elapsed       | 721       |\n",
      "|    total_timesteps    | 147000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.7     |\n",
      "|    explained_variance | 0.0377    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 29399     |\n",
      "|    policy_loss        | 3.95e+03  |\n",
      "|    reward             | 4.1187162 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 1.19e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 203       |\n",
      "|    iterations         | 29500     |\n",
      "|    time_elapsed       | 724       |\n",
      "|    total_timesteps    | 147500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.7     |\n",
      "|    explained_variance | 0.0945    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 29499     |\n",
      "|    policy_loss        | -2.83e+03 |\n",
      "|    reward             | 29.409779 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 1.09e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 203       |\n",
      "|    iterations         | 29600     |\n",
      "|    time_elapsed       | 726       |\n",
      "|    total_timesteps    | 148000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.7     |\n",
      "|    explained_variance | 0.0645    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 29599     |\n",
      "|    policy_loss        | 1.35e+03  |\n",
      "|    reward             | 184.15953 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 3.06e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 203       |\n",
      "|    iterations         | 29700     |\n",
      "|    time_elapsed       | 728       |\n",
      "|    total_timesteps    | 148500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.7     |\n",
      "|    explained_variance | 0.0123    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 29699     |\n",
      "|    policy_loss        | -9.1e+03  |\n",
      "|    reward             | 322.95374 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 5.4e+04   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 203      |\n",
      "|    iterations         | 29800    |\n",
      "|    time_elapsed       | 731      |\n",
      "|    total_timesteps    | 149000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -44.7    |\n",
      "|    explained_variance | -0.158   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29799    |\n",
      "|    policy_loss        | -1.1e+03 |\n",
      "|    reward             | 22.87842 |\n",
      "|    std                | 1.08     |\n",
      "|    value_loss         | 1.61e+03 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 203        |\n",
      "|    iterations         | 29900      |\n",
      "|    time_elapsed       | 733        |\n",
      "|    total_timesteps    | 149500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.7      |\n",
      "|    explained_variance | 0.0622     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 29899      |\n",
      "|    policy_loss        | 3.15e+03   |\n",
      "|    reward             | -32.500805 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 8.63e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 203       |\n",
      "|    iterations         | 30000     |\n",
      "|    time_elapsed       | 735       |\n",
      "|    total_timesteps    | 150000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.8     |\n",
      "|    explained_variance | -0.0261   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 29999     |\n",
      "|    policy_loss        | 4.7e+03   |\n",
      "|    reward             | -150.4766 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 1.22e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 203       |\n",
      "|    iterations         | 30100     |\n",
      "|    time_elapsed       | 738       |\n",
      "|    total_timesteps    | 150500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.8     |\n",
      "|    explained_variance | 0.0464    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 30099     |\n",
      "|    policy_loss        | -2.28e+03 |\n",
      "|    reward             | 210.27475 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 2e+04     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 203       |\n",
      "|    iterations         | 30200     |\n",
      "|    time_elapsed       | 740       |\n",
      "|    total_timesteps    | 151000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.7     |\n",
      "|    explained_variance | 0.0167    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 30199     |\n",
      "|    policy_loss        | -9.94e+03 |\n",
      "|    reward             | 96.94712  |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 9.35e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 203        |\n",
      "|    iterations         | 30300      |\n",
      "|    time_elapsed       | 743        |\n",
      "|    total_timesteps    | 151500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.7      |\n",
      "|    explained_variance | 0.0145     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 30299      |\n",
      "|    policy_loss        | 8.56e+03   |\n",
      "|    reward             | -791.26306 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 8.17e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 203        |\n",
      "|    iterations         | 30400      |\n",
      "|    time_elapsed       | 745        |\n",
      "|    total_timesteps    | 152000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.7      |\n",
      "|    explained_variance | -0.101     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 30399      |\n",
      "|    policy_loss        | -7.01e+03  |\n",
      "|    reward             | -534.48395 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 5.49e+04   |\n",
      "--------------------------------------\n",
      "day: 3102, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5792385.39\n",
      "total_reward: 4792385.39\n",
      "total_cost: 47035.91\n",
      "total_trades: 50869\n",
      "Sharpe: 3.568\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 203        |\n",
      "|    iterations         | 30500      |\n",
      "|    time_elapsed       | 747        |\n",
      "|    total_timesteps    | 152500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.6      |\n",
      "|    explained_variance | -0.0139    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 30499      |\n",
      "|    policy_loss        | -4.92e+03  |\n",
      "|    reward             | -66.536385 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 1.9e+04    |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 203       |\n",
      "|    iterations         | 30600     |\n",
      "|    time_elapsed       | 750       |\n",
      "|    total_timesteps    | 153000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.6     |\n",
      "|    explained_variance | 0.0254    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 30599     |\n",
      "|    policy_loss        | 1.83e+03  |\n",
      "|    reward             | -54.20141 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 4.97e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 203       |\n",
      "|    iterations         | 30700     |\n",
      "|    time_elapsed       | 752       |\n",
      "|    total_timesteps    | 153500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.7     |\n",
      "|    explained_variance | 0.00486   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 30699     |\n",
      "|    policy_loss        | -2.73e+03 |\n",
      "|    reward             | -86.16931 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 1.15e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 204       |\n",
      "|    iterations         | 30800     |\n",
      "|    time_elapsed       | 754       |\n",
      "|    total_timesteps    | 154000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.8     |\n",
      "|    explained_variance | 0.0452    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 30799     |\n",
      "|    policy_loss        | -5.37e+03 |\n",
      "|    reward             | 134.87119 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 1.98e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 204        |\n",
      "|    iterations         | 30900      |\n",
      "|    time_elapsed       | 757        |\n",
      "|    total_timesteps    | 154500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.8      |\n",
      "|    explained_variance | -8.62e-05  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 30899      |\n",
      "|    policy_loss        | 1.24e+03   |\n",
      "|    reward             | 116.914986 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 1.65e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 204        |\n",
      "|    iterations         | 31000      |\n",
      "|    time_elapsed       | 759        |\n",
      "|    total_timesteps    | 155000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.8      |\n",
      "|    explained_variance | 0.00264    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 30999      |\n",
      "|    policy_loss        | -1.22e+04  |\n",
      "|    reward             | -267.81775 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 1.19e+05   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 204       |\n",
      "|    iterations         | 31100     |\n",
      "|    time_elapsed       | 761       |\n",
      "|    total_timesteps    | 155500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.8     |\n",
      "|    explained_variance | 0.393     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 31099     |\n",
      "|    policy_loss        | -1.66e+03 |\n",
      "|    reward             | 39.178875 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 2.33e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 204       |\n",
      "|    iterations         | 31200     |\n",
      "|    time_elapsed       | 764       |\n",
      "|    total_timesteps    | 156000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.8     |\n",
      "|    explained_variance | -0.122    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 31199     |\n",
      "|    policy_loss        | -1.41e+03 |\n",
      "|    reward             | 25.933353 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 3.17e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 204       |\n",
      "|    iterations         | 31300     |\n",
      "|    time_elapsed       | 766       |\n",
      "|    total_timesteps    | 156500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.9     |\n",
      "|    explained_variance | -0.115    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 31299     |\n",
      "|    policy_loss        | -843      |\n",
      "|    reward             | -65.09269 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 4.48e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 204       |\n",
      "|    iterations         | 31400     |\n",
      "|    time_elapsed       | 768       |\n",
      "|    total_timesteps    | 157000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.9     |\n",
      "|    explained_variance | 0.0161    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 31399     |\n",
      "|    policy_loss        | 2.7e+03   |\n",
      "|    reward             | 6.8752365 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 2.03e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 204       |\n",
      "|    iterations         | 31500     |\n",
      "|    time_elapsed       | 771       |\n",
      "|    total_timesteps    | 157500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.9     |\n",
      "|    explained_variance | -0.0643   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 31499     |\n",
      "|    policy_loss        | -3.55e+03 |\n",
      "|    reward             | -9.127544 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 1.84e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 204       |\n",
      "|    iterations         | 31600     |\n",
      "|    time_elapsed       | 773       |\n",
      "|    total_timesteps    | 158000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.8     |\n",
      "|    explained_variance | 0.004     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 31599     |\n",
      "|    policy_loss        | 5.71e+03  |\n",
      "|    reward             | 128.24048 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 1.14e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 204       |\n",
      "|    iterations         | 31700     |\n",
      "|    time_elapsed       | 775       |\n",
      "|    total_timesteps    | 158500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.9     |\n",
      "|    explained_variance | -0.0299   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 31699     |\n",
      "|    policy_loss        | 2.09e+03  |\n",
      "|    reward             | 58.062557 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 3.36e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 204        |\n",
      "|    iterations         | 31800      |\n",
      "|    time_elapsed       | 778        |\n",
      "|    total_timesteps    | 159000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.8      |\n",
      "|    explained_variance | -0.0922    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 31799      |\n",
      "|    policy_loss        | -3.24e+03  |\n",
      "|    reward             | -71.381516 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 9.57e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 204        |\n",
      "|    iterations         | 31900      |\n",
      "|    time_elapsed       | 780        |\n",
      "|    total_timesteps    | 159500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.9      |\n",
      "|    explained_variance | 0.0184     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 31899      |\n",
      "|    policy_loss        | 230        |\n",
      "|    reward             | -102.13908 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 1.01e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 204        |\n",
      "|    iterations         | 32000      |\n",
      "|    time_elapsed       | 782        |\n",
      "|    total_timesteps    | 160000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.8      |\n",
      "|    explained_variance | 0.0459     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 31999      |\n",
      "|    policy_loss        | -3.26e+03  |\n",
      "|    reward             | -359.98624 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 2.17e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 204        |\n",
      "|    iterations         | 32100      |\n",
      "|    time_elapsed       | 786        |\n",
      "|    total_timesteps    | 160500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.8      |\n",
      "|    explained_variance | 0.123      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 32099      |\n",
      "|    policy_loss        | 2.87e+03   |\n",
      "|    reward             | -125.12423 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 1.01e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 204        |\n",
      "|    iterations         | 32200      |\n",
      "|    time_elapsed       | 788        |\n",
      "|    total_timesteps    | 161000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.8      |\n",
      "|    explained_variance | 0.00924    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 32199      |\n",
      "|    policy_loss        | -7.91e+03  |\n",
      "|    reward             | -44.119125 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 9.09e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 204       |\n",
      "|    iterations         | 32300     |\n",
      "|    time_elapsed       | 790       |\n",
      "|    total_timesteps    | 161500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.7     |\n",
      "|    explained_variance | -0.472    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 32299     |\n",
      "|    policy_loss        | 2.88e+03  |\n",
      "|    reward             | 1.3532425 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 4.75e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 204       |\n",
      "|    iterations         | 32400     |\n",
      "|    time_elapsed       | 793       |\n",
      "|    total_timesteps    | 162000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.7     |\n",
      "|    explained_variance | 0.0563    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 32399     |\n",
      "|    policy_loss        | -2.75e+03 |\n",
      "|    reward             | 74.29216  |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 7.41e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 204       |\n",
      "|    iterations         | 32500     |\n",
      "|    time_elapsed       | 795       |\n",
      "|    total_timesteps    | 162500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.8     |\n",
      "|    explained_variance | 0.0589    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 32499     |\n",
      "|    policy_loss        | 2.2e+03   |\n",
      "|    reward             | 3.5859601 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 5.7e+03   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 204       |\n",
      "|    iterations         | 32600     |\n",
      "|    time_elapsed       | 797       |\n",
      "|    total_timesteps    | 163000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.8     |\n",
      "|    explained_variance | -0.56     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 32599     |\n",
      "|    policy_loss        | 1.08e+04  |\n",
      "|    reward             | -97.30019 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 6.27e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 204       |\n",
      "|    iterations         | 32700     |\n",
      "|    time_elapsed       | 800       |\n",
      "|    total_timesteps    | 163500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.9     |\n",
      "|    explained_variance | -0.0273   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 32699     |\n",
      "|    policy_loss        | 3.39e+03  |\n",
      "|    reward             | -398.3808 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 1.53e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 204       |\n",
      "|    iterations         | 32800     |\n",
      "|    time_elapsed       | 802       |\n",
      "|    total_timesteps    | 164000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.9     |\n",
      "|    explained_variance | -0.00479  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 32799     |\n",
      "|    policy_loss        | 1.4e+04   |\n",
      "|    reward             | 274.62885 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 1.76e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 204       |\n",
      "|    iterations         | 32900     |\n",
      "|    time_elapsed       | 804       |\n",
      "|    total_timesteps    | 164500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.9     |\n",
      "|    explained_variance | -0.195    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 32899     |\n",
      "|    policy_loss        | 58.5      |\n",
      "|    reward             | -7.500846 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 286       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 204        |\n",
      "|    iterations         | 33000      |\n",
      "|    time_elapsed       | 807        |\n",
      "|    total_timesteps    | 165000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.8      |\n",
      "|    explained_variance | -0.101     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 32999      |\n",
      "|    policy_loss        | 1.56e+03   |\n",
      "|    reward             | -122.43746 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 2.43e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 204        |\n",
      "|    iterations         | 33100      |\n",
      "|    time_elapsed       | 809        |\n",
      "|    total_timesteps    | 165500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.8      |\n",
      "|    explained_variance | 0.00252    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 33099      |\n",
      "|    policy_loss        | 1.49e+03   |\n",
      "|    reward             | -39.621597 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 3.92e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 204       |\n",
      "|    iterations         | 33200     |\n",
      "|    time_elapsed       | 811       |\n",
      "|    total_timesteps    | 166000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.9     |\n",
      "|    explained_variance | -0.114    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 33199     |\n",
      "|    policy_loss        | -1.29e+03 |\n",
      "|    reward             | 96.25804  |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 3.07e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 204       |\n",
      "|    iterations         | 33300     |\n",
      "|    time_elapsed       | 814       |\n",
      "|    total_timesteps    | 166500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.9     |\n",
      "|    explained_variance | 0.187     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 33299     |\n",
      "|    policy_loss        | 1.3e+03   |\n",
      "|    reward             | 107.29996 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 7.72e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 204       |\n",
      "|    iterations         | 33400     |\n",
      "|    time_elapsed       | 816       |\n",
      "|    total_timesteps    | 167000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45       |\n",
      "|    explained_variance | 0.0454    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 33399     |\n",
      "|    policy_loss        | -8.64e+03 |\n",
      "|    reward             | 493.80704 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 8.4e+04   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 204       |\n",
      "|    iterations         | 33500     |\n",
      "|    time_elapsed       | 818       |\n",
      "|    total_timesteps    | 167500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45       |\n",
      "|    explained_variance | -0.0146   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 33499     |\n",
      "|    policy_loss        | -5.46e+03 |\n",
      "|    reward             | 90.387825 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 1.04e+05  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 204        |\n",
      "|    iterations         | 33600      |\n",
      "|    time_elapsed       | 821        |\n",
      "|    total_timesteps    | 168000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.9      |\n",
      "|    explained_variance | 0.0828     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 33599      |\n",
      "|    policy_loss        | -620       |\n",
      "|    reward             | -3.3807998 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 1.56e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 204       |\n",
      "|    iterations         | 33700     |\n",
      "|    time_elapsed       | 823       |\n",
      "|    total_timesteps    | 168500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45       |\n",
      "|    explained_variance | 0.13      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 33699     |\n",
      "|    policy_loss        | -413      |\n",
      "|    reward             | 80.047714 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 402       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 204       |\n",
      "|    iterations         | 33800     |\n",
      "|    time_elapsed       | 825       |\n",
      "|    total_timesteps    | 169000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45       |\n",
      "|    explained_variance | 0.0636    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 33799     |\n",
      "|    policy_loss        | -6.51e+03 |\n",
      "|    reward             | 27.162367 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 2.53e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 204       |\n",
      "|    iterations         | 33900     |\n",
      "|    time_elapsed       | 828       |\n",
      "|    total_timesteps    | 169500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45       |\n",
      "|    explained_variance | 0.0242    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 33899     |\n",
      "|    policy_loss        | 3.46e+03  |\n",
      "|    reward             | 200.45874 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 8.83e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 204      |\n",
      "|    iterations         | 34000    |\n",
      "|    time_elapsed       | 830      |\n",
      "|    total_timesteps    | 170000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -45      |\n",
      "|    explained_variance | 0.0144   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33999    |\n",
      "|    policy_loss        | 3.87e+03 |\n",
      "|    reward             | -66.5263 |\n",
      "|    std                | 1.09     |\n",
      "|    value_loss         | 2.48e+04 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 204       |\n",
      "|    iterations         | 34100     |\n",
      "|    time_elapsed       | 832       |\n",
      "|    total_timesteps    | 170500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45       |\n",
      "|    explained_variance | 0.00452   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 34099     |\n",
      "|    policy_loss        | -2.71e+03 |\n",
      "|    reward             | 172.35645 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 1.11e+05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 204      |\n",
      "|    iterations         | 34200    |\n",
      "|    time_elapsed       | 834      |\n",
      "|    total_timesteps    | 171000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -45      |\n",
      "|    explained_variance | 0.167    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34199    |\n",
      "|    policy_loss        | -708     |\n",
      "|    reward             | 65.89795 |\n",
      "|    std                | 1.09     |\n",
      "|    value_loss         | 889      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 204       |\n",
      "|    iterations         | 34300     |\n",
      "|    time_elapsed       | 837       |\n",
      "|    total_timesteps    | 171500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45       |\n",
      "|    explained_variance | -0.141    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 34299     |\n",
      "|    policy_loss        | 1.82e+03  |\n",
      "|    reward             | 28.996675 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 2.74e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 204        |\n",
      "|    iterations         | 34400      |\n",
      "|    time_elapsed       | 839        |\n",
      "|    total_timesteps    | 172000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.1      |\n",
      "|    explained_variance | 0.00712    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 34399      |\n",
      "|    policy_loss        | -6.4e+03   |\n",
      "|    reward             | -12.907601 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 2.43e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 204        |\n",
      "|    iterations         | 34500      |\n",
      "|    time_elapsed       | 841        |\n",
      "|    total_timesteps    | 172500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.1      |\n",
      "|    explained_variance | -0.00245   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 34499      |\n",
      "|    policy_loss        | -3.22e+03  |\n",
      "|    reward             | -250.81763 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 1.86e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 204       |\n",
      "|    iterations         | 34600     |\n",
      "|    time_elapsed       | 844       |\n",
      "|    total_timesteps    | 173000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.1     |\n",
      "|    explained_variance | 0.0178    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 34599     |\n",
      "|    policy_loss        | 1.46e+04  |\n",
      "|    reward             | 4.6710386 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 1.23e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 204       |\n",
      "|    iterations         | 34700     |\n",
      "|    time_elapsed       | 846       |\n",
      "|    total_timesteps    | 173500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.1     |\n",
      "|    explained_variance | 0.011     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 34699     |\n",
      "|    policy_loss        | -1.89e+04 |\n",
      "|    reward             | 426.74442 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 2.38e+05  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 204        |\n",
      "|    iterations         | 34800      |\n",
      "|    time_elapsed       | 848        |\n",
      "|    total_timesteps    | 174000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.1      |\n",
      "|    explained_variance | 0.183      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 34799      |\n",
      "|    policy_loss        | 11.1       |\n",
      "|    reward             | -72.245995 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 861        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 205       |\n",
      "|    iterations         | 34900     |\n",
      "|    time_elapsed       | 851       |\n",
      "|    total_timesteps    | 174500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.2     |\n",
      "|    explained_variance | -0.042    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 34899     |\n",
      "|    policy_loss        | 3.28e+03  |\n",
      "|    reward             | 82.322334 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 7.5e+03   |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 205         |\n",
      "|    iterations         | 35000       |\n",
      "|    time_elapsed       | 853         |\n",
      "|    total_timesteps    | 175000      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -45.3       |\n",
      "|    explained_variance | -0.114      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 34999       |\n",
      "|    policy_loss        | -5.37e+03   |\n",
      "|    reward             | -127.581116 |\n",
      "|    std                | 1.1         |\n",
      "|    value_loss         | 2.41e+04    |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 205        |\n",
      "|    iterations         | 35100      |\n",
      "|    time_elapsed       | 855        |\n",
      "|    total_timesteps    | 175500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.3      |\n",
      "|    explained_variance | -0.0493    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 35099      |\n",
      "|    policy_loss        | -8.33e+03  |\n",
      "|    reward             | -26.904593 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 3.81e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 205        |\n",
      "|    iterations         | 35200      |\n",
      "|    time_elapsed       | 858        |\n",
      "|    total_timesteps    | 176000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.3      |\n",
      "|    explained_variance | -0.0187    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 35199      |\n",
      "|    policy_loss        | 1.83e+03   |\n",
      "|    reward             | -3.4153683 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 5.54e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 205        |\n",
      "|    iterations         | 35300      |\n",
      "|    time_elapsed       | 860        |\n",
      "|    total_timesteps    | 176500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.3      |\n",
      "|    explained_variance | 0.0504     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 35299      |\n",
      "|    policy_loss        | -2.87e+03  |\n",
      "|    reward             | -295.46332 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 1.48e+04   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 205      |\n",
      "|    iterations         | 35400    |\n",
      "|    time_elapsed       | 862      |\n",
      "|    total_timesteps    | 177000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -45.2    |\n",
      "|    explained_variance | 0.00571  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35399    |\n",
      "|    policy_loss        | 592      |\n",
      "|    reward             | 11.52207 |\n",
      "|    std                | 1.1      |\n",
      "|    value_loss         | 934      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 205       |\n",
      "|    iterations         | 35500     |\n",
      "|    time_elapsed       | 865       |\n",
      "|    total_timesteps    | 177500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.2     |\n",
      "|    explained_variance | -0.0674   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 35499     |\n",
      "|    policy_loss        | -551      |\n",
      "|    reward             | 43.126377 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 4.28e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 205        |\n",
      "|    iterations         | 35600      |\n",
      "|    time_elapsed       | 867        |\n",
      "|    total_timesteps    | 178000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.3      |\n",
      "|    explained_variance | -0.14      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 35599      |\n",
      "|    policy_loss        | 910        |\n",
      "|    reward             | -12.176819 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 2.86e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 205       |\n",
      "|    iterations         | 35700     |\n",
      "|    time_elapsed       | 869       |\n",
      "|    total_timesteps    | 178500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.2     |\n",
      "|    explained_variance | 0.0062    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 35699     |\n",
      "|    policy_loss        | -1.17e+03 |\n",
      "|    reward             | -43.74288 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 1.74e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 205        |\n",
      "|    iterations         | 35800      |\n",
      "|    time_elapsed       | 872        |\n",
      "|    total_timesteps    | 179000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.1      |\n",
      "|    explained_variance | 0.00216    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 35799      |\n",
      "|    policy_loss        | -1.04e+04  |\n",
      "|    reward             | -126.92784 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 9.08e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 205       |\n",
      "|    iterations         | 35900     |\n",
      "|    time_elapsed       | 874       |\n",
      "|    total_timesteps    | 179500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.2     |\n",
      "|    explained_variance | 0.0037    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 35899     |\n",
      "|    policy_loss        | -8.96e+03 |\n",
      "|    reward             | 61.697895 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 9.97e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 205       |\n",
      "|    iterations         | 36000     |\n",
      "|    time_elapsed       | 876       |\n",
      "|    total_timesteps    | 180000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.2     |\n",
      "|    explained_variance | 0.187     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 35999     |\n",
      "|    policy_loss        | 2.37e+03  |\n",
      "|    reward             | 33.425545 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 3.18e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 205       |\n",
      "|    iterations         | 36100     |\n",
      "|    time_elapsed       | 879       |\n",
      "|    total_timesteps    | 180500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.2     |\n",
      "|    explained_variance | 0.108     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 36099     |\n",
      "|    policy_loss        | -621      |\n",
      "|    reward             | 221.78783 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 2.08e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 205       |\n",
      "|    iterations         | 36200     |\n",
      "|    time_elapsed       | 881       |\n",
      "|    total_timesteps    | 181000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.1     |\n",
      "|    explained_variance | 0.114     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 36199     |\n",
      "|    policy_loss        | -1.12e+03 |\n",
      "|    reward             | -10.13071 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 3.14e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 205       |\n",
      "|    iterations         | 36300     |\n",
      "|    time_elapsed       | 883       |\n",
      "|    total_timesteps    | 181500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.1     |\n",
      "|    explained_variance | -0.0356   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 36299     |\n",
      "|    policy_loss        | 5.21e+03  |\n",
      "|    reward             | -68.14687 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 1.41e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 205       |\n",
      "|    iterations         | 36400     |\n",
      "|    time_elapsed       | 885       |\n",
      "|    total_timesteps    | 182000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.1     |\n",
      "|    explained_variance | -0.0177   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 36399     |\n",
      "|    policy_loss        | -5.84e+03 |\n",
      "|    reward             | 161.75687 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 3.25e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 205        |\n",
      "|    iterations         | 36500      |\n",
      "|    time_elapsed       | 888        |\n",
      "|    total_timesteps    | 182500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.1      |\n",
      "|    explained_variance | 0.00965    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 36499      |\n",
      "|    policy_loss        | -70.5      |\n",
      "|    reward             | -106.90018 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 9.86e+03   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 205      |\n",
      "|    iterations         | 36600    |\n",
      "|    time_elapsed       | 890      |\n",
      "|    total_timesteps    | 183000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -45.1    |\n",
      "|    explained_variance | 0.0214   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36599    |\n",
      "|    policy_loss        | 1.37e+04 |\n",
      "|    reward             | 566.8431 |\n",
      "|    std                | 1.1      |\n",
      "|    value_loss         | 1.53e+05 |\n",
      "------------------------------------\n",
      "day: 3102, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4812641.57\n",
      "total_reward: 3812641.57\n",
      "total_cost: 69000.56\n",
      "total_trades: 49388\n",
      "Sharpe: 3.445\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 205        |\n",
      "|    iterations         | 36700      |\n",
      "|    time_elapsed       | 892        |\n",
      "|    total_timesteps    | 183500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.2      |\n",
      "|    explained_variance | 0.0621     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 36699      |\n",
      "|    policy_loss        | 409        |\n",
      "|    reward             | -48.553192 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 1.84e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 205       |\n",
      "|    iterations         | 36800     |\n",
      "|    time_elapsed       | 895       |\n",
      "|    total_timesteps    | 184000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.2     |\n",
      "|    explained_variance | 0.029     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 36799     |\n",
      "|    policy_loss        | -206      |\n",
      "|    reward             | 100.75998 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 1.72e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 205        |\n",
      "|    iterations         | 36900      |\n",
      "|    time_elapsed       | 897        |\n",
      "|    total_timesteps    | 184500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.1      |\n",
      "|    explained_variance | -0.0166    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 36899      |\n",
      "|    policy_loss        | -5.14e+03  |\n",
      "|    reward             | -191.92369 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 2.31e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 205        |\n",
      "|    iterations         | 37000      |\n",
      "|    time_elapsed       | 899        |\n",
      "|    total_timesteps    | 185000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.1      |\n",
      "|    explained_variance | 0.161      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 36999      |\n",
      "|    policy_loss        | 5.73e+03   |\n",
      "|    reward             | -133.36984 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 2.21e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 205        |\n",
      "|    iterations         | 37100      |\n",
      "|    time_elapsed       | 902        |\n",
      "|    total_timesteps    | 185500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.1      |\n",
      "|    explained_variance | -0.0832    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 37099      |\n",
      "|    policy_loss        | -5.98e+03  |\n",
      "|    reward             | -57.076027 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 2.67e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 205        |\n",
      "|    iterations         | 37200      |\n",
      "|    time_elapsed       | 904        |\n",
      "|    total_timesteps    | 186000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.1      |\n",
      "|    explained_variance | 0.0139     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 37199      |\n",
      "|    policy_loss        | 2.98e+03   |\n",
      "|    reward             | -398.97293 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 9.53e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 205        |\n",
      "|    iterations         | 37300      |\n",
      "|    time_elapsed       | 906        |\n",
      "|    total_timesteps    | 186500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.2      |\n",
      "|    explained_variance | 0.42       |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 37299      |\n",
      "|    policy_loss        | 1.16e+03   |\n",
      "|    reward             | -40.256355 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 742        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 205       |\n",
      "|    iterations         | 37400     |\n",
      "|    time_elapsed       | 909       |\n",
      "|    total_timesteps    | 187000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.2     |\n",
      "|    explained_variance | 0.0604    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 37399     |\n",
      "|    policy_loss        | -2.44e+03 |\n",
      "|    reward             | 143.72618 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 4.69e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 205        |\n",
      "|    iterations         | 37500      |\n",
      "|    time_elapsed       | 911        |\n",
      "|    total_timesteps    | 187500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.3      |\n",
      "|    explained_variance | 0.0448     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 37499      |\n",
      "|    policy_loss        | 5.24e+03   |\n",
      "|    reward             | -94.112526 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 1.96e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 205       |\n",
      "|    iterations         | 37600     |\n",
      "|    time_elapsed       | 913       |\n",
      "|    total_timesteps    | 188000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.3     |\n",
      "|    explained_variance | 0.000834  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 37599     |\n",
      "|    policy_loss        | -5.84e+03 |\n",
      "|    reward             | -178.2288 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 2.91e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 205       |\n",
      "|    iterations         | 37700     |\n",
      "|    time_elapsed       | 916       |\n",
      "|    total_timesteps    | 188500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.3     |\n",
      "|    explained_variance | -0.0624   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 37699     |\n",
      "|    policy_loss        | 937       |\n",
      "|    reward             | 105.04082 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 9.96e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 205        |\n",
      "|    iterations         | 37800      |\n",
      "|    time_elapsed       | 918        |\n",
      "|    total_timesteps    | 189000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.4      |\n",
      "|    explained_variance | -0.0131    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 37799      |\n",
      "|    policy_loss        | -1.64e+04  |\n",
      "|    reward             | 106.957596 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 1.45e+05   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 205      |\n",
      "|    iterations         | 37900    |\n",
      "|    time_elapsed       | 920      |\n",
      "|    total_timesteps    | 189500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -45.4    |\n",
      "|    explained_variance | 0.164    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37899    |\n",
      "|    policy_loss        | 1.45e+03 |\n",
      "|    reward             | 87.93061 |\n",
      "|    std                | 1.11     |\n",
      "|    value_loss         | 2.39e+03 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 205        |\n",
      "|    iterations         | 38000      |\n",
      "|    time_elapsed       | 923        |\n",
      "|    total_timesteps    | 190000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.4      |\n",
      "|    explained_variance | 0.113      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 37999      |\n",
      "|    policy_loss        | -406       |\n",
      "|    reward             | -32.692978 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 329        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 205        |\n",
      "|    iterations         | 38100      |\n",
      "|    time_elapsed       | 925        |\n",
      "|    total_timesteps    | 190500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.4      |\n",
      "|    explained_variance | -0.014     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 38099      |\n",
      "|    policy_loss        | 2.28e+03   |\n",
      "|    reward             | -51.944595 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 4.73e+03   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 205      |\n",
      "|    iterations         | 38200    |\n",
      "|    time_elapsed       | 927      |\n",
      "|    total_timesteps    | 191000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -45.4    |\n",
      "|    explained_variance | 0.0565   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38199    |\n",
      "|    policy_loss        | 3.28e+03 |\n",
      "|    reward             | 166.3823 |\n",
      "|    std                | 1.11     |\n",
      "|    value_loss         | 9.15e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 205       |\n",
      "|    iterations         | 38300     |\n",
      "|    time_elapsed       | 930       |\n",
      "|    total_timesteps    | 191500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.4     |\n",
      "|    explained_variance | -0.0507   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 38299     |\n",
      "|    policy_loss        | 7.77e+03  |\n",
      "|    reward             | 109.56518 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 3.4e+04   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 205       |\n",
      "|    iterations         | 38400     |\n",
      "|    time_elapsed       | 932       |\n",
      "|    total_timesteps    | 192000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.5     |\n",
      "|    explained_variance | 0.0229    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 38399     |\n",
      "|    policy_loss        | -6.96e+03 |\n",
      "|    reward             | 265.41617 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 3.52e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 205       |\n",
      "|    iterations         | 38500     |\n",
      "|    time_elapsed       | 934       |\n",
      "|    total_timesteps    | 192500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.5     |\n",
      "|    explained_variance | -0.291    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 38499     |\n",
      "|    policy_loss        | 558       |\n",
      "|    reward             | 6.3984632 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 624       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 205       |\n",
      "|    iterations         | 38600     |\n",
      "|    time_elapsed       | 937       |\n",
      "|    total_timesteps    | 193000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.5     |\n",
      "|    explained_variance | 0.042     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 38599     |\n",
      "|    policy_loss        | -1.5e+03  |\n",
      "|    reward             | 33.670444 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 2e+03     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 205        |\n",
      "|    iterations         | 38700      |\n",
      "|    time_elapsed       | 939        |\n",
      "|    total_timesteps    | 193500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.6      |\n",
      "|    explained_variance | -0.00693   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 38699      |\n",
      "|    policy_loss        | -855       |\n",
      "|    reward             | -31.631874 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 2.94e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 206        |\n",
      "|    iterations         | 38800      |\n",
      "|    time_elapsed       | 941        |\n",
      "|    total_timesteps    | 194000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.6      |\n",
      "|    explained_variance | -0.226     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 38799      |\n",
      "|    policy_loss        | 1.78e+03   |\n",
      "|    reward             | -56.393658 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 2.67e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 38900     |\n",
      "|    time_elapsed       | 943       |\n",
      "|    total_timesteps    | 194500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.6     |\n",
      "|    explained_variance | -0.0404   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 38899     |\n",
      "|    policy_loss        | 8.85e+03  |\n",
      "|    reward             | 116.09509 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 3.66e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 39000     |\n",
      "|    time_elapsed       | 946       |\n",
      "|    total_timesteps    | 195000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.6     |\n",
      "|    explained_variance | 0.0317    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 38999     |\n",
      "|    policy_loss        | 2.04e+03  |\n",
      "|    reward             | 253.32932 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 6.52e+03  |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 206         |\n",
      "|    iterations         | 39100       |\n",
      "|    time_elapsed       | 948         |\n",
      "|    total_timesteps    | 195500      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -45.6       |\n",
      "|    explained_variance | 0.104       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 39099       |\n",
      "|    policy_loss        | -78.7       |\n",
      "|    reward             | -0.28564715 |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 112         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 39200     |\n",
      "|    time_elapsed       | 950       |\n",
      "|    total_timesteps    | 196000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.7     |\n",
      "|    explained_variance | 0.0905    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 39199     |\n",
      "|    policy_loss        | -1.06e+03 |\n",
      "|    reward             | 52.222054 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 1.79e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 206      |\n",
      "|    iterations         | 39300    |\n",
      "|    time_elapsed       | 953      |\n",
      "|    total_timesteps    | 196500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -45.6    |\n",
      "|    explained_variance | -0.0446  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39299    |\n",
      "|    policy_loss        | 3.39e+03 |\n",
      "|    reward             | 5.906813 |\n",
      "|    std                | 1.11     |\n",
      "|    value_loss         | 6.84e+03 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 206        |\n",
      "|    iterations         | 39400      |\n",
      "|    time_elapsed       | 955        |\n",
      "|    total_timesteps    | 197000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.6      |\n",
      "|    explained_variance | 0.202      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 39399      |\n",
      "|    policy_loss        | -1.48e+03  |\n",
      "|    reward             | -23.368813 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 1.88e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 206        |\n",
      "|    iterations         | 39500      |\n",
      "|    time_elapsed       | 957        |\n",
      "|    total_timesteps    | 197500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.5      |\n",
      "|    explained_variance | -0.0801    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 39499      |\n",
      "|    policy_loss        | -3.97e+03  |\n",
      "|    reward             | -17.137665 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 1.88e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 39600     |\n",
      "|    time_elapsed       | 960       |\n",
      "|    total_timesteps    | 198000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.5     |\n",
      "|    explained_variance | -0.0372   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 39599     |\n",
      "|    policy_loss        | -1.5e+03  |\n",
      "|    reward             | 242.58824 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 2.11e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 39700     |\n",
      "|    time_elapsed       | 962       |\n",
      "|    total_timesteps    | 198500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.5     |\n",
      "|    explained_variance | 0.0431    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 39699     |\n",
      "|    policy_loss        | -9.79e+03 |\n",
      "|    reward             | 155.4995  |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 7.01e+04  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 206      |\n",
      "|    iterations         | 39800    |\n",
      "|    time_elapsed       | 964      |\n",
      "|    total_timesteps    | 199000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -45.6    |\n",
      "|    explained_variance | 0.00299  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39799    |\n",
      "|    policy_loss        | 7.52e+03 |\n",
      "|    reward             | 93.09852 |\n",
      "|    std                | 1.11     |\n",
      "|    value_loss         | 2.84e+04 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 206        |\n",
      "|    iterations         | 39900      |\n",
      "|    time_elapsed       | 967        |\n",
      "|    total_timesteps    | 199500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.6      |\n",
      "|    explained_variance | 0.0746     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 39899      |\n",
      "|    policy_loss        | 2.94e+03   |\n",
      "|    reward             | -64.940506 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 5.48e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 40000     |\n",
      "|    time_elapsed       | 969       |\n",
      "|    total_timesteps    | 200000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.6     |\n",
      "|    explained_variance | -0.0651   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 39999     |\n",
      "|    policy_loss        | 1.34e+03  |\n",
      "|    reward             | 115.09141 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 4.61e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 40100     |\n",
      "|    time_elapsed       | 971       |\n",
      "|    total_timesteps    | 200500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.6     |\n",
      "|    explained_variance | 0.0633    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 40099     |\n",
      "|    policy_loss        | -4.29e+03 |\n",
      "|    reward             | -99.73385 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 2.19e+04  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 206      |\n",
      "|    iterations         | 40200    |\n",
      "|    time_elapsed       | 974      |\n",
      "|    total_timesteps    | 201000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -45.7    |\n",
      "|    explained_variance | 0.0411   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40199    |\n",
      "|    policy_loss        | -1.1e+03 |\n",
      "|    reward             | 0.491288 |\n",
      "|    std                | 1.12     |\n",
      "|    value_loss         | 9.33e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 206      |\n",
      "|    iterations         | 40300    |\n",
      "|    time_elapsed       | 976      |\n",
      "|    total_timesteps    | 201500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -45.7    |\n",
      "|    explained_variance | -0.0455  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40299    |\n",
      "|    policy_loss        | -5.4e+03 |\n",
      "|    reward             | 79.19674 |\n",
      "|    std                | 1.12     |\n",
      "|    value_loss         | 5.52e+04 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 40400     |\n",
      "|    time_elapsed       | 978       |\n",
      "|    total_timesteps    | 202000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.7     |\n",
      "|    explained_variance | -0.126    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 40399     |\n",
      "|    policy_loss        | 2.14e+03  |\n",
      "|    reward             | -18.43051 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 3.78e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 206        |\n",
      "|    iterations         | 40500      |\n",
      "|    time_elapsed       | 981        |\n",
      "|    total_timesteps    | 202500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.8      |\n",
      "|    explained_variance | 0.0168     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 40499      |\n",
      "|    policy_loss        | -663       |\n",
      "|    reward             | -162.99832 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 5.45e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 40600     |\n",
      "|    time_elapsed       | 985       |\n",
      "|    total_timesteps    | 203000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.8     |\n",
      "|    explained_variance | -0.0155   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 40599     |\n",
      "|    policy_loss        | 7.23e+03  |\n",
      "|    reward             | 119.88551 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 2.81e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 206        |\n",
      "|    iterations         | 40700      |\n",
      "|    time_elapsed       | 987        |\n",
      "|    total_timesteps    | 203500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.8      |\n",
      "|    explained_variance | 0.0611     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 40699      |\n",
      "|    policy_loss        | 1.85e+03   |\n",
      "|    reward             | -1.2686819 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 1.99e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 40800     |\n",
      "|    time_elapsed       | 989       |\n",
      "|    total_timesteps    | 204000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.8     |\n",
      "|    explained_variance | 0.128     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 40799     |\n",
      "|    policy_loss        | -1.82e+03 |\n",
      "|    reward             | 206.26767 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 2.43e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 206        |\n",
      "|    iterations         | 40900      |\n",
      "|    time_elapsed       | 992        |\n",
      "|    total_timesteps    | 204500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.7      |\n",
      "|    explained_variance | 0.00845    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 40899      |\n",
      "|    policy_loss        | -9.85e+03  |\n",
      "|    reward             | -341.51215 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 5.99e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 206        |\n",
      "|    iterations         | 41000      |\n",
      "|    time_elapsed       | 994        |\n",
      "|    total_timesteps    | 205000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.7      |\n",
      "|    explained_variance | 0.24       |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 40999      |\n",
      "|    policy_loss        | -1.22e+03  |\n",
      "|    reward             | -25.713774 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 1.49e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 41100     |\n",
      "|    time_elapsed       | 996       |\n",
      "|    total_timesteps    | 205500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.8     |\n",
      "|    explained_variance | -0.216    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 41099     |\n",
      "|    policy_loss        | 3.79e+03  |\n",
      "|    reward             | 44.520546 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 7.71e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 41200     |\n",
      "|    time_elapsed       | 999       |\n",
      "|    total_timesteps    | 206000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.8     |\n",
      "|    explained_variance | 0.053     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 41199     |\n",
      "|    policy_loss        | 4.83e+03  |\n",
      "|    reward             | 17.600765 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 1.26e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 206        |\n",
      "|    iterations         | 41300      |\n",
      "|    time_elapsed       | 1001       |\n",
      "|    total_timesteps    | 206500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.8      |\n",
      "|    explained_variance | -0.0134    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 41299      |\n",
      "|    policy_loss        | -745       |\n",
      "|    reward             | -27.180435 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 399        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 206        |\n",
      "|    iterations         | 41400      |\n",
      "|    time_elapsed       | 1003       |\n",
      "|    total_timesteps    | 207000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.8      |\n",
      "|    explained_variance | -0.126     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 41399      |\n",
      "|    policy_loss        | -1.63e+03  |\n",
      "|    reward             | -125.09975 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 2.77e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 41500     |\n",
      "|    time_elapsed       | 1006      |\n",
      "|    total_timesteps    | 207500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.8     |\n",
      "|    explained_variance | 0.0942    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 41499     |\n",
      "|    policy_loss        | -6.71e+03 |\n",
      "|    reward             | -9.348563 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 2.65e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 206        |\n",
      "|    iterations         | 41600      |\n",
      "|    time_elapsed       | 1008       |\n",
      "|    total_timesteps    | 208000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.8      |\n",
      "|    explained_variance | 0.1        |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 41599      |\n",
      "|    policy_loss        | -162       |\n",
      "|    reward             | -139.84053 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 446        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 41700     |\n",
      "|    time_elapsed       | 1010      |\n",
      "|    total_timesteps    | 208500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.8     |\n",
      "|    explained_variance | 0.052     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 41699     |\n",
      "|    policy_loss        | -827      |\n",
      "|    reward             | 16.420088 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 2.86e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 206      |\n",
      "|    iterations         | 41800    |\n",
      "|    time_elapsed       | 1013     |\n",
      "|    total_timesteps    | 209000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -45.8    |\n",
      "|    explained_variance | -0.221   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41799    |\n",
      "|    policy_loss        | 520      |\n",
      "|    reward             | 7.762169 |\n",
      "|    std                | 1.12     |\n",
      "|    value_loss         | 547      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 206        |\n",
      "|    iterations         | 41900      |\n",
      "|    time_elapsed       | 1015       |\n",
      "|    total_timesteps    | 209500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.9      |\n",
      "|    explained_variance | 0.185      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 41899      |\n",
      "|    policy_loss        | -1.19e+03  |\n",
      "|    reward             | -45.937572 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 1.68e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 42000     |\n",
      "|    time_elapsed       | 1017      |\n",
      "|    total_timesteps    | 210000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.9     |\n",
      "|    explained_variance | 0.178     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 41999     |\n",
      "|    policy_loss        | 3.95e+03  |\n",
      "|    reward             | 146.50713 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 8.38e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 206      |\n",
      "|    iterations         | 42100    |\n",
      "|    time_elapsed       | 1019     |\n",
      "|    total_timesteps    | 210500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -45.9    |\n",
      "|    explained_variance | -0.0167  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42099    |\n",
      "|    policy_loss        | 38.9     |\n",
      "|    reward             | 32.60154 |\n",
      "|    std                | 1.13     |\n",
      "|    value_loss         | 1.43e+03 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 206        |\n",
      "|    iterations         | 42200      |\n",
      "|    time_elapsed       | 1022       |\n",
      "|    total_timesteps    | 211000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.9      |\n",
      "|    explained_variance | 0.0416     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 42199      |\n",
      "|    policy_loss        | 643        |\n",
      "|    reward             | -19.411543 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 3.98e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 42300     |\n",
      "|    time_elapsed       | 1024      |\n",
      "|    total_timesteps    | 211500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46       |\n",
      "|    explained_variance | 0.114     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 42299     |\n",
      "|    policy_loss        | -1.66e+03 |\n",
      "|    reward             | 80.43665  |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 3.53e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 42400     |\n",
      "|    time_elapsed       | 1026      |\n",
      "|    total_timesteps    | 212000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46       |\n",
      "|    explained_variance | 0.0328    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 42399     |\n",
      "|    policy_loss        | -1.93e+03 |\n",
      "|    reward             | 41.526363 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 1.98e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 42500     |\n",
      "|    time_elapsed       | 1029      |\n",
      "|    total_timesteps    | 212500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46       |\n",
      "|    explained_variance | 0.12      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 42499     |\n",
      "|    policy_loss        | -2.27e+03 |\n",
      "|    reward             | 19.369724 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 3.44e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 42600     |\n",
      "|    time_elapsed       | 1031      |\n",
      "|    total_timesteps    | 213000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46       |\n",
      "|    explained_variance | 0.167     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 42599     |\n",
      "|    policy_loss        | -1.52e+03 |\n",
      "|    reward             | -28.01454 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 1.62e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 206        |\n",
      "|    iterations         | 42700      |\n",
      "|    time_elapsed       | 1033       |\n",
      "|    total_timesteps    | 213500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.1      |\n",
      "|    explained_variance | -0.252     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 42699      |\n",
      "|    policy_loss        | 4.37e+03   |\n",
      "|    reward             | -12.067116 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 1.04e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 42800     |\n",
      "|    time_elapsed       | 1036      |\n",
      "|    total_timesteps    | 214000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.2     |\n",
      "|    explained_variance | -0.00542  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 42799     |\n",
      "|    policy_loss        | -2.66e+03 |\n",
      "|    reward             | -125.4182 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 5.68e+03  |\n",
      "-------------------------------------\n",
      "day: 3102, episode: 70\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1843203.88\n",
      "total_reward: 843203.88\n",
      "total_cost: 5174.59\n",
      "total_trades: 36151\n",
      "Sharpe: 3.861\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 206        |\n",
      "|    iterations         | 42900      |\n",
      "|    time_elapsed       | 1038       |\n",
      "|    total_timesteps    | 214500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.2      |\n",
      "|    explained_variance | 0.0991     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 42899      |\n",
      "|    policy_loss        | 2.67e+03   |\n",
      "|    reward             | -60.661514 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 5.01e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 43000     |\n",
      "|    time_elapsed       | 1041      |\n",
      "|    total_timesteps    | 215000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.1     |\n",
      "|    explained_variance | 0.169     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 42999     |\n",
      "|    policy_loss        | -646      |\n",
      "|    reward             | 7.9221177 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 1.08e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 43100     |\n",
      "|    time_elapsed       | 1043      |\n",
      "|    total_timesteps    | 215500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.2     |\n",
      "|    explained_variance | -0.102    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 43099     |\n",
      "|    policy_loss        | -629      |\n",
      "|    reward             | -74.95318 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 1.79e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 43200     |\n",
      "|    time_elapsed       | 1045      |\n",
      "|    total_timesteps    | 216000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.2     |\n",
      "|    explained_variance | -0.00827  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 43199     |\n",
      "|    policy_loss        | 3.01e+03  |\n",
      "|    reward             | 12.615606 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 6.25e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 43300     |\n",
      "|    time_elapsed       | 1048      |\n",
      "|    total_timesteps    | 216500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.2     |\n",
      "|    explained_variance | 0.058     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 43299     |\n",
      "|    policy_loss        | -1.11e+03 |\n",
      "|    reward             | 195.43024 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 4.27e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 206        |\n",
      "|    iterations         | 43400      |\n",
      "|    time_elapsed       | 1050       |\n",
      "|    total_timesteps    | 217000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.3      |\n",
      "|    explained_variance | -0.0274    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 43399      |\n",
      "|    policy_loss        | -3.26e+03  |\n",
      "|    reward             | -23.112972 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 1.34e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 206        |\n",
      "|    iterations         | 43500      |\n",
      "|    time_elapsed       | 1052       |\n",
      "|    total_timesteps    | 217500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.2      |\n",
      "|    explained_variance | 0.0155     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 43499      |\n",
      "|    policy_loss        | 523        |\n",
      "|    reward             | -17.370867 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 1.9e+03    |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 206      |\n",
      "|    iterations         | 43600    |\n",
      "|    time_elapsed       | 1055     |\n",
      "|    total_timesteps    | 218000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -46.2    |\n",
      "|    explained_variance | 0.0114   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43599    |\n",
      "|    policy_loss        | -54.9    |\n",
      "|    reward             | 48.66579 |\n",
      "|    std                | 1.14     |\n",
      "|    value_loss         | 3.16e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 43700     |\n",
      "|    time_elapsed       | 1057      |\n",
      "|    total_timesteps    | 218500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.2     |\n",
      "|    explained_variance | 0.258     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 43699     |\n",
      "|    policy_loss        | -46.7     |\n",
      "|    reward             | -97.24201 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 298       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 43800     |\n",
      "|    time_elapsed       | 1059      |\n",
      "|    total_timesteps    | 219000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.2     |\n",
      "|    explained_variance | -0.0837   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 43799     |\n",
      "|    policy_loss        | -999      |\n",
      "|    reward             | 141.61389 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 1.2e+03   |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 206        |\n",
      "|    iterations         | 43900      |\n",
      "|    time_elapsed       | 1062       |\n",
      "|    total_timesteps    | 219500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.2      |\n",
      "|    explained_variance | -0.0212    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 43899      |\n",
      "|    policy_loss        | -1.03e+03  |\n",
      "|    reward             | -31.761183 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 2e+03      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 206        |\n",
      "|    iterations         | 44000      |\n",
      "|    time_elapsed       | 1064       |\n",
      "|    total_timesteps    | 220000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.1      |\n",
      "|    explained_variance | 0.0645     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 43999      |\n",
      "|    policy_loss        | -2.22e+03  |\n",
      "|    reward             | -12.240848 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 5.73e+03   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 206      |\n",
      "|    iterations         | 44100    |\n",
      "|    time_elapsed       | 1066     |\n",
      "|    total_timesteps    | 220500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -46.2    |\n",
      "|    explained_variance | -0.04    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44099    |\n",
      "|    policy_loss        | 562      |\n",
      "|    reward             | -89.8094 |\n",
      "|    std                | 1.14     |\n",
      "|    value_loss         | 855      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 44200     |\n",
      "|    time_elapsed       | 1069      |\n",
      "|    total_timesteps    | 221000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.2     |\n",
      "|    explained_variance | 0.029     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 44199     |\n",
      "|    policy_loss        | -1.55e+03 |\n",
      "|    reward             | 24.017437 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 4.07e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 44300     |\n",
      "|    time_elapsed       | 1071      |\n",
      "|    total_timesteps    | 221500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.2     |\n",
      "|    explained_variance | 0.512     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 44299     |\n",
      "|    policy_loss        | 2.33e+03  |\n",
      "|    reward             | 2.1285377 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 2.55e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 44400     |\n",
      "|    time_elapsed       | 1074      |\n",
      "|    total_timesteps    | 222000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.1     |\n",
      "|    explained_variance | -0.0295   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 44399     |\n",
      "|    policy_loss        | 1.42e+03  |\n",
      "|    reward             | 6.1850176 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 1.6e+03   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 44500     |\n",
      "|    time_elapsed       | 1076      |\n",
      "|    total_timesteps    | 222500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.1     |\n",
      "|    explained_variance | 0.0511    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 44499     |\n",
      "|    policy_loss        | 251       |\n",
      "|    reward             | 11.660895 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 670       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 44600     |\n",
      "|    time_elapsed       | 1078      |\n",
      "|    total_timesteps    | 223000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.1     |\n",
      "|    explained_variance | 0.181     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 44599     |\n",
      "|    policy_loss        | -1.79e+03 |\n",
      "|    reward             | 31.015617 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 4.87e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 44700     |\n",
      "|    time_elapsed       | 1081      |\n",
      "|    total_timesteps    | 223500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.2     |\n",
      "|    explained_variance | 0.208     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 44699     |\n",
      "|    policy_loss        | 1.14e+03  |\n",
      "|    reward             | -32.89813 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 681       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 206      |\n",
      "|    iterations         | 44800    |\n",
      "|    time_elapsed       | 1083     |\n",
      "|    total_timesteps    | 224000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -46.2    |\n",
      "|    explained_variance | 0.0166   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44799    |\n",
      "|    policy_loss        | 1.91e+03 |\n",
      "|    reward             | 40.4452  |\n",
      "|    std                | 1.14     |\n",
      "|    value_loss         | 2.01e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 206      |\n",
      "|    iterations         | 44900    |\n",
      "|    time_elapsed       | 1085     |\n",
      "|    total_timesteps    | 224500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -46.3    |\n",
      "|    explained_variance | -0.16    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44899    |\n",
      "|    policy_loss        | -55.2    |\n",
      "|    reward             | 74.0543  |\n",
      "|    std                | 1.14     |\n",
      "|    value_loss         | 449      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 45000     |\n",
      "|    time_elapsed       | 1088      |\n",
      "|    total_timesteps    | 225000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.3     |\n",
      "|    explained_variance | 0.0481    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 44999     |\n",
      "|    policy_loss        | -1.59e+03 |\n",
      "|    reward             | -44.37658 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 2.47e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 45100     |\n",
      "|    time_elapsed       | 1090      |\n",
      "|    total_timesteps    | 225500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.3     |\n",
      "|    explained_variance | 0.0784    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 45099     |\n",
      "|    policy_loss        | 529       |\n",
      "|    reward             | 32.261536 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 1.82e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 45200     |\n",
      "|    time_elapsed       | 1092      |\n",
      "|    total_timesteps    | 226000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.3     |\n",
      "|    explained_variance | 0.00306   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 45199     |\n",
      "|    policy_loss        | -1.45e+03 |\n",
      "|    reward             | -134.7265 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 3.72e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 45300     |\n",
      "|    time_elapsed       | 1095      |\n",
      "|    total_timesteps    | 226500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.3     |\n",
      "|    explained_variance | 0.0127    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 45299     |\n",
      "|    policy_loss        | -1.65e+03 |\n",
      "|    reward             | -82.65193 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 1.15e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 45400     |\n",
      "|    time_elapsed       | 1097      |\n",
      "|    total_timesteps    | 227000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.3     |\n",
      "|    explained_variance | 0.158     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 45399     |\n",
      "|    policy_loss        | -195      |\n",
      "|    reward             | 15.655204 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 237       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 206        |\n",
      "|    iterations         | 45500      |\n",
      "|    time_elapsed       | 1099       |\n",
      "|    total_timesteps    | 227500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.3      |\n",
      "|    explained_variance | 0.218      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 45499      |\n",
      "|    policy_loss        | 1.58e+03   |\n",
      "|    reward             | -24.908272 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 2.06e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 206        |\n",
      "|    iterations         | 45600      |\n",
      "|    time_elapsed       | 1102       |\n",
      "|    total_timesteps    | 228000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.3      |\n",
      "|    explained_variance | -0.433     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 45599      |\n",
      "|    policy_loss        | -52        |\n",
      "|    reward             | -10.773362 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 202        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 45700     |\n",
      "|    time_elapsed       | 1104      |\n",
      "|    total_timesteps    | 228500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.2     |\n",
      "|    explained_variance | -0.224    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 45699     |\n",
      "|    policy_loss        | -1.35e+03 |\n",
      "|    reward             | -35.35416 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 1.01e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 206        |\n",
      "|    iterations         | 45800      |\n",
      "|    time_elapsed       | 1106       |\n",
      "|    total_timesteps    | 229000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.3      |\n",
      "|    explained_variance | -0.0713    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 45799      |\n",
      "|    policy_loss        | -1.4e+03   |\n",
      "|    reward             | -56.789898 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 3.1e+03    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 206        |\n",
      "|    iterations         | 45900      |\n",
      "|    time_elapsed       | 1109       |\n",
      "|    total_timesteps    | 229500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.3      |\n",
      "|    explained_variance | -0.0258    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 45899      |\n",
      "|    policy_loss        | 2.15e+03   |\n",
      "|    reward             | -1.9569445 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 3.56e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 206        |\n",
      "|    iterations         | 46000      |\n",
      "|    time_elapsed       | 1111       |\n",
      "|    total_timesteps    | 230000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.4      |\n",
      "|    explained_variance | -0.32      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 45999      |\n",
      "|    policy_loss        | 1.02e+03   |\n",
      "|    reward             | -14.104026 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 520        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 206        |\n",
      "|    iterations         | 46100      |\n",
      "|    time_elapsed       | 1113       |\n",
      "|    total_timesteps    | 230500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.4      |\n",
      "|    explained_variance | 0.293      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 46099      |\n",
      "|    policy_loss        | 16.5       |\n",
      "|    reward             | -119.14113 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 192        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 46200     |\n",
      "|    time_elapsed       | 1116      |\n",
      "|    total_timesteps    | 231000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.4     |\n",
      "|    explained_variance | 0.06      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 46199     |\n",
      "|    policy_loss        | -1.13e+03 |\n",
      "|    reward             | 4.0716496 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 1.34e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 46300     |\n",
      "|    time_elapsed       | 1118      |\n",
      "|    total_timesteps    | 231500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.4     |\n",
      "|    explained_variance | -0.0668   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 46299     |\n",
      "|    policy_loss        | -3.38e+03 |\n",
      "|    reward             | 46.152313 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 6.06e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 46400      |\n",
      "|    time_elapsed       | 1120       |\n",
      "|    total_timesteps    | 232000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.4      |\n",
      "|    explained_variance | -0.0604    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 46399      |\n",
      "|    policy_loss        | -55.8      |\n",
      "|    reward             | -161.51157 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 4.98e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 46500     |\n",
      "|    time_elapsed       | 1123      |\n",
      "|    total_timesteps    | 232500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.4     |\n",
      "|    explained_variance | 0.0012    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 46499     |\n",
      "|    policy_loss        | -2.48e+03 |\n",
      "|    reward             | 137.84253 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 5.22e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 46600     |\n",
      "|    time_elapsed       | 1125      |\n",
      "|    total_timesteps    | 233000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.4     |\n",
      "|    explained_variance | 0.113     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 46599     |\n",
      "|    policy_loss        | 993       |\n",
      "|    reward             | -27.52681 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 671       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 46700      |\n",
      "|    time_elapsed       | 1127       |\n",
      "|    total_timesteps    | 233500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.5      |\n",
      "|    explained_variance | 0.0121     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 46699      |\n",
      "|    policy_loss        | -516       |\n",
      "|    reward             | -79.573784 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 634        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 46800     |\n",
      "|    time_elapsed       | 1129      |\n",
      "|    total_timesteps    | 234000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.4     |\n",
      "|    explained_variance | 0.0806    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 46799     |\n",
      "|    policy_loss        | 3.18e+03  |\n",
      "|    reward             | -11.88294 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 5.19e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 207      |\n",
      "|    iterations         | 46900    |\n",
      "|    time_elapsed       | 1132     |\n",
      "|    total_timesteps    | 234500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -46.5    |\n",
      "|    explained_variance | 0.0517   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46899    |\n",
      "|    policy_loss        | 167      |\n",
      "|    reward             | 63.44409 |\n",
      "|    std                | 1.15     |\n",
      "|    value_loss         | 1.26e+03 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 47000      |\n",
      "|    time_elapsed       | 1134       |\n",
      "|    total_timesteps    | 235000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.5      |\n",
      "|    explained_variance | -0.0794    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 46999      |\n",
      "|    policy_loss        | 1.02e+03   |\n",
      "|    reward             | -181.52771 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 3.25e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 47100      |\n",
      "|    time_elapsed       | 1136       |\n",
      "|    total_timesteps    | 235500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.6      |\n",
      "|    explained_variance | 0.0725     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 47099      |\n",
      "|    policy_loss        | 4.32e+03   |\n",
      "|    reward             | -153.25424 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 1.56e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 47200      |\n",
      "|    time_elapsed       | 1139       |\n",
      "|    total_timesteps    | 236000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.6      |\n",
      "|    explained_variance | -0.253     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 47199      |\n",
      "|    policy_loss        | -1.82e+03  |\n",
      "|    reward             | -22.699884 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 2.83e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 47300     |\n",
      "|    time_elapsed       | 1141      |\n",
      "|    total_timesteps    | 236500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.5     |\n",
      "|    explained_variance | -0.000755 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 47299     |\n",
      "|    policy_loss        | 3.36e+03  |\n",
      "|    reward             | -67.0963  |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 9.13e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 47400     |\n",
      "|    time_elapsed       | 1143      |\n",
      "|    total_timesteps    | 237000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.6     |\n",
      "|    explained_variance | -0.713    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 47399     |\n",
      "|    policy_loss        | -1.11e+03 |\n",
      "|    reward             | -88.0569  |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 885       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 47500     |\n",
      "|    time_elapsed       | 1146      |\n",
      "|    total_timesteps    | 237500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.6     |\n",
      "|    explained_variance | 0.111     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 47499     |\n",
      "|    policy_loss        | -1.12e+03 |\n",
      "|    reward             | 38.74318  |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 1.81e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 47600     |\n",
      "|    time_elapsed       | 1148      |\n",
      "|    total_timesteps    | 238000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.5     |\n",
      "|    explained_variance | 0.133     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 47599     |\n",
      "|    policy_loss        | -3.07e+03 |\n",
      "|    reward             | 18.038486 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 5.44e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 47700     |\n",
      "|    time_elapsed       | 1150      |\n",
      "|    total_timesteps    | 238500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.6     |\n",
      "|    explained_variance | 0.071     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 47699     |\n",
      "|    policy_loss        | 1.58e+03  |\n",
      "|    reward             | -97.05859 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 3.54e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 47800      |\n",
      "|    time_elapsed       | 1153       |\n",
      "|    total_timesteps    | 239000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.7      |\n",
      "|    explained_variance | 0.203      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 47799      |\n",
      "|    policy_loss        | 171        |\n",
      "|    reward             | -17.310614 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 575        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 47900      |\n",
      "|    time_elapsed       | 1155       |\n",
      "|    total_timesteps    | 239500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.6      |\n",
      "|    explained_variance | 0.00358    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 47899      |\n",
      "|    policy_loss        | 2.85e+03   |\n",
      "|    reward             | -40.891537 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 4.82e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 48000     |\n",
      "|    time_elapsed       | 1157      |\n",
      "|    total_timesteps    | 240000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.6     |\n",
      "|    explained_variance | 0.402     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 47999     |\n",
      "|    policy_loss        | -1.64e+03 |\n",
      "|    reward             | 9.828076  |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 1.21e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 48100     |\n",
      "|    time_elapsed       | 1160      |\n",
      "|    total_timesteps    | 240500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.6     |\n",
      "|    explained_variance | -0.248    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 48099     |\n",
      "|    policy_loss        | 2.21e+03  |\n",
      "|    reward             | -39.41366 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 3.62e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 48200     |\n",
      "|    time_elapsed       | 1162      |\n",
      "|    total_timesteps    | 241000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.7     |\n",
      "|    explained_variance | 0.067     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 48199     |\n",
      "|    policy_loss        | 1.15e+03  |\n",
      "|    reward             | -36.57115 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 2.47e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 48300     |\n",
      "|    time_elapsed       | 1164      |\n",
      "|    total_timesteps    | 241500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.7     |\n",
      "|    explained_variance | 0.0776    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 48299     |\n",
      "|    policy_loss        | 3.47e+03  |\n",
      "|    reward             | -65.43846 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 8.09e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 48400     |\n",
      "|    time_elapsed       | 1167      |\n",
      "|    total_timesteps    | 242000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.7     |\n",
      "|    explained_variance | 0.0751    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 48399     |\n",
      "|    policy_loss        | -1.62e+03 |\n",
      "|    reward             | 41.47651  |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 6.38e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 207      |\n",
      "|    iterations         | 48500    |\n",
      "|    time_elapsed       | 1169     |\n",
      "|    total_timesteps    | 242500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -46.6    |\n",
      "|    explained_variance | 0.0393   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48499    |\n",
      "|    policy_loss        | 2.83e+03 |\n",
      "|    reward             | 2.388824 |\n",
      "|    std                | 1.15     |\n",
      "|    value_loss         | 5e+03    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 48600     |\n",
      "|    time_elapsed       | 1171      |\n",
      "|    total_timesteps    | 243000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.7     |\n",
      "|    explained_variance | 0.315     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 48599     |\n",
      "|    policy_loss        | 64.8      |\n",
      "|    reward             | -52.34544 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 120       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 48700     |\n",
      "|    time_elapsed       | 1173      |\n",
      "|    total_timesteps    | 243500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.7     |\n",
      "|    explained_variance | -0.0319   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 48699     |\n",
      "|    policy_loss        | -414      |\n",
      "|    reward             | 108.35895 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 1.53e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 207      |\n",
      "|    iterations         | 48800    |\n",
      "|    time_elapsed       | 1176     |\n",
      "|    total_timesteps    | 244000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -46.6    |\n",
      "|    explained_variance | 0.3      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48799    |\n",
      "|    policy_loss        | 2.27e+03 |\n",
      "|    reward             | 95.31847 |\n",
      "|    std                | 1.15     |\n",
      "|    value_loss         | 3.14e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 48900     |\n",
      "|    time_elapsed       | 1178      |\n",
      "|    total_timesteps    | 244500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.7     |\n",
      "|    explained_variance | -0.19     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 48899     |\n",
      "|    policy_loss        | -2.63e+03 |\n",
      "|    reward             | 157.08327 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 4.48e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 49000     |\n",
      "|    time_elapsed       | 1180      |\n",
      "|    total_timesteps    | 245000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.7     |\n",
      "|    explained_variance | 0.132     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 48999     |\n",
      "|    policy_loss        | 2.33e+03  |\n",
      "|    reward             | 43.767326 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 4.32e+03  |\n",
      "-------------------------------------\n",
      "day: 3102, episode: 80\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1889014.08\n",
      "total_reward: 889014.08\n",
      "total_cost: 4292.10\n",
      "total_trades: 38958\n",
      "Sharpe: 4.006\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 49100      |\n",
      "|    time_elapsed       | 1183       |\n",
      "|    total_timesteps    | 245500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.7      |\n",
      "|    explained_variance | 0.122      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 49099      |\n",
      "|    policy_loss        | 1.29e+03   |\n",
      "|    reward             | -71.157845 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 1.16e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 49200     |\n",
      "|    time_elapsed       | 1185      |\n",
      "|    total_timesteps    | 246000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.7     |\n",
      "|    explained_variance | -0.0422   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 49199     |\n",
      "|    policy_loss        | 738       |\n",
      "|    reward             | 58.148373 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 710       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 49300     |\n",
      "|    time_elapsed       | 1187      |\n",
      "|    total_timesteps    | 246500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.7     |\n",
      "|    explained_variance | -0.0286   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 49299     |\n",
      "|    policy_loss        | 4.48e+03  |\n",
      "|    reward             | 36.143818 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 9.65e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 207      |\n",
      "|    iterations         | 49400    |\n",
      "|    time_elapsed       | 1190     |\n",
      "|    total_timesteps    | 247000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -46.8    |\n",
      "|    explained_variance | 0.195    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49399    |\n",
      "|    policy_loss        | -1.9e+03 |\n",
      "|    reward             | 108.1661 |\n",
      "|    std                | 1.16     |\n",
      "|    value_loss         | 2.52e+03 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 49500      |\n",
      "|    time_elapsed       | 1192       |\n",
      "|    total_timesteps    | 247500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.8      |\n",
      "|    explained_variance | 0.265      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 49499      |\n",
      "|    policy_loss        | 6.02e+03   |\n",
      "|    reward             | -54.821396 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 1.7e+04    |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 49600     |\n",
      "|    time_elapsed       | 1194      |\n",
      "|    total_timesteps    | 248000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.8     |\n",
      "|    explained_variance | 0.0871    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 49599     |\n",
      "|    policy_loss        | 3.85e+03  |\n",
      "|    reward             | -61.97399 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 7.6e+03   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 207      |\n",
      "|    iterations         | 49700    |\n",
      "|    time_elapsed       | 1197     |\n",
      "|    total_timesteps    | 248500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -46.8    |\n",
      "|    explained_variance | 0.0131   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49699    |\n",
      "|    policy_loss        | 1.52e+03 |\n",
      "|    reward             | 75.8774  |\n",
      "|    std                | 1.16     |\n",
      "|    value_loss         | 1.82e+03 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 49800      |\n",
      "|    time_elapsed       | 1199       |\n",
      "|    total_timesteps    | 249000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.8      |\n",
      "|    explained_variance | -0.0555    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 49799      |\n",
      "|    policy_loss        | -829       |\n",
      "|    reward             | -15.791113 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 1.81e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 49900     |\n",
      "|    time_elapsed       | 1201      |\n",
      "|    total_timesteps    | 249500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.8     |\n",
      "|    explained_variance | 0.165     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 49899     |\n",
      "|    policy_loss        | -1.98e+03 |\n",
      "|    reward             | 101.55175 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 5.25e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 50000     |\n",
      "|    time_elapsed       | 1204      |\n",
      "|    total_timesteps    | 250000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.7     |\n",
      "|    explained_variance | 0.0978    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 49999     |\n",
      "|    policy_loss        | 799       |\n",
      "|    reward             | -59.19941 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 738       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 50100     |\n",
      "|    time_elapsed       | 1206      |\n",
      "|    total_timesteps    | 250500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.7     |\n",
      "|    explained_variance | 0.541     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 50099     |\n",
      "|    policy_loss        | -129      |\n",
      "|    reward             | -10.40839 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 428       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 50200     |\n",
      "|    time_elapsed       | 1208      |\n",
      "|    total_timesteps    | 251000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.7     |\n",
      "|    explained_variance | 0.0561    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 50199     |\n",
      "|    policy_loss        | -2.24e+03 |\n",
      "|    reward             | 91.49039  |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 4.99e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 50300     |\n",
      "|    time_elapsed       | 1210      |\n",
      "|    total_timesteps    | 251500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.7     |\n",
      "|    explained_variance | -0.161    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 50299     |\n",
      "|    policy_loss        | -826      |\n",
      "|    reward             | 21.356298 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 1.94e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 50400      |\n",
      "|    time_elapsed       | 1213       |\n",
      "|    total_timesteps    | 252000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.8      |\n",
      "|    explained_variance | 0.109      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 50399      |\n",
      "|    policy_loss        | 817        |\n",
      "|    reward             | -56.328957 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 1.42e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 50500     |\n",
      "|    time_elapsed       | 1215      |\n",
      "|    total_timesteps    | 252500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.8     |\n",
      "|    explained_variance | -0.00792  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 50499     |\n",
      "|    policy_loss        | 461       |\n",
      "|    reward             | 1.1881857 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 628       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 50600     |\n",
      "|    time_elapsed       | 1217      |\n",
      "|    total_timesteps    | 253000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.7     |\n",
      "|    explained_variance | 0.45      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 50599     |\n",
      "|    policy_loss        | 22.5      |\n",
      "|    reward             | 23.502426 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 173       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 207      |\n",
      "|    iterations         | 50700    |\n",
      "|    time_elapsed       | 1220     |\n",
      "|    total_timesteps    | 253500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -46.8    |\n",
      "|    explained_variance | -0.0958  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50699    |\n",
      "|    policy_loss        | -970     |\n",
      "|    reward             | 70.65807 |\n",
      "|    std                | 1.16     |\n",
      "|    value_loss         | 3.22e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 207      |\n",
      "|    iterations         | 50800    |\n",
      "|    time_elapsed       | 1222     |\n",
      "|    total_timesteps    | 254000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -46.8    |\n",
      "|    explained_variance | 0.058    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50799    |\n",
      "|    policy_loss        | 1.64e+03 |\n",
      "|    reward             | 68.8442  |\n",
      "|    std                | 1.16     |\n",
      "|    value_loss         | 5.5e+03  |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 50900      |\n",
      "|    time_elapsed       | 1224       |\n",
      "|    total_timesteps    | 254500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.8      |\n",
      "|    explained_variance | 0.253      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 50899      |\n",
      "|    policy_loss        | -423       |\n",
      "|    reward             | -50.732174 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 510        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 51000      |\n",
      "|    time_elapsed       | 1227       |\n",
      "|    total_timesteps    | 255000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.9      |\n",
      "|    explained_variance | -0.0137    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 50999      |\n",
      "|    policy_loss        | -1.06e+03  |\n",
      "|    reward             | -23.650229 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 2.45e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 51100     |\n",
      "|    time_elapsed       | 1233      |\n",
      "|    total_timesteps    | 255500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.9     |\n",
      "|    explained_variance | -0.206    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 51099     |\n",
      "|    policy_loss        | -1.85e+03 |\n",
      "|    reward             | 3.936473  |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 1.76e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 51200      |\n",
      "|    time_elapsed       | 1236       |\n",
      "|    total_timesteps    | 256000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.8      |\n",
      "|    explained_variance | 0.00517    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 51199      |\n",
      "|    policy_loss        | 3.39e+03   |\n",
      "|    reward             | -18.431633 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 6.78e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 51300     |\n",
      "|    time_elapsed       | 1238      |\n",
      "|    total_timesteps    | 256500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.8     |\n",
      "|    explained_variance | -0.257    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 51299     |\n",
      "|    policy_loss        | -1.56e+03 |\n",
      "|    reward             | -72.59679 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 2.13e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 51400      |\n",
      "|    time_elapsed       | 1240       |\n",
      "|    total_timesteps    | 257000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.8      |\n",
      "|    explained_variance | -0.0453    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 51399      |\n",
      "|    policy_loss        | 4.76e+03   |\n",
      "|    reward             | -131.04109 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 1.49e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 51500      |\n",
      "|    time_elapsed       | 1242       |\n",
      "|    total_timesteps    | 257500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.7      |\n",
      "|    explained_variance | 0.045      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 51499      |\n",
      "|    policy_loss        | -3.79e+03  |\n",
      "|    reward             | -227.69154 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 1.12e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 51600      |\n",
      "|    time_elapsed       | 1245       |\n",
      "|    total_timesteps    | 258000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.7      |\n",
      "|    explained_variance | -0.0573    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 51599      |\n",
      "|    policy_loss        | 3.25e+03   |\n",
      "|    reward             | -45.590908 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 6.52e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 51700      |\n",
      "|    time_elapsed       | 1247       |\n",
      "|    total_timesteps    | 258500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.7      |\n",
      "|    explained_variance | 0.186      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 51699      |\n",
      "|    policy_loss        | 55.9       |\n",
      "|    reward             | -75.551735 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 648        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 51800      |\n",
      "|    time_elapsed       | 1249       |\n",
      "|    total_timesteps    | 259000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.8      |\n",
      "|    explained_variance | -0.103     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 51799      |\n",
      "|    policy_loss        | -1.68e+03  |\n",
      "|    reward             | -11.736645 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 2.87e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 51900      |\n",
      "|    time_elapsed       | 1252       |\n",
      "|    total_timesteps    | 259500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.8      |\n",
      "|    explained_variance | 0.045      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 51899      |\n",
      "|    policy_loss        | 1.63e+03   |\n",
      "|    reward             | -40.683456 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 2.23e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 52000      |\n",
      "|    time_elapsed       | 1254       |\n",
      "|    total_timesteps    | 260000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.8      |\n",
      "|    explained_variance | -0.0281    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 51999      |\n",
      "|    policy_loss        | 2.94e+03   |\n",
      "|    reward             | -19.334167 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 6.67e+03   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 207      |\n",
      "|    iterations         | 52100    |\n",
      "|    time_elapsed       | 1256     |\n",
      "|    total_timesteps    | 260500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -46.8    |\n",
      "|    explained_variance | 0.00227  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52099    |\n",
      "|    policy_loss        | -1.5e+03 |\n",
      "|    reward             | 298.0279 |\n",
      "|    std                | 1.16     |\n",
      "|    value_loss         | 6.26e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 207      |\n",
      "|    iterations         | 52200    |\n",
      "|    time_elapsed       | 1259     |\n",
      "|    total_timesteps    | 261000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -46.9    |\n",
      "|    explained_variance | -0.0283  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52199    |\n",
      "|    policy_loss        | 783      |\n",
      "|    reward             | 78.99189 |\n",
      "|    std                | 1.17     |\n",
      "|    value_loss         | 4.54e+03 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 52300      |\n",
      "|    time_elapsed       | 1261       |\n",
      "|    total_timesteps    | 261500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.9      |\n",
      "|    explained_variance | 0.112      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 52299      |\n",
      "|    policy_loss        | -390       |\n",
      "|    reward             | -32.625854 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 1.17e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 52400      |\n",
      "|    time_elapsed       | 1263       |\n",
      "|    total_timesteps    | 262000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.9      |\n",
      "|    explained_variance | 0.0917     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 52399      |\n",
      "|    policy_loss        | -1.05e+03  |\n",
      "|    reward             | -2.5742853 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 2.18e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 52500      |\n",
      "|    time_elapsed       | 1266       |\n",
      "|    total_timesteps    | 262500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47        |\n",
      "|    explained_variance | 0.0247     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 52499      |\n",
      "|    policy_loss        | -675       |\n",
      "|    reward             | -111.70168 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 6.07e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 52600     |\n",
      "|    time_elapsed       | 1268      |\n",
      "|    total_timesteps    | 263000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47       |\n",
      "|    explained_variance | 0.0876    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 52599     |\n",
      "|    policy_loss        | 2.85e+03  |\n",
      "|    reward             | -97.99153 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 5.91e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 52700     |\n",
      "|    time_elapsed       | 1270      |\n",
      "|    total_timesteps    | 263500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.9     |\n",
      "|    explained_variance | 0.00579   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 52699     |\n",
      "|    policy_loss        | 2e+03     |\n",
      "|    reward             | 136.46011 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 2.18e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 52800     |\n",
      "|    time_elapsed       | 1273      |\n",
      "|    total_timesteps    | 264000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47       |\n",
      "|    explained_variance | 0.0101    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 52799     |\n",
      "|    policy_loss        | -586      |\n",
      "|    reward             | 10.542365 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 9.65e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 52900      |\n",
      "|    time_elapsed       | 1275       |\n",
      "|    total_timesteps    | 264500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.1      |\n",
      "|    explained_variance | -0.184     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 52899      |\n",
      "|    policy_loss        | -2.86e+03  |\n",
      "|    reward             | -10.774341 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 5.77e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 53000     |\n",
      "|    time_elapsed       | 1277      |\n",
      "|    total_timesteps    | 265000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.1     |\n",
      "|    explained_variance | 0.0395    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 52999     |\n",
      "|    policy_loss        | 1.41e+03  |\n",
      "|    reward             | 137.52914 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 1.75e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 53100      |\n",
      "|    time_elapsed       | 1280       |\n",
      "|    total_timesteps    | 265500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47        |\n",
      "|    explained_variance | 0.0388     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 53099      |\n",
      "|    policy_loss        | -1.82e+03  |\n",
      "|    reward             | 120.827065 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 1.42e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 53200     |\n",
      "|    time_elapsed       | 1282      |\n",
      "|    total_timesteps    | 266000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.1     |\n",
      "|    explained_variance | -0.00622  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 53199     |\n",
      "|    policy_loss        | -1.66e+03 |\n",
      "|    reward             | 102.17611 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 3.31e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 53300     |\n",
      "|    time_elapsed       | 1284      |\n",
      "|    total_timesteps    | 266500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.1     |\n",
      "|    explained_variance | 0.0616    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 53299     |\n",
      "|    policy_loss        | -3.54e+03 |\n",
      "|    reward             | -89.76043 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 1.26e+04  |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 207         |\n",
      "|    iterations         | 53400       |\n",
      "|    time_elapsed       | 1287        |\n",
      "|    total_timesteps    | 267000      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -47.1       |\n",
      "|    explained_variance | 0.108       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 53399       |\n",
      "|    policy_loss        | -831        |\n",
      "|    reward             | -14.5645075 |\n",
      "|    std                | 1.18        |\n",
      "|    value_loss         | 1.5e+03     |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 53500     |\n",
      "|    time_elapsed       | 1289      |\n",
      "|    total_timesteps    | 267500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.1     |\n",
      "|    explained_variance | 0.226     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 53499     |\n",
      "|    policy_loss        | 540       |\n",
      "|    reward             | -43.01709 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 581       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 53600     |\n",
      "|    time_elapsed       | 1291      |\n",
      "|    total_timesteps    | 268000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.1     |\n",
      "|    explained_variance | 0.199     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 53599     |\n",
      "|    policy_loss        | 723       |\n",
      "|    reward             | -66.96278 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 1.78e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 53700     |\n",
      "|    time_elapsed       | 1293      |\n",
      "|    total_timesteps    | 268500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.1     |\n",
      "|    explained_variance | 0.396     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 53699     |\n",
      "|    policy_loss        | 5.68e+03  |\n",
      "|    reward             | -69.79011 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 1.6e+04   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 53800     |\n",
      "|    time_elapsed       | 1296      |\n",
      "|    total_timesteps    | 269000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.2     |\n",
      "|    explained_variance | 0.134     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 53799     |\n",
      "|    policy_loss        | 4.07e+03  |\n",
      "|    reward             | 307.27353 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 1.02e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 53900     |\n",
      "|    time_elapsed       | 1298      |\n",
      "|    total_timesteps    | 269500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.2     |\n",
      "|    explained_variance | 0.144     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 53899     |\n",
      "|    policy_loss        | 2.15e+03  |\n",
      "|    reward             | -44.83143 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 3.09e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 54000     |\n",
      "|    time_elapsed       | 1300      |\n",
      "|    total_timesteps    | 270000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.2     |\n",
      "|    explained_variance | -0.375    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 53999     |\n",
      "|    policy_loss        | -86.3     |\n",
      "|    reward             | 2.9557042 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 156       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 54100     |\n",
      "|    time_elapsed       | 1303      |\n",
      "|    total_timesteps    | 270500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.3     |\n",
      "|    explained_variance | 0.0431    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 54099     |\n",
      "|    policy_loss        | 3.65e+03  |\n",
      "|    reward             | 27.966602 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 9.18e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 207      |\n",
      "|    iterations         | 54200    |\n",
      "|    time_elapsed       | 1305     |\n",
      "|    total_timesteps    | 271000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -47.2    |\n",
      "|    explained_variance | -0.136   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54199    |\n",
      "|    policy_loss        | 1.25e+03 |\n",
      "|    reward             | 9.769451 |\n",
      "|    std                | 1.18     |\n",
      "|    value_loss         | 2.35e+03 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 54300      |\n",
      "|    time_elapsed       | 1307       |\n",
      "|    total_timesteps    | 271500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.2      |\n",
      "|    explained_variance | 0.107      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 54299      |\n",
      "|    policy_loss        | 6.08e+03   |\n",
      "|    reward             | -43.702957 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 2.05e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 54400      |\n",
      "|    time_elapsed       | 1310       |\n",
      "|    total_timesteps    | 272000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.2      |\n",
      "|    explained_variance | 0.149      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 54399      |\n",
      "|    policy_loss        | 1.28e+03   |\n",
      "|    reward             | -143.19154 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 2.18e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 54500      |\n",
      "|    time_elapsed       | 1312       |\n",
      "|    total_timesteps    | 272500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.2      |\n",
      "|    explained_variance | 0.074      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 54499      |\n",
      "|    policy_loss        | -1.58e+03  |\n",
      "|    reward             | -135.88756 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 1.1e+04    |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 54600     |\n",
      "|    time_elapsed       | 1314      |\n",
      "|    total_timesteps    | 273000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.3     |\n",
      "|    explained_variance | 0.0271    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 54599     |\n",
      "|    policy_loss        | 291       |\n",
      "|    reward             | 135.34232 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 4.66e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 54700      |\n",
      "|    time_elapsed       | 1317       |\n",
      "|    total_timesteps    | 273500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.2      |\n",
      "|    explained_variance | 0.116      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 54699      |\n",
      "|    policy_loss        | 820        |\n",
      "|    reward             | -21.166002 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 2.11e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 54800     |\n",
      "|    time_elapsed       | 1319      |\n",
      "|    total_timesteps    | 274000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.2     |\n",
      "|    explained_variance | 0.0825    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 54799     |\n",
      "|    policy_loss        | 1.96e+03  |\n",
      "|    reward             | -42.61523 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 2.83e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 54900      |\n",
      "|    time_elapsed       | 1321       |\n",
      "|    total_timesteps    | 274500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.3      |\n",
      "|    explained_variance | -0.0393    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 54899      |\n",
      "|    policy_loss        | 2.8e+03    |\n",
      "|    reward             | -18.058329 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 5.09e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 55000     |\n",
      "|    time_elapsed       | 1324      |\n",
      "|    total_timesteps    | 275000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.3     |\n",
      "|    explained_variance | -0.105    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 54999     |\n",
      "|    policy_loss        | -1.58e+03 |\n",
      "|    reward             | -8.181682 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 1.4e+03   |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 55100      |\n",
      "|    time_elapsed       | 1326       |\n",
      "|    total_timesteps    | 275500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.3      |\n",
      "|    explained_variance | -0.0392    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 55099      |\n",
      "|    policy_loss        | -829       |\n",
      "|    reward             | -25.352245 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 5.64e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 55200      |\n",
      "|    time_elapsed       | 1328       |\n",
      "|    total_timesteps    | 276000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.3      |\n",
      "|    explained_variance | -0.00127   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 55199      |\n",
      "|    policy_loss        | -5.96e+03  |\n",
      "|    reward             | -117.20825 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 2.69e+04   |\n",
      "--------------------------------------\n",
      "day: 3102, episode: 90\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3288109.07\n",
      "total_reward: 2288109.07\n",
      "total_cost: 8982.00\n",
      "total_trades: 52229\n",
      "Sharpe: 4.110\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 55300     |\n",
      "|    time_elapsed       | 1330      |\n",
      "|    total_timesteps    | 276500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.3     |\n",
      "|    explained_variance | 0.228     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 55299     |\n",
      "|    policy_loss        | -1.88e+03 |\n",
      "|    reward             | 9.657061  |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 2.38e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 55400     |\n",
      "|    time_elapsed       | 1333      |\n",
      "|    total_timesteps    | 277000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.4     |\n",
      "|    explained_variance | 0.179     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 55399     |\n",
      "|    policy_loss        | -1.46e+03 |\n",
      "|    reward             | 59.083347 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 4.97e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 55500     |\n",
      "|    time_elapsed       | 1335      |\n",
      "|    total_timesteps    | 277500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.3     |\n",
      "|    explained_variance | 0.323     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 55499     |\n",
      "|    policy_loss        | 2e+03     |\n",
      "|    reward             | 13.531379 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 1.8e+03   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 207      |\n",
      "|    iterations         | 55600    |\n",
      "|    time_elapsed       | 1337     |\n",
      "|    total_timesteps    | 278000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -47.3    |\n",
      "|    explained_variance | -0.0648  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55599    |\n",
      "|    policy_loss        | 418      |\n",
      "|    reward             | 5.598208 |\n",
      "|    std                | 1.18     |\n",
      "|    value_loss         | 596      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 207      |\n",
      "|    iterations         | 55700    |\n",
      "|    time_elapsed       | 1340     |\n",
      "|    total_timesteps    | 278500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -47.4    |\n",
      "|    explained_variance | 0.231    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55699    |\n",
      "|    policy_loss        | 2.51e+03 |\n",
      "|    reward             | 4.745579 |\n",
      "|    std                | 1.19     |\n",
      "|    value_loss         | 3.95e+03 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 55800      |\n",
      "|    time_elapsed       | 1342       |\n",
      "|    total_timesteps    | 279000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.4      |\n",
      "|    explained_variance | 0.00426    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 55799      |\n",
      "|    policy_loss        | -8.98e+03  |\n",
      "|    reward             | -241.61227 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 5.02e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 55900      |\n",
      "|    time_elapsed       | 1344       |\n",
      "|    total_timesteps    | 279500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.4      |\n",
      "|    explained_variance | -0.0128    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 55899      |\n",
      "|    policy_loss        | 914        |\n",
      "|    reward             | -29.794754 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 4.63e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 56000      |\n",
      "|    time_elapsed       | 1347       |\n",
      "|    total_timesteps    | 280000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.4      |\n",
      "|    explained_variance | -0.334     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 55999      |\n",
      "|    policy_loss        | 1.23e+03   |\n",
      "|    reward             | -23.975706 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 1.1e+03    |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 56100     |\n",
      "|    time_elapsed       | 1349      |\n",
      "|    total_timesteps    | 280500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.4     |\n",
      "|    explained_variance | 0.12      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 56099     |\n",
      "|    policy_loss        | 1.68e+03  |\n",
      "|    reward             | -8.023911 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 1.51e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 56200      |\n",
      "|    time_elapsed       | 1351       |\n",
      "|    total_timesteps    | 281000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.4      |\n",
      "|    explained_variance | 0.0203     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 56199      |\n",
      "|    policy_loss        | 562        |\n",
      "|    reward             | -109.00761 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 1.75e+03   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 207      |\n",
      "|    iterations         | 56300    |\n",
      "|    time_elapsed       | 1354     |\n",
      "|    total_timesteps    | 281500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -47.4    |\n",
      "|    explained_variance | 0.0753   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56299    |\n",
      "|    policy_loss        | 3.47e+03 |\n",
      "|    reward             | 85.15436 |\n",
      "|    std                | 1.19     |\n",
      "|    value_loss         | 1.01e+04 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 56400     |\n",
      "|    time_elapsed       | 1356      |\n",
      "|    total_timesteps    | 282000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.4     |\n",
      "|    explained_variance | -0.0472   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 56399     |\n",
      "|    policy_loss        | 1.07e+04  |\n",
      "|    reward             | -27.64795 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 6.14e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 56500     |\n",
      "|    time_elapsed       | 1358      |\n",
      "|    total_timesteps    | 282500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.4     |\n",
      "|    explained_variance | 0.225     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 56499     |\n",
      "|    policy_loss        | 430       |\n",
      "|    reward             | -98.82313 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 580       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 207      |\n",
      "|    iterations         | 56600    |\n",
      "|    time_elapsed       | 1361     |\n",
      "|    total_timesteps    | 283000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -47.4    |\n",
      "|    explained_variance | 0.127    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56599    |\n",
      "|    policy_loss        | 5.5e+03  |\n",
      "|    reward             | 48.27407 |\n",
      "|    std                | 1.19     |\n",
      "|    value_loss         | 1.55e+04 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 56700      |\n",
      "|    time_elapsed       | 1363       |\n",
      "|    total_timesteps    | 283500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.4      |\n",
      "|    explained_variance | 0.236      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 56699      |\n",
      "|    policy_loss        | 576        |\n",
      "|    reward             | -17.077381 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 1.54e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 56800     |\n",
      "|    time_elapsed       | 1365      |\n",
      "|    total_timesteps    | 284000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.4     |\n",
      "|    explained_variance | -0.108    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 56799     |\n",
      "|    policy_loss        | -461      |\n",
      "|    reward             | -7.798654 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 4.19e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 207       |\n",
      "|    iterations         | 56900     |\n",
      "|    time_elapsed       | 1367      |\n",
      "|    total_timesteps    | 284500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.4     |\n",
      "|    explained_variance | 0.16      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 56899     |\n",
      "|    policy_loss        | -2.87e+03 |\n",
      "|    reward             | -88.02875 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 8.25e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 207      |\n",
      "|    iterations         | 57000    |\n",
      "|    time_elapsed       | 1370     |\n",
      "|    total_timesteps    | 285000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -47.5    |\n",
      "|    explained_variance | 0.0101   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56999    |\n",
      "|    policy_loss        | -890     |\n",
      "|    reward             | 30.62129 |\n",
      "|    std                | 1.19     |\n",
      "|    value_loss         | 3.04e+03 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 57100      |\n",
      "|    time_elapsed       | 1372       |\n",
      "|    total_timesteps    | 285500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.5      |\n",
      "|    explained_variance | -0.595     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 57099      |\n",
      "|    policy_loss        | -1.68e+03  |\n",
      "|    reward             | 0.60006684 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 1.23e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 57200      |\n",
      "|    time_elapsed       | 1375       |\n",
      "|    total_timesteps    | 286000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.5      |\n",
      "|    explained_variance | -0.157     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 57199      |\n",
      "|    policy_loss        | 4.24e+03   |\n",
      "|    reward             | -36.917233 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 9.26e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 57300      |\n",
      "|    time_elapsed       | 1377       |\n",
      "|    total_timesteps    | 286500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.5      |\n",
      "|    explained_variance | 0.0688     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 57299      |\n",
      "|    policy_loss        | -1.89e+03  |\n",
      "|    reward             | -74.370316 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 1.89e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 57400      |\n",
      "|    time_elapsed       | 1379       |\n",
      "|    total_timesteps    | 287000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.5      |\n",
      "|    explained_variance | -0.123     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 57399      |\n",
      "|    policy_loss        | -3.54e+03  |\n",
      "|    reward             | -22.434576 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 8.72e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 57500      |\n",
      "|    time_elapsed       | 1381       |\n",
      "|    total_timesteps    | 287500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.4      |\n",
      "|    explained_variance | -0.351     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 57499      |\n",
      "|    policy_loss        | 2.37e+03   |\n",
      "|    reward             | -1.7435393 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 3.31e+03   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 57600    |\n",
      "|    time_elapsed       | 1384     |\n",
      "|    total_timesteps    | 288000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -47.5    |\n",
      "|    explained_variance | 0.272    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57599    |\n",
      "|    policy_loss        | -134     |\n",
      "|    reward             | 74.07446 |\n",
      "|    std                | 1.19     |\n",
      "|    value_loss         | 1.06e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 57700     |\n",
      "|    time_elapsed       | 1386      |\n",
      "|    total_timesteps    | 288500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.4     |\n",
      "|    explained_variance | 0.0892    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 57699     |\n",
      "|    policy_loss        | -300      |\n",
      "|    reward             | 12.647948 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 2.06e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 57800     |\n",
      "|    time_elapsed       | 1389      |\n",
      "|    total_timesteps    | 289000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.4     |\n",
      "|    explained_variance | -0.0801   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 57799     |\n",
      "|    policy_loss        | -131      |\n",
      "|    reward             | 4.0783315 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 447       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 57900     |\n",
      "|    time_elapsed       | 1391      |\n",
      "|    total_timesteps    | 289500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.4     |\n",
      "|    explained_variance | -0.393    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 57899     |\n",
      "|    policy_loss        | -1.18e+03 |\n",
      "|    reward             | 31.197897 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 832       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 58000      |\n",
      "|    time_elapsed       | 1393       |\n",
      "|    total_timesteps    | 290000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.4      |\n",
      "|    explained_variance | 0.173      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 57999      |\n",
      "|    policy_loss        | -987       |\n",
      "|    reward             | -57.596832 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 1.83e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 58100     |\n",
      "|    time_elapsed       | 1396      |\n",
      "|    total_timesteps    | 290500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.4     |\n",
      "|    explained_variance | -0.0369   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 58099     |\n",
      "|    policy_loss        | -1.88e+03 |\n",
      "|    reward             | 113.67563 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 2.24e+03  |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 208         |\n",
      "|    iterations         | 58200       |\n",
      "|    time_elapsed       | 1398        |\n",
      "|    total_timesteps    | 291000      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -47.4       |\n",
      "|    explained_variance | 0.122       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 58199       |\n",
      "|    policy_loss        | -1.46e+03   |\n",
      "|    reward             | -108.642006 |\n",
      "|    std                | 1.19        |\n",
      "|    value_loss         | 3.61e+03    |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 58300      |\n",
      "|    time_elapsed       | 1400       |\n",
      "|    total_timesteps    | 291500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.5      |\n",
      "|    explained_variance | 0.0724     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 58299      |\n",
      "|    policy_loss        | -4.24e+03  |\n",
      "|    reward             | -65.125046 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 1.09e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 58400      |\n",
      "|    time_elapsed       | 1403       |\n",
      "|    total_timesteps    | 292000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.5      |\n",
      "|    explained_variance | 0.42       |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 58399      |\n",
      "|    policy_loss        | 420        |\n",
      "|    reward             | -13.575156 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 586        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 58500     |\n",
      "|    time_elapsed       | 1405      |\n",
      "|    total_timesteps    | 292500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.4     |\n",
      "|    explained_variance | 0.157     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 58499     |\n",
      "|    policy_loss        | -320      |\n",
      "|    reward             | 79.448265 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 419       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 58600      |\n",
      "|    time_elapsed       | 1407       |\n",
      "|    total_timesteps    | 293000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.3      |\n",
      "|    explained_variance | 0.144      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 58599      |\n",
      "|    policy_loss        | -1.47e+03  |\n",
      "|    reward             | -19.859451 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 2.2e+03    |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 58700     |\n",
      "|    time_elapsed       | 1410      |\n",
      "|    total_timesteps    | 293500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.4     |\n",
      "|    explained_variance | 0.179     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 58699     |\n",
      "|    policy_loss        | 840       |\n",
      "|    reward             | 52.604603 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 1.22e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 58800    |\n",
      "|    time_elapsed       | 1412     |\n",
      "|    total_timesteps    | 294000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -47.3    |\n",
      "|    explained_variance | 0.231    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58799    |\n",
      "|    policy_loss        | 630      |\n",
      "|    reward             | 85.9719  |\n",
      "|    std                | 1.19     |\n",
      "|    value_loss         | 440      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 58900     |\n",
      "|    time_elapsed       | 1414      |\n",
      "|    total_timesteps    | 294500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.4     |\n",
      "|    explained_variance | 0.534     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 58899     |\n",
      "|    policy_loss        | 4.16e+03  |\n",
      "|    reward             | -6.346086 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 8.62e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 59000     |\n",
      "|    time_elapsed       | 1417      |\n",
      "|    total_timesteps    | 295000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.4     |\n",
      "|    explained_variance | 0.506     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 58999     |\n",
      "|    policy_loss        | -540      |\n",
      "|    reward             | -4.537134 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 500       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 59100     |\n",
      "|    time_elapsed       | 1419      |\n",
      "|    total_timesteps    | 295500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.4     |\n",
      "|    explained_variance | 0.172     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 59099     |\n",
      "|    policy_loss        | -445      |\n",
      "|    reward             | 13.762042 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 617       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 59200    |\n",
      "|    time_elapsed       | 1421     |\n",
      "|    total_timesteps    | 296000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -47.4    |\n",
      "|    explained_variance | 0.191    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59199    |\n",
      "|    policy_loss        | -998     |\n",
      "|    reward             | 4.712037 |\n",
      "|    std                | 1.19     |\n",
      "|    value_loss         | 546      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 59300      |\n",
      "|    time_elapsed       | 1424       |\n",
      "|    total_timesteps    | 296500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.4      |\n",
      "|    explained_variance | 0.103      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 59299      |\n",
      "|    policy_loss        | 1.08e+03   |\n",
      "|    reward             | -20.943115 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 1.02e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 59400     |\n",
      "|    time_elapsed       | 1426      |\n",
      "|    total_timesteps    | 297000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.3     |\n",
      "|    explained_variance | -0.137    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 59399     |\n",
      "|    policy_loss        | 979       |\n",
      "|    reward             | 40.682343 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 4.17e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 59500     |\n",
      "|    time_elapsed       | 1428      |\n",
      "|    total_timesteps    | 297500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.3     |\n",
      "|    explained_variance | -0.0744   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 59499     |\n",
      "|    policy_loss        | 5.45e+03  |\n",
      "|    reward             | 114.62415 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 2.18e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 59600     |\n",
      "|    time_elapsed       | 1431      |\n",
      "|    total_timesteps    | 298000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.4     |\n",
      "|    explained_variance | -0.158    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 59599     |\n",
      "|    policy_loss        | -978      |\n",
      "|    reward             | 61.786385 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 872       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 59700     |\n",
      "|    time_elapsed       | 1433      |\n",
      "|    total_timesteps    | 298500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.4     |\n",
      "|    explained_variance | 0.0693    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 59699     |\n",
      "|    policy_loss        | -468      |\n",
      "|    reward             | -73.23781 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 789       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 59800     |\n",
      "|    time_elapsed       | 1435      |\n",
      "|    total_timesteps    | 299000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.4     |\n",
      "|    explained_variance | -0.138    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 59799     |\n",
      "|    policy_loss        | 175       |\n",
      "|    reward             | 22.812262 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 494       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 59900     |\n",
      "|    time_elapsed       | 1438      |\n",
      "|    total_timesteps    | 299500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.5     |\n",
      "|    explained_variance | -0.176    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 59899     |\n",
      "|    policy_loss        | -670      |\n",
      "|    reward             | 16.500212 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 1.66e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 60000      |\n",
      "|    time_elapsed       | 1440       |\n",
      "|    total_timesteps    | 300000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.5      |\n",
      "|    explained_variance | 0.109      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 59999      |\n",
      "|    policy_loss        | 1.13e+03   |\n",
      "|    reward             | -44.826004 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 1.35e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 60100      |\n",
      "|    time_elapsed       | 1442       |\n",
      "|    total_timesteps    | 300500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.5      |\n",
      "|    explained_variance | 0.0449     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 60099      |\n",
      "|    policy_loss        | 4.8e+03    |\n",
      "|    reward             | -16.212791 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 1.58e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 60200      |\n",
      "|    time_elapsed       | 1445       |\n",
      "|    total_timesteps    | 301000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.5      |\n",
      "|    explained_variance | 0.0312     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 60199      |\n",
      "|    policy_loss        | 2.86e+03   |\n",
      "|    reward             | -22.148638 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 1.7e+04    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 60300      |\n",
      "|    time_elapsed       | 1447       |\n",
      "|    total_timesteps    | 301500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.5      |\n",
      "|    explained_variance | 0.0852     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 60299      |\n",
      "|    policy_loss        | -2.63e+03  |\n",
      "|    reward             | -39.077267 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 4.89e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 60400     |\n",
      "|    time_elapsed       | 1449      |\n",
      "|    total_timesteps    | 302000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.6     |\n",
      "|    explained_variance | 0.176     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 60399     |\n",
      "|    policy_loss        | -1.55e+03 |\n",
      "|    reward             | -5.352251 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 1.54e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 60500      |\n",
      "|    time_elapsed       | 1452       |\n",
      "|    total_timesteps    | 302500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.5      |\n",
      "|    explained_variance | 0.203      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 60499      |\n",
      "|    policy_loss        | -1.6e+03   |\n",
      "|    reward             | -23.267706 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 1.32e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 60600      |\n",
      "|    time_elapsed       | 1454       |\n",
      "|    total_timesteps    | 303000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.5      |\n",
      "|    explained_variance | 0.0633     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 60599      |\n",
      "|    policy_loss        | -2.62e+03  |\n",
      "|    reward             | -60.559414 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 6.14e+03   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 60700    |\n",
      "|    time_elapsed       | 1456     |\n",
      "|    total_timesteps    | 303500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -47.5    |\n",
      "|    explained_variance | -0.166   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60699    |\n",
      "|    policy_loss        | 7.08e+03 |\n",
      "|    reward             | -7.55159 |\n",
      "|    std                | 1.19     |\n",
      "|    value_loss         | 2.27e+04 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 60800     |\n",
      "|    time_elapsed       | 1459      |\n",
      "|    total_timesteps    | 304000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.4     |\n",
      "|    explained_variance | 0.038     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 60799     |\n",
      "|    policy_loss        | 4.51e+03  |\n",
      "|    reward             | 138.96338 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 1.09e+04  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 60900    |\n",
      "|    time_elapsed       | 1461     |\n",
      "|    total_timesteps    | 304500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -47.5    |\n",
      "|    explained_variance | 0.076    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60899    |\n",
      "|    policy_loss        | 721      |\n",
      "|    reward             | 8.521886 |\n",
      "|    std                | 1.19     |\n",
      "|    value_loss         | 1.02e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 61000     |\n",
      "|    time_elapsed       | 1463      |\n",
      "|    total_timesteps    | 305000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.5     |\n",
      "|    explained_variance | 0.202     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 60999     |\n",
      "|    policy_loss        | 1.55e+03  |\n",
      "|    reward             | 26.132017 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 1.38e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 61100     |\n",
      "|    time_elapsed       | 1465      |\n",
      "|    total_timesteps    | 305500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.4     |\n",
      "|    explained_variance | 0.114     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 61099     |\n",
      "|    policy_loss        | -2.92e+03 |\n",
      "|    reward             | 72.43105  |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 6.48e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 61200      |\n",
      "|    time_elapsed       | 1468       |\n",
      "|    total_timesteps    | 306000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.3      |\n",
      "|    explained_variance | 0.228      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 61199      |\n",
      "|    policy_loss        | -486       |\n",
      "|    reward             | -127.73837 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 2.48e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 61300     |\n",
      "|    time_elapsed       | 1470      |\n",
      "|    total_timesteps    | 306500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.4     |\n",
      "|    explained_variance | 0.228     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 61299     |\n",
      "|    policy_loss        | -2.22e+03 |\n",
      "|    reward             | 42.084232 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 8.36e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 61400      |\n",
      "|    time_elapsed       | 1472       |\n",
      "|    total_timesteps    | 307000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.4      |\n",
      "|    explained_variance | -0.0308    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 61399      |\n",
      "|    policy_loss        | -4.22e+03  |\n",
      "|    reward             | -102.97578 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 1.51e+04   |\n",
      "--------------------------------------\n",
      "day: 3102, episode: 100\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2798200.13\n",
      "total_reward: 1798200.13\n",
      "total_cost: 11313.11\n",
      "total_trades: 50528\n",
      "Sharpe: 3.984\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 61500     |\n",
      "|    time_elapsed       | 1475      |\n",
      "|    total_timesteps    | 307500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.4     |\n",
      "|    explained_variance | 0.149     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 61499     |\n",
      "|    policy_loss        | 2.24e+03  |\n",
      "|    reward             | 93.572495 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 3.17e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 61600     |\n",
      "|    time_elapsed       | 1477      |\n",
      "|    total_timesteps    | 308000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.4     |\n",
      "|    explained_variance | -0.0144   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 61599     |\n",
      "|    policy_loss        | -2.13e+03 |\n",
      "|    reward             | 125.91425 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 4.77e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 61700      |\n",
      "|    time_elapsed       | 1479       |\n",
      "|    total_timesteps    | 308500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.4      |\n",
      "|    explained_variance | -0.0606    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 61699      |\n",
      "|    policy_loss        | -948       |\n",
      "|    reward             | -6.0139036 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 2.08e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 61800     |\n",
      "|    time_elapsed       | 1482      |\n",
      "|    total_timesteps    | 309000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.4     |\n",
      "|    explained_variance | 0.149     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 61799     |\n",
      "|    policy_loss        | 1.43e+03  |\n",
      "|    reward             | -86.79866 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 2.46e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 61900     |\n",
      "|    time_elapsed       | 1484      |\n",
      "|    total_timesteps    | 309500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.4     |\n",
      "|    explained_variance | -0.0039   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 61899     |\n",
      "|    policy_loss        | 3.29e+03  |\n",
      "|    reward             | -15.15227 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 7.69e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 62000     |\n",
      "|    time_elapsed       | 1486      |\n",
      "|    total_timesteps    | 310000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.5     |\n",
      "|    explained_variance | 0.125     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 61999     |\n",
      "|    policy_loss        | -1.11e+03 |\n",
      "|    reward             | 61.153442 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 2.63e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 62100     |\n",
      "|    time_elapsed       | 1489      |\n",
      "|    total_timesteps    | 310500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.5     |\n",
      "|    explained_variance | -0.156    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 62099     |\n",
      "|    policy_loss        | 1.36e+03  |\n",
      "|    reward             | 44.134907 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 1.23e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 62200    |\n",
      "|    time_elapsed       | 1491     |\n",
      "|    total_timesteps    | 311000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -47.5    |\n",
      "|    explained_variance | -0.086   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62199    |\n",
      "|    policy_loss        | 922      |\n",
      "|    reward             | 6.458722 |\n",
      "|    std                | 1.19     |\n",
      "|    value_loss         | 802      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 62300     |\n",
      "|    time_elapsed       | 1493      |\n",
      "|    total_timesteps    | 311500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.6     |\n",
      "|    explained_variance | 0.308     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 62299     |\n",
      "|    policy_loss        | 334       |\n",
      "|    reward             | 37.210617 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 458       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 62400     |\n",
      "|    time_elapsed       | 1496      |\n",
      "|    total_timesteps    | 312000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.6     |\n",
      "|    explained_variance | -1.21     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 62399     |\n",
      "|    policy_loss        | 2.03e+03  |\n",
      "|    reward             | 0.3717295 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 1.91e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 62500     |\n",
      "|    time_elapsed       | 1498      |\n",
      "|    total_timesteps    | 312500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.6     |\n",
      "|    explained_variance | 0.0353    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 62499     |\n",
      "|    policy_loss        | -3.26e+03 |\n",
      "|    reward             | 23.35074  |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 5.38e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 62600     |\n",
      "|    time_elapsed       | 1500      |\n",
      "|    total_timesteps    | 313000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.7     |\n",
      "|    explained_variance | -0.0797   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 62599     |\n",
      "|    policy_loss        | -2.68e+03 |\n",
      "|    reward             | 141.37668 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 8.02e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 62700     |\n",
      "|    time_elapsed       | 1503      |\n",
      "|    total_timesteps    | 313500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.6     |\n",
      "|    explained_variance | -1.02     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 62699     |\n",
      "|    policy_loss        | 2.86e+03  |\n",
      "|    reward             | -40.78535 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 3.96e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 62800     |\n",
      "|    time_elapsed       | 1505      |\n",
      "|    total_timesteps    | 314000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.6     |\n",
      "|    explained_variance | 0.0713    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 62799     |\n",
      "|    policy_loss        | 4.17e+03  |\n",
      "|    reward             | 28.609304 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 1.43e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 62900     |\n",
      "|    time_elapsed       | 1507      |\n",
      "|    total_timesteps    | 314500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.7     |\n",
      "|    explained_variance | -0.186    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 62899     |\n",
      "|    policy_loss        | -344      |\n",
      "|    reward             | 2.8473618 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 646       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 63000     |\n",
      "|    time_elapsed       | 1509      |\n",
      "|    total_timesteps    | 315000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.7     |\n",
      "|    explained_variance | 0.369     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 62999     |\n",
      "|    policy_loss        | -323      |\n",
      "|    reward             | 23.798534 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 995       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 63100      |\n",
      "|    time_elapsed       | 1512       |\n",
      "|    total_timesteps    | 315500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.7      |\n",
      "|    explained_variance | -0.275     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 63099      |\n",
      "|    policy_loss        | -1.14e+03  |\n",
      "|    reward             | -127.42875 |\n",
      "|    std                | 1.2        |\n",
      "|    value_loss         | 1.64e+03   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 63200    |\n",
      "|    time_elapsed       | 1514     |\n",
      "|    total_timesteps    | 316000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -47.8    |\n",
      "|    explained_variance | -0.0621  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63199    |\n",
      "|    policy_loss        | -554     |\n",
      "|    reward             | 65.40141 |\n",
      "|    std                | 1.2      |\n",
      "|    value_loss         | 5.03e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 63300     |\n",
      "|    time_elapsed       | 1516      |\n",
      "|    total_timesteps    | 316500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.7     |\n",
      "|    explained_variance | 0.0842    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 63299     |\n",
      "|    policy_loss        | -1.48e+03 |\n",
      "|    reward             | 168.09544 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 6.93e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 63400     |\n",
      "|    time_elapsed       | 1519      |\n",
      "|    total_timesteps    | 317000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.7     |\n",
      "|    explained_variance | 0.139     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 63399     |\n",
      "|    policy_loss        | -2.32e+03 |\n",
      "|    reward             | 23.641378 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 2.59e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 63500    |\n",
      "|    time_elapsed       | 1521     |\n",
      "|    total_timesteps    | 317500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -47.7    |\n",
      "|    explained_variance | 0.121    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63499    |\n",
      "|    policy_loss        | 144      |\n",
      "|    reward             | 3.615263 |\n",
      "|    std                | 1.2      |\n",
      "|    value_loss         | 181      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 63600     |\n",
      "|    time_elapsed       | 1523      |\n",
      "|    total_timesteps    | 318000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.7     |\n",
      "|    explained_variance | 0.111     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 63599     |\n",
      "|    policy_loss        | -35.5     |\n",
      "|    reward             | 2.3712463 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 1.23e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 63700      |\n",
      "|    time_elapsed       | 1526       |\n",
      "|    total_timesteps    | 318500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.7      |\n",
      "|    explained_variance | 0.185      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 63699      |\n",
      "|    policy_loss        | -2.31e+03  |\n",
      "|    reward             | -127.88697 |\n",
      "|    std                | 1.2        |\n",
      "|    value_loss         | 3.57e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 63800      |\n",
      "|    time_elapsed       | 1528       |\n",
      "|    total_timesteps    | 319000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.8      |\n",
      "|    explained_variance | 0.041      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 63799      |\n",
      "|    policy_loss        | -3.08e+03  |\n",
      "|    reward             | -21.709589 |\n",
      "|    std                | 1.2        |\n",
      "|    value_loss         | 7.16e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 63900     |\n",
      "|    time_elapsed       | 1530      |\n",
      "|    total_timesteps    | 319500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.8     |\n",
      "|    explained_variance | 0.192     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 63899     |\n",
      "|    policy_loss        | -352      |\n",
      "|    reward             | 131.02475 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 1.04e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 64000     |\n",
      "|    time_elapsed       | 1533      |\n",
      "|    total_timesteps    | 320000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.8     |\n",
      "|    explained_variance | 0.344     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 63999     |\n",
      "|    policy_loss        | -279      |\n",
      "|    reward             | 12.520004 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 204       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 64100      |\n",
      "|    time_elapsed       | 1540       |\n",
      "|    total_timesteps    | 320500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.8      |\n",
      "|    explained_variance | -0.132     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 64099      |\n",
      "|    policy_loss        | 667        |\n",
      "|    reward             | -10.993652 |\n",
      "|    std                | 1.2        |\n",
      "|    value_loss         | 528        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 64200     |\n",
      "|    time_elapsed       | 1543      |\n",
      "|    total_timesteps    | 321000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.8     |\n",
      "|    explained_variance | -0.123    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 64199     |\n",
      "|    policy_loss        | 165       |\n",
      "|    reward             | -69.94043 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 1.08e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 64300     |\n",
      "|    time_elapsed       | 1545      |\n",
      "|    total_timesteps    | 321500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.8     |\n",
      "|    explained_variance | 0.182     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 64299     |\n",
      "|    policy_loss        | 180       |\n",
      "|    reward             | -40.65609 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 1.35e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 64400     |\n",
      "|    time_elapsed       | 1547      |\n",
      "|    total_timesteps    | 322000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.9     |\n",
      "|    explained_variance | -0.0931   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 64399     |\n",
      "|    policy_loss        | 162       |\n",
      "|    reward             | -79.75981 |\n",
      "|    std                | 1.21      |\n",
      "|    value_loss         | 1.54e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 64500      |\n",
      "|    time_elapsed       | 1550       |\n",
      "|    total_timesteps    | 322500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.8      |\n",
      "|    explained_variance | -0.144     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 64499      |\n",
      "|    policy_loss        | -1.97e+03  |\n",
      "|    reward             | -98.994705 |\n",
      "|    std                | 1.21       |\n",
      "|    value_loss         | 5.41e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 64600     |\n",
      "|    time_elapsed       | 1552      |\n",
      "|    total_timesteps    | 323000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.8     |\n",
      "|    explained_variance | 0.0831    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 64599     |\n",
      "|    policy_loss        | 3.03e+03  |\n",
      "|    reward             | -89.86316 |\n",
      "|    std                | 1.21      |\n",
      "|    value_loss         | 5.64e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 64700      |\n",
      "|    time_elapsed       | 1554       |\n",
      "|    total_timesteps    | 323500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.8      |\n",
      "|    explained_variance | -0.142     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 64699      |\n",
      "|    policy_loss        | 1.05e+03   |\n",
      "|    reward             | -4.2208843 |\n",
      "|    std                | 1.2        |\n",
      "|    value_loss         | 1.21e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 64800     |\n",
      "|    time_elapsed       | 1557      |\n",
      "|    total_timesteps    | 324000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.7     |\n",
      "|    explained_variance | -0.25     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 64799     |\n",
      "|    policy_loss        | -702      |\n",
      "|    reward             | -76.09318 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 510       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 64900      |\n",
      "|    time_elapsed       | 1559       |\n",
      "|    total_timesteps    | 324500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.8      |\n",
      "|    explained_variance | 0.178      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 64899      |\n",
      "|    policy_loss        | -4.49e+03  |\n",
      "|    reward             | -10.653933 |\n",
      "|    std                | 1.2        |\n",
      "|    value_loss         | 1.18e+04   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 65000    |\n",
      "|    time_elapsed       | 1561     |\n",
      "|    total_timesteps    | 325000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -47.8    |\n",
      "|    explained_variance | -0.477   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64999    |\n",
      "|    policy_loss        | 2.92e+03 |\n",
      "|    reward             | 66.39485 |\n",
      "|    std                | 1.21     |\n",
      "|    value_loss         | 5.32e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 65100     |\n",
      "|    time_elapsed       | 1563      |\n",
      "|    total_timesteps    | 325500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.8     |\n",
      "|    explained_variance | 0.122     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 65099     |\n",
      "|    policy_loss        | 6.77e+03  |\n",
      "|    reward             | 63.202084 |\n",
      "|    std                | 1.21      |\n",
      "|    value_loss         | 2.37e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 65200     |\n",
      "|    time_elapsed       | 1566      |\n",
      "|    total_timesteps    | 326000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.8     |\n",
      "|    explained_variance | -0.0153   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 65199     |\n",
      "|    policy_loss        | -1.38e+03 |\n",
      "|    reward             | 30.712729 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 1.23e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 65300     |\n",
      "|    time_elapsed       | 1568      |\n",
      "|    total_timesteps    | 326500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.7     |\n",
      "|    explained_variance | 0.0118    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 65299     |\n",
      "|    policy_loss        | -88       |\n",
      "|    reward             | 27.829916 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 701       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 65400     |\n",
      "|    time_elapsed       | 1570      |\n",
      "|    total_timesteps    | 327000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.7     |\n",
      "|    explained_variance | 0.0838    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 65399     |\n",
      "|    policy_loss        | 6.94e+03  |\n",
      "|    reward             | 22.660816 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 2.19e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 65500     |\n",
      "|    time_elapsed       | 1573      |\n",
      "|    total_timesteps    | 327500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.7     |\n",
      "|    explained_variance | -0.0513   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 65499     |\n",
      "|    policy_loss        | -4.13e+03 |\n",
      "|    reward             | -92.52347 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 1.22e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 65600     |\n",
      "|    time_elapsed       | 1575      |\n",
      "|    total_timesteps    | 328000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.8     |\n",
      "|    explained_variance | 0.157     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 65599     |\n",
      "|    policy_loss        | 4.62e+03  |\n",
      "|    reward             | 47.776955 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 1.27e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 65700     |\n",
      "|    time_elapsed       | 1577      |\n",
      "|    total_timesteps    | 328500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.7     |\n",
      "|    explained_variance | 0.0571    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 65699     |\n",
      "|    policy_loss        | -7.14e+03 |\n",
      "|    reward             | 285.50565 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 3.67e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 65800     |\n",
      "|    time_elapsed       | 1580      |\n",
      "|    total_timesteps    | 329000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.8     |\n",
      "|    explained_variance | 0.307     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 65799     |\n",
      "|    policy_loss        | 2.85e+03  |\n",
      "|    reward             | 3.2998774 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 4e+03     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 65900     |\n",
      "|    time_elapsed       | 1582      |\n",
      "|    total_timesteps    | 329500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.8     |\n",
      "|    explained_variance | 0.32      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 65899     |\n",
      "|    policy_loss        | 2.17e+03  |\n",
      "|    reward             | -75.65303 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 2.29e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 66000      |\n",
      "|    time_elapsed       | 1584       |\n",
      "|    total_timesteps    | 330000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.7      |\n",
      "|    explained_variance | -0.969     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 65999      |\n",
      "|    policy_loss        | 1.65e+03   |\n",
      "|    reward             | -38.822495 |\n",
      "|    std                | 1.2        |\n",
      "|    value_loss         | 1.36e+03   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 66100    |\n",
      "|    time_elapsed       | 1587     |\n",
      "|    total_timesteps    | 330500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -47.7    |\n",
      "|    explained_variance | -0.268   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66099    |\n",
      "|    policy_loss        | 299      |\n",
      "|    reward             | 88.46175 |\n",
      "|    std                | 1.2      |\n",
      "|    value_loss         | 198      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 66200     |\n",
      "|    time_elapsed       | 1589      |\n",
      "|    total_timesteps    | 331000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.7     |\n",
      "|    explained_variance | 0.0274    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 66199     |\n",
      "|    policy_loss        | -3.93e+03 |\n",
      "|    reward             | -20.07522 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 1.28e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 66300     |\n",
      "|    time_elapsed       | 1591      |\n",
      "|    total_timesteps    | 331500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.7     |\n",
      "|    explained_variance | -0.0214   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 66299     |\n",
      "|    policy_loss        | -2.07e+03 |\n",
      "|    reward             | 182.87343 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 1.41e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 66400     |\n",
      "|    time_elapsed       | 1594      |\n",
      "|    total_timesteps    | 332000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.8     |\n",
      "|    explained_variance | 0.035     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 66399     |\n",
      "|    policy_loss        | 8.69e+03  |\n",
      "|    reward             | 188.11974 |\n",
      "|    std                | 1.21      |\n",
      "|    value_loss         | 4.26e+04  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 66500    |\n",
      "|    time_elapsed       | 1596     |\n",
      "|    total_timesteps    | 332500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -47.7    |\n",
      "|    explained_variance | -0.331   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66499    |\n",
      "|    policy_loss        | -1.8e+03 |\n",
      "|    reward             | 7.735847 |\n",
      "|    std                | 1.2      |\n",
      "|    value_loss         | 2.59e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 66600     |\n",
      "|    time_elapsed       | 1598      |\n",
      "|    total_timesteps    | 333000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.8     |\n",
      "|    explained_variance | -0.094    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 66599     |\n",
      "|    policy_loss        | 2.41e+03  |\n",
      "|    reward             | 0.7473925 |\n",
      "|    std                | 1.21      |\n",
      "|    value_loss         | 3.86e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 66700     |\n",
      "|    time_elapsed       | 1601      |\n",
      "|    total_timesteps    | 333500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.7     |\n",
      "|    explained_variance | 0.131     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 66699     |\n",
      "|    policy_loss        | -127      |\n",
      "|    reward             | 29.046112 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 1.12e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 66800    |\n",
      "|    time_elapsed       | 1603     |\n",
      "|    total_timesteps    | 334000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -47.7    |\n",
      "|    explained_variance | 0.179    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66799    |\n",
      "|    policy_loss        | 250      |\n",
      "|    reward             | 34.53513 |\n",
      "|    std                | 1.2      |\n",
      "|    value_loss         | 2.13e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 66900     |\n",
      "|    time_elapsed       | 1605      |\n",
      "|    total_timesteps    | 334500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.7     |\n",
      "|    explained_variance | -0.0677   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 66899     |\n",
      "|    policy_loss        | -5.12     |\n",
      "|    reward             | -71.10043 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 1.14e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 67000     |\n",
      "|    time_elapsed       | 1608      |\n",
      "|    total_timesteps    | 335000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.7     |\n",
      "|    explained_variance | -0.107    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 66999     |\n",
      "|    policy_loss        | 1.47e+03  |\n",
      "|    reward             | 95.670845 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 1.47e+04  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 67100    |\n",
      "|    time_elapsed       | 1610     |\n",
      "|    total_timesteps    | 335500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -47.6    |\n",
      "|    explained_variance | 0.0584   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67099    |\n",
      "|    policy_loss        | 2.33e+03 |\n",
      "|    reward             | 66.30996 |\n",
      "|    std                | 1.2      |\n",
      "|    value_loss         | 3.51e+03 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 67200      |\n",
      "|    time_elapsed       | 1612       |\n",
      "|    total_timesteps    | 336000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.6      |\n",
      "|    explained_variance | 0.213      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 67199      |\n",
      "|    policy_loss        | -2.39e+03  |\n",
      "|    reward             | -22.718456 |\n",
      "|    std                | 1.2        |\n",
      "|    value_loss         | 3.81e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 67300      |\n",
      "|    time_elapsed       | 1615       |\n",
      "|    total_timesteps    | 336500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.5      |\n",
      "|    explained_variance | 0.0782     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 67299      |\n",
      "|    policy_loss        | 1.85e+03   |\n",
      "|    reward             | -1.8405099 |\n",
      "|    std                | 1.2        |\n",
      "|    value_loss         | 2.65e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 67400      |\n",
      "|    time_elapsed       | 1617       |\n",
      "|    total_timesteps    | 337000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.5      |\n",
      "|    explained_variance | 0.0611     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 67399      |\n",
      "|    policy_loss        | 91.2       |\n",
      "|    reward             | -38.640133 |\n",
      "|    std                | 1.2        |\n",
      "|    value_loss         | 7.62e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 67500      |\n",
      "|    time_elapsed       | 1619       |\n",
      "|    total_timesteps    | 337500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.6      |\n",
      "|    explained_variance | 0.0731     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 67499      |\n",
      "|    policy_loss        | -3.42e+03  |\n",
      "|    reward             | -157.75725 |\n",
      "|    std                | 1.2        |\n",
      "|    value_loss         | 1.09e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 67600     |\n",
      "|    time_elapsed       | 1622      |\n",
      "|    total_timesteps    | 338000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.6     |\n",
      "|    explained_variance | -0.0165   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 67599     |\n",
      "|    policy_loss        | -5.41e+03 |\n",
      "|    reward             | -63.48125 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 1.99e+04  |\n",
      "-------------------------------------\n",
      "day: 3102, episode: 110\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4534597.17\n",
      "total_reward: 3534597.17\n",
      "total_cost: 27850.41\n",
      "total_trades: 44942\n",
      "Sharpe: 2.966\n",
      "=================================\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 67700    |\n",
      "|    time_elapsed       | 1624     |\n",
      "|    total_timesteps    | 338500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -47.6    |\n",
      "|    explained_variance | -0.022   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67699    |\n",
      "|    policy_loss        | 245      |\n",
      "|    reward             | 38.31468 |\n",
      "|    std                | 1.2      |\n",
      "|    value_loss         | 530      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 67800    |\n",
      "|    time_elapsed       | 1626     |\n",
      "|    total_timesteps    | 339000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -47.7    |\n",
      "|    explained_variance | -0.378   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67799    |\n",
      "|    policy_loss        | 2.09e+03 |\n",
      "|    reward             | 92.22477 |\n",
      "|    std                | 1.2      |\n",
      "|    value_loss         | 2.62e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 67900     |\n",
      "|    time_elapsed       | 1629      |\n",
      "|    total_timesteps    | 339500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.7     |\n",
      "|    explained_variance | 0.245     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 67899     |\n",
      "|    policy_loss        | 803       |\n",
      "|    reward             | -41.61492 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 1.08e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 68000      |\n",
      "|    time_elapsed       | 1631       |\n",
      "|    total_timesteps    | 340000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.7      |\n",
      "|    explained_variance | -0.078     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 67999      |\n",
      "|    policy_loss        | -2.47e+03  |\n",
      "|    reward             | -61.368385 |\n",
      "|    std                | 1.2        |\n",
      "|    value_loss         | 3.09e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 68100      |\n",
      "|    time_elapsed       | 1633       |\n",
      "|    total_timesteps    | 340500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.7      |\n",
      "|    explained_variance | 0.13       |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 68099      |\n",
      "|    policy_loss        | -6.23e+03  |\n",
      "|    reward             | -138.82413 |\n",
      "|    std                | 1.2        |\n",
      "|    value_loss         | 1.95e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 68200     |\n",
      "|    time_elapsed       | 1636      |\n",
      "|    total_timesteps    | 341000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.8     |\n",
      "|    explained_variance | 0.00771   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 68199     |\n",
      "|    policy_loss        | -5.28e+03 |\n",
      "|    reward             | 175.89429 |\n",
      "|    std                | 1.21      |\n",
      "|    value_loss         | 4.86e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 68300      |\n",
      "|    time_elapsed       | 1638       |\n",
      "|    total_timesteps    | 341500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.8      |\n",
      "|    explained_variance | -0.0481    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 68299      |\n",
      "|    policy_loss        | 2.58e+03   |\n",
      "|    reward             | -17.834597 |\n",
      "|    std                | 1.21       |\n",
      "|    value_loss         | 3.31e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 68400     |\n",
      "|    time_elapsed       | 1640      |\n",
      "|    total_timesteps    | 342000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.9     |\n",
      "|    explained_variance | 0.262     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 68399     |\n",
      "|    policy_loss        | 1.72e+03  |\n",
      "|    reward             | 14.827231 |\n",
      "|    std                | 1.21      |\n",
      "|    value_loss         | 3.53e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 68500     |\n",
      "|    time_elapsed       | 1643      |\n",
      "|    total_timesteps    | 342500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.8     |\n",
      "|    explained_variance | 0.126     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 68499     |\n",
      "|    policy_loss        | 2.38e+03  |\n",
      "|    reward             | 2.5095074 |\n",
      "|    std                | 1.21      |\n",
      "|    value_loss         | 5.54e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 68600     |\n",
      "|    time_elapsed       | 1645      |\n",
      "|    total_timesteps    | 343000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.8     |\n",
      "|    explained_variance | -0.0784   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 68599     |\n",
      "|    policy_loss        | 1.05e+03  |\n",
      "|    reward             | 17.750895 |\n",
      "|    std                | 1.21      |\n",
      "|    value_loss         | 2.1e+03   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 68700     |\n",
      "|    time_elapsed       | 1647      |\n",
      "|    total_timesteps    | 343500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.8     |\n",
      "|    explained_variance | 0.0444    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 68699     |\n",
      "|    policy_loss        | 4.87e+03  |\n",
      "|    reward             | 137.35068 |\n",
      "|    std                | 1.21      |\n",
      "|    value_loss         | 1.59e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 68800     |\n",
      "|    time_elapsed       | 1650      |\n",
      "|    total_timesteps    | 344000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.7     |\n",
      "|    explained_variance | 0.048     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 68799     |\n",
      "|    policy_loss        | -1.89e+03 |\n",
      "|    reward             | -201.5101 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 1.57e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 68900     |\n",
      "|    time_elapsed       | 1652      |\n",
      "|    total_timesteps    | 344500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.8     |\n",
      "|    explained_variance | 0.2       |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 68899     |\n",
      "|    policy_loss        | -1.26e+03 |\n",
      "|    reward             | -98.66304 |\n",
      "|    std                | 1.21      |\n",
      "|    value_loss         | 1.8e+03   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 69000     |\n",
      "|    time_elapsed       | 1654      |\n",
      "|    total_timesteps    | 345000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.8     |\n",
      "|    explained_variance | 0.153     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 68999     |\n",
      "|    policy_loss        | -4.69e+03 |\n",
      "|    reward             | -49.83254 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 1.08e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 69100      |\n",
      "|    time_elapsed       | 1657       |\n",
      "|    total_timesteps    | 345500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.7      |\n",
      "|    explained_variance | -0.185     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 69099      |\n",
      "|    policy_loss        | -3.79e+03  |\n",
      "|    reward             | -66.144516 |\n",
      "|    std                | 1.2        |\n",
      "|    value_loss         | 6.05e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 69200     |\n",
      "|    time_elapsed       | 1659      |\n",
      "|    total_timesteps    | 346000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.7     |\n",
      "|    explained_variance | 0.176     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 69199     |\n",
      "|    policy_loss        | 2.5e+03   |\n",
      "|    reward             | 29.877079 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 4.07e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 69300      |\n",
      "|    time_elapsed       | 1661       |\n",
      "|    total_timesteps    | 346500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.7      |\n",
      "|    explained_variance | 0.252      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 69299      |\n",
      "|    policy_loss        | -2.49e+03  |\n",
      "|    reward             | 122.125305 |\n",
      "|    std                | 1.2        |\n",
      "|    value_loss         | 6.49e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 69400     |\n",
      "|    time_elapsed       | 1664      |\n",
      "|    total_timesteps    | 347000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.8     |\n",
      "|    explained_variance | -0.0467   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 69399     |\n",
      "|    policy_loss        | 1.1e+04   |\n",
      "|    reward             | 310.98956 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 6.36e+04  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 69500    |\n",
      "|    time_elapsed       | 1666     |\n",
      "|    total_timesteps    | 347500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -47.8    |\n",
      "|    explained_variance | 0.0729   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69499    |\n",
      "|    policy_loss        | 823      |\n",
      "|    reward             | 142.4272 |\n",
      "|    std                | 1.21     |\n",
      "|    value_loss         | 3.51e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 69600     |\n",
      "|    time_elapsed       | 1668      |\n",
      "|    total_timesteps    | 348000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.8     |\n",
      "|    explained_variance | 0.229     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 69599     |\n",
      "|    policy_loss        | -1.93e+03 |\n",
      "|    reward             | -50.54429 |\n",
      "|    std                | 1.21      |\n",
      "|    value_loss         | 2.79e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 69700      |\n",
      "|    time_elapsed       | 1671       |\n",
      "|    total_timesteps    | 348500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.8      |\n",
      "|    explained_variance | 0.174      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 69699      |\n",
      "|    policy_loss        | 1.81e+03   |\n",
      "|    reward             | -18.305428 |\n",
      "|    std                | 1.2        |\n",
      "|    value_loss         | 1.67e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 69800      |\n",
      "|    time_elapsed       | 1673       |\n",
      "|    total_timesteps    | 349000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.8      |\n",
      "|    explained_variance | -0.211     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 69799      |\n",
      "|    policy_loss        | 1.76e+03   |\n",
      "|    reward             | -49.726616 |\n",
      "|    std                | 1.21       |\n",
      "|    value_loss         | 3.06e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 69900      |\n",
      "|    time_elapsed       | 1675       |\n",
      "|    total_timesteps    | 349500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.8      |\n",
      "|    explained_variance | -0.049     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 69899      |\n",
      "|    policy_loss        | 152        |\n",
      "|    reward             | -278.76715 |\n",
      "|    std                | 1.21       |\n",
      "|    value_loss         | 5.81e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 70000      |\n",
      "|    time_elapsed       | 1678       |\n",
      "|    total_timesteps    | 350000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.8      |\n",
      "|    explained_variance | -0.022     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 69999      |\n",
      "|    policy_loss        | -5.07e+03  |\n",
      "|    reward             | 108.400566 |\n",
      "|    std                | 1.21       |\n",
      "|    value_loss         | 1.9e+04    |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 70100    |\n",
      "|    time_elapsed       | 1680     |\n",
      "|    total_timesteps    | 350500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -47.8    |\n",
      "|    explained_variance | -0.031   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70099    |\n",
      "|    policy_loss        | -520     |\n",
      "|    reward             | 45.26397 |\n",
      "|    std                | 1.21     |\n",
      "|    value_loss         | 1.38e+04 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 70200      |\n",
      "|    time_elapsed       | 1682       |\n",
      "|    total_timesteps    | 351000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.8      |\n",
      "|    explained_variance | 0.179      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 70199      |\n",
      "|    policy_loss        | 1.45e+03   |\n",
      "|    reward             | -20.117579 |\n",
      "|    std                | 1.21       |\n",
      "|    value_loss         | 1.98e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 70300      |\n",
      "|    time_elapsed       | 1685       |\n",
      "|    total_timesteps    | 351500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.8      |\n",
      "|    explained_variance | -0.0704    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 70299      |\n",
      "|    policy_loss        | 1.67e+03   |\n",
      "|    reward             | 115.838264 |\n",
      "|    std                | 1.21       |\n",
      "|    value_loss         | 2.55e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 70400      |\n",
      "|    time_elapsed       | 1687       |\n",
      "|    total_timesteps    | 352000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.8      |\n",
      "|    explained_variance | 0.0879     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 70399      |\n",
      "|    policy_loss        | 1.6e+03    |\n",
      "|    reward             | -44.112965 |\n",
      "|    std                | 1.21       |\n",
      "|    value_loss         | 1.39e+03   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 70500    |\n",
      "|    time_elapsed       | 1689     |\n",
      "|    total_timesteps    | 352500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -47.8    |\n",
      "|    explained_variance | -0.192   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70499    |\n",
      "|    policy_loss        | -950     |\n",
      "|    reward             | 68.81177 |\n",
      "|    std                | 1.21     |\n",
      "|    value_loss         | 5.5e+03  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 70600     |\n",
      "|    time_elapsed       | 1692      |\n",
      "|    total_timesteps    | 353000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.9     |\n",
      "|    explained_variance | 0.0372    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 70599     |\n",
      "|    policy_loss        | 1.81e+03  |\n",
      "|    reward             | 23.501984 |\n",
      "|    std                | 1.21      |\n",
      "|    value_loss         | 1.05e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 70700     |\n",
      "|    time_elapsed       | 1694      |\n",
      "|    total_timesteps    | 353500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.8     |\n",
      "|    explained_variance | 0.0208    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 70699     |\n",
      "|    policy_loss        | 3.46e+03  |\n",
      "|    reward             | 97.194595 |\n",
      "|    std                | 1.21      |\n",
      "|    value_loss         | 5.55e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 70800      |\n",
      "|    time_elapsed       | 1697       |\n",
      "|    total_timesteps    | 354000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.9      |\n",
      "|    explained_variance | -0.154     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 70799      |\n",
      "|    policy_loss        | 4.63e+03   |\n",
      "|    reward             | -17.750055 |\n",
      "|    std                | 1.21       |\n",
      "|    value_loss         | 8.52e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 70900     |\n",
      "|    time_elapsed       | 1699      |\n",
      "|    total_timesteps    | 354500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.9     |\n",
      "|    explained_variance | -0.133    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 70899     |\n",
      "|    policy_loss        | -1.91e+03 |\n",
      "|    reward             | 68.33058  |\n",
      "|    std                | 1.21      |\n",
      "|    value_loss         | 3.53e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 71000     |\n",
      "|    time_elapsed       | 1701      |\n",
      "|    total_timesteps    | 355000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.9     |\n",
      "|    explained_variance | 0.242     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 70999     |\n",
      "|    policy_loss        | 351       |\n",
      "|    reward             | 15.994496 |\n",
      "|    std                | 1.21      |\n",
      "|    value_loss         | 4.61e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 71100      |\n",
      "|    time_elapsed       | 1704       |\n",
      "|    total_timesteps    | 355500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48        |\n",
      "|    explained_variance | 0.1        |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 71099      |\n",
      "|    policy_loss        | -2.59e+03  |\n",
      "|    reward             | -58.274315 |\n",
      "|    std                | 1.22       |\n",
      "|    value_loss         | 1.45e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 71200     |\n",
      "|    time_elapsed       | 1706      |\n",
      "|    total_timesteps    | 356000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.9     |\n",
      "|    explained_variance | 0.0741    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 71199     |\n",
      "|    policy_loss        | 580       |\n",
      "|    reward             | 41.172054 |\n",
      "|    std                | 1.21      |\n",
      "|    value_loss         | 1.67e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 71300      |\n",
      "|    time_elapsed       | 1708       |\n",
      "|    total_timesteps    | 356500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.9      |\n",
      "|    explained_variance | 0.0913     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 71299      |\n",
      "|    policy_loss        | -7.22e+03  |\n",
      "|    reward             | -18.571709 |\n",
      "|    std                | 1.21       |\n",
      "|    value_loss         | 4.03e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 71400      |\n",
      "|    time_elapsed       | 1711       |\n",
      "|    total_timesteps    | 357000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48        |\n",
      "|    explained_variance | -0.449     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 71399      |\n",
      "|    policy_loss        | 5.34e+03   |\n",
      "|    reward             | -209.72728 |\n",
      "|    std                | 1.21       |\n",
      "|    value_loss         | 1.21e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 71500      |\n",
      "|    time_elapsed       | 1713       |\n",
      "|    total_timesteps    | 357500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48        |\n",
      "|    explained_variance | 0.181      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 71499      |\n",
      "|    policy_loss        | 524        |\n",
      "|    reward             | -25.385157 |\n",
      "|    std                | 1.21       |\n",
      "|    value_loss         | 445        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 71600      |\n",
      "|    time_elapsed       | 1715       |\n",
      "|    total_timesteps    | 358000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48        |\n",
      "|    explained_variance | -0.343     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 71599      |\n",
      "|    policy_loss        | -1.73e+03  |\n",
      "|    reward             | -1.5156943 |\n",
      "|    std                | 1.21       |\n",
      "|    value_loss         | 2.08e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 71700      |\n",
      "|    time_elapsed       | 1718       |\n",
      "|    total_timesteps    | 358500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48        |\n",
      "|    explained_variance | 0.131      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 71699      |\n",
      "|    policy_loss        | -4.04e+03  |\n",
      "|    reward             | -19.396444 |\n",
      "|    std                | 1.22       |\n",
      "|    value_loss         | 1.02e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 71800      |\n",
      "|    time_elapsed       | 1720       |\n",
      "|    total_timesteps    | 359000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48        |\n",
      "|    explained_variance | 0.146      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 71799      |\n",
      "|    policy_loss        | 5.66e+03   |\n",
      "|    reward             | -366.11667 |\n",
      "|    std                | 1.22       |\n",
      "|    value_loss         | 1.95e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 71900      |\n",
      "|    time_elapsed       | 1723       |\n",
      "|    total_timesteps    | 359500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48.1      |\n",
      "|    explained_variance | -0.013     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 71899      |\n",
      "|    policy_loss        | 5.43e+03   |\n",
      "|    reward             | -117.24765 |\n",
      "|    std                | 1.22       |\n",
      "|    value_loss         | 1.39e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 72000     |\n",
      "|    time_elapsed       | 1725      |\n",
      "|    total_timesteps    | 360000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.1     |\n",
      "|    explained_variance | -0.238    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 71999     |\n",
      "|    policy_loss        | 255       |\n",
      "|    reward             | -56.66474 |\n",
      "|    std                | 1.22      |\n",
      "|    value_loss         | 1.2e+03   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 72100     |\n",
      "|    time_elapsed       | 1727      |\n",
      "|    total_timesteps    | 360500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.1     |\n",
      "|    explained_variance | 0.361     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 72099     |\n",
      "|    policy_loss        | -3.1e+03  |\n",
      "|    reward             | 15.587166 |\n",
      "|    std                | 1.22      |\n",
      "|    value_loss         | 6.12e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 72200     |\n",
      "|    time_elapsed       | 1730      |\n",
      "|    total_timesteps    | 361000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.1     |\n",
      "|    explained_variance | 0.153     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 72199     |\n",
      "|    policy_loss        | 482       |\n",
      "|    reward             | -8.983312 |\n",
      "|    std                | 1.22      |\n",
      "|    value_loss         | 1.45e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 72300     |\n",
      "|    time_elapsed       | 1732      |\n",
      "|    total_timesteps    | 361500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.1     |\n",
      "|    explained_variance | 0.041     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 72299     |\n",
      "|    policy_loss        | 5.83e+03  |\n",
      "|    reward             | 0.8676702 |\n",
      "|    std                | 1.22      |\n",
      "|    value_loss         | 1.75e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 72400     |\n",
      "|    time_elapsed       | 1734      |\n",
      "|    total_timesteps    | 362000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.2     |\n",
      "|    explained_variance | 0.137     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 72399     |\n",
      "|    policy_loss        | 4.5e+03   |\n",
      "|    reward             | 137.34532 |\n",
      "|    std                | 1.22      |\n",
      "|    value_loss         | 1.07e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 72500     |\n",
      "|    time_elapsed       | 1737      |\n",
      "|    total_timesteps    | 362500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.2     |\n",
      "|    explained_variance | -0.153    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 72499     |\n",
      "|    policy_loss        | -6.68e+03 |\n",
      "|    reward             | 163.8767  |\n",
      "|    std                | 1.22      |\n",
      "|    value_loss         | 2.96e+04  |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 208         |\n",
      "|    iterations         | 72600       |\n",
      "|    time_elapsed       | 1739        |\n",
      "|    total_timesteps    | 363000      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -48.3       |\n",
      "|    explained_variance | 0.0608      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 72599       |\n",
      "|    policy_loss        | -783        |\n",
      "|    reward             | -105.157074 |\n",
      "|    std                | 1.23        |\n",
      "|    value_loss         | 9.22e+04    |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 72700     |\n",
      "|    time_elapsed       | 1742      |\n",
      "|    total_timesteps    | 363500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.3     |\n",
      "|    explained_variance | -0.0912   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 72699     |\n",
      "|    policy_loss        | -2.23e+03 |\n",
      "|    reward             | 86.46937  |\n",
      "|    std                | 1.23      |\n",
      "|    value_loss         | 6.75e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 72800      |\n",
      "|    time_elapsed       | 1744       |\n",
      "|    total_timesteps    | 364000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48.3      |\n",
      "|    explained_variance | 0.176      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 72799      |\n",
      "|    policy_loss        | -1.5e+03   |\n",
      "|    reward             | -14.600855 |\n",
      "|    std                | 1.23       |\n",
      "|    value_loss         | 2.28e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 72900     |\n",
      "|    time_elapsed       | 1746      |\n",
      "|    total_timesteps    | 364500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.4     |\n",
      "|    explained_variance | 0.122     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 72899     |\n",
      "|    policy_loss        | -5.66e+03 |\n",
      "|    reward             | 4.6206174 |\n",
      "|    std                | 1.23      |\n",
      "|    value_loss         | 1.71e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 73000     |\n",
      "|    time_elapsed       | 1748      |\n",
      "|    total_timesteps    | 365000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.4     |\n",
      "|    explained_variance | 0.0131    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 72999     |\n",
      "|    policy_loss        | 1.16e+04  |\n",
      "|    reward             | -326.4269 |\n",
      "|    std                | 1.23      |\n",
      "|    value_loss         | 8.55e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 73100      |\n",
      "|    time_elapsed       | 1751       |\n",
      "|    total_timesteps    | 365500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48.4      |\n",
      "|    explained_variance | -0.0141    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 73099      |\n",
      "|    policy_loss        | 1.7e+03    |\n",
      "|    reward             | -291.45004 |\n",
      "|    std                | 1.23       |\n",
      "|    value_loss         | 1.15e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 73200     |\n",
      "|    time_elapsed       | 1753      |\n",
      "|    total_timesteps    | 366000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.4     |\n",
      "|    explained_variance | 0.0123    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 73199     |\n",
      "|    policy_loss        | -1.42e+04 |\n",
      "|    reward             | -20.72443 |\n",
      "|    std                | 1.23      |\n",
      "|    value_loss         | 1.23e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 73300     |\n",
      "|    time_elapsed       | 1755      |\n",
      "|    total_timesteps    | 366500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.4     |\n",
      "|    explained_variance | -0.315    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 73299     |\n",
      "|    policy_loss        | 853       |\n",
      "|    reward             | 25.095606 |\n",
      "|    std                | 1.23      |\n",
      "|    value_loss         | 2.19e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 73400      |\n",
      "|    time_elapsed       | 1758       |\n",
      "|    total_timesteps    | 367000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48.4      |\n",
      "|    explained_variance | 0.157      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 73399      |\n",
      "|    policy_loss        | -1.27e+03  |\n",
      "|    reward             | -5.4417596 |\n",
      "|    std                | 1.23       |\n",
      "|    value_loss         | 4.04e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 73500      |\n",
      "|    time_elapsed       | 1760       |\n",
      "|    total_timesteps    | 367500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48.4      |\n",
      "|    explained_variance | 0.013      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 73499      |\n",
      "|    policy_loss        | -623       |\n",
      "|    reward             | -149.01276 |\n",
      "|    std                | 1.23       |\n",
      "|    value_loss         | 1.32e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 73600     |\n",
      "|    time_elapsed       | 1762      |\n",
      "|    total_timesteps    | 368000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.4     |\n",
      "|    explained_variance | 0.252     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 73599     |\n",
      "|    policy_loss        | 1.08e+04  |\n",
      "|    reward             | 198.93462 |\n",
      "|    std                | 1.23      |\n",
      "|    value_loss         | 4.64e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 73700     |\n",
      "|    time_elapsed       | 1765      |\n",
      "|    total_timesteps    | 368500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.4     |\n",
      "|    explained_variance | -0.052    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 73699     |\n",
      "|    policy_loss        | -2.52e+03 |\n",
      "|    reward             | -160.3461 |\n",
      "|    std                | 1.23      |\n",
      "|    value_loss         | 1.26e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 73800     |\n",
      "|    time_elapsed       | 1767      |\n",
      "|    total_timesteps    | 369000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.5     |\n",
      "|    explained_variance | 0.0082    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 73799     |\n",
      "|    policy_loss        | -5.33e+03 |\n",
      "|    reward             | 148.81168 |\n",
      "|    std                | 1.23      |\n",
      "|    value_loss         | 7.14e+04  |\n",
      "-------------------------------------\n",
      "day: 3102, episode: 120\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 8985842.22\n",
      "total_reward: 7985842.22\n",
      "total_cost: 45115.56\n",
      "total_trades: 43939\n",
      "Sharpe: 3.304\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 73900     |\n",
      "|    time_elapsed       | 1769      |\n",
      "|    total_timesteps    | 369500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.5     |\n",
      "|    explained_variance | 0.253     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 73899     |\n",
      "|    policy_loss        | -2.37e+03 |\n",
      "|    reward             | 3.5771978 |\n",
      "|    std                | 1.24      |\n",
      "|    value_loss         | 2.88e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 74000     |\n",
      "|    time_elapsed       | 1772      |\n",
      "|    total_timesteps    | 370000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.6     |\n",
      "|    explained_variance | -0.199    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 73999     |\n",
      "|    policy_loss        | -959      |\n",
      "|    reward             | 47.250183 |\n",
      "|    std                | 1.24      |\n",
      "|    value_loss         | 5.69e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 74100      |\n",
      "|    time_elapsed       | 1774       |\n",
      "|    total_timesteps    | 370500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48.6      |\n",
      "|    explained_variance | 0.0375     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 74099      |\n",
      "|    policy_loss        | -1.37e+03  |\n",
      "|    reward             | -25.826067 |\n",
      "|    std                | 1.24       |\n",
      "|    value_loss         | 8.98e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 74200     |\n",
      "|    time_elapsed       | 1776      |\n",
      "|    total_timesteps    | 371000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.6     |\n",
      "|    explained_variance | 0.0677    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 74199     |\n",
      "|    policy_loss        | 5.23e+03  |\n",
      "|    reward             | 106.98192 |\n",
      "|    std                | 1.24      |\n",
      "|    value_loss         | 1.58e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 74300     |\n",
      "|    time_elapsed       | 1779      |\n",
      "|    total_timesteps    | 371500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.6     |\n",
      "|    explained_variance | 0.0237    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 74299     |\n",
      "|    policy_loss        | -1.43e+04 |\n",
      "|    reward             | 107.34946 |\n",
      "|    std                | 1.24      |\n",
      "|    value_loss         | 1.21e+05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 74400    |\n",
      "|    time_elapsed       | 1781     |\n",
      "|    total_timesteps    | 372000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -48.6    |\n",
      "|    explained_variance | 0.00588  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74399    |\n",
      "|    policy_loss        | 2.74e+03 |\n",
      "|    reward             | 291.7972 |\n",
      "|    std                | 1.24     |\n",
      "|    value_loss         | 2.76e+04 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 74500      |\n",
      "|    time_elapsed       | 1783       |\n",
      "|    total_timesteps    | 372500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48.7      |\n",
      "|    explained_variance | 0.0164     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 74499      |\n",
      "|    policy_loss        | 1.92e+03   |\n",
      "|    reward             | -3.9093235 |\n",
      "|    std                | 1.24       |\n",
      "|    value_loss         | 1.89e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 74600     |\n",
      "|    time_elapsed       | 1786      |\n",
      "|    total_timesteps    | 373000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.7     |\n",
      "|    explained_variance | 0.0673    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 74599     |\n",
      "|    policy_loss        | 1.86e+03  |\n",
      "|    reward             | 127.40143 |\n",
      "|    std                | 1.24      |\n",
      "|    value_loss         | 3.76e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 74700    |\n",
      "|    time_elapsed       | 1788     |\n",
      "|    total_timesteps    | 373500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -48.7    |\n",
      "|    explained_variance | 0.0933   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74699    |\n",
      "|    policy_loss        | 556      |\n",
      "|    reward             | 211.9563 |\n",
      "|    std                | 1.24     |\n",
      "|    value_loss         | 3.77e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 74800     |\n",
      "|    time_elapsed       | 1790      |\n",
      "|    total_timesteps    | 374000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.7     |\n",
      "|    explained_variance | 0.0592    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 74799     |\n",
      "|    policy_loss        | -4.67e+03 |\n",
      "|    reward             | 174.73372 |\n",
      "|    std                | 1.25      |\n",
      "|    value_loss         | 2.57e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 74900     |\n",
      "|    time_elapsed       | 1793      |\n",
      "|    total_timesteps    | 374500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.8     |\n",
      "|    explained_variance | 0.102     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 74899     |\n",
      "|    policy_loss        | 5.85e+03  |\n",
      "|    reward             | 494.20624 |\n",
      "|    std                | 1.25      |\n",
      "|    value_loss         | 3.04e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 75000      |\n",
      "|    time_elapsed       | 1795       |\n",
      "|    total_timesteps    | 375000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48.8      |\n",
      "|    explained_variance | 0.0561     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 74999      |\n",
      "|    policy_loss        | 5.04e+03   |\n",
      "|    reward             | -410.91995 |\n",
      "|    std                | 1.25       |\n",
      "|    value_loss         | 4.86e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 75100     |\n",
      "|    time_elapsed       | 1797      |\n",
      "|    total_timesteps    | 375500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.8     |\n",
      "|    explained_variance | -0.00216  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 75099     |\n",
      "|    policy_loss        | -791      |\n",
      "|    reward             | 38.008495 |\n",
      "|    std                | 1.25      |\n",
      "|    value_loss         | 1.36e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 75200     |\n",
      "|    time_elapsed       | 1800      |\n",
      "|    total_timesteps    | 376000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.9     |\n",
      "|    explained_variance | 0.175     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 75199     |\n",
      "|    policy_loss        | -6.26e+03 |\n",
      "|    reward             | 18.804396 |\n",
      "|    std                | 1.25      |\n",
      "|    value_loss         | 1.97e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 75300      |\n",
      "|    time_elapsed       | 1802       |\n",
      "|    total_timesteps    | 376500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48.9      |\n",
      "|    explained_variance | 0.114      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 75299      |\n",
      "|    policy_loss        | 1.15e+04   |\n",
      "|    reward             | -182.11322 |\n",
      "|    std                | 1.25       |\n",
      "|    value_loss         | 5.77e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 75400      |\n",
      "|    time_elapsed       | 1804       |\n",
      "|    total_timesteps    | 377000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48.9      |\n",
      "|    explained_variance | 0.0799     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 75399      |\n",
      "|    policy_loss        | 1.54e+03   |\n",
      "|    reward             | -253.42632 |\n",
      "|    std                | 1.25       |\n",
      "|    value_loss         | 3.65e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 75500     |\n",
      "|    time_elapsed       | 1807      |\n",
      "|    total_timesteps    | 377500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.9     |\n",
      "|    explained_variance | -0.0143   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 75499     |\n",
      "|    policy_loss        | -1.45e+04 |\n",
      "|    reward             | 56.704742 |\n",
      "|    std                | 1.25      |\n",
      "|    value_loss         | 9.71e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 75600      |\n",
      "|    time_elapsed       | 1809       |\n",
      "|    total_timesteps    | 378000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48.9      |\n",
      "|    explained_variance | 0.0233     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 75599      |\n",
      "|    policy_loss        | -1.77e+03  |\n",
      "|    reward             | -135.41891 |\n",
      "|    std                | 1.25       |\n",
      "|    value_loss         | 1.42e+05   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 75700      |\n",
      "|    time_elapsed       | 1811       |\n",
      "|    total_timesteps    | 378500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48.9      |\n",
      "|    explained_variance | 0.0215     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 75699      |\n",
      "|    policy_loss        | -546       |\n",
      "|    reward             | -1527.3868 |\n",
      "|    std                | 1.25       |\n",
      "|    value_loss         | 8.48e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 75800     |\n",
      "|    time_elapsed       | 1814      |\n",
      "|    total_timesteps    | 379000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.9     |\n",
      "|    explained_variance | 0.12      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 75799     |\n",
      "|    policy_loss        | -535      |\n",
      "|    reward             | 203.15396 |\n",
      "|    std                | 1.25      |\n",
      "|    value_loss         | 5.03e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 75900      |\n",
      "|    time_elapsed       | 1816       |\n",
      "|    total_timesteps    | 379500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48.9      |\n",
      "|    explained_variance | -0.0255    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 75899      |\n",
      "|    policy_loss        | -5.85e+03  |\n",
      "|    reward             | -156.54108 |\n",
      "|    std                | 1.26       |\n",
      "|    value_loss         | 3.28e+04   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 76000    |\n",
      "|    time_elapsed       | 1818     |\n",
      "|    total_timesteps    | 380000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -49      |\n",
      "|    explained_variance | -0.0162  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75999    |\n",
      "|    policy_loss        | 2.17e+03 |\n",
      "|    reward             | 79.23062 |\n",
      "|    std                | 1.26     |\n",
      "|    value_loss         | 1.2e+04  |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 76100      |\n",
      "|    time_elapsed       | 1821       |\n",
      "|    total_timesteps    | 380500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49        |\n",
      "|    explained_variance | -0.0903    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 76099      |\n",
      "|    policy_loss        | -569       |\n",
      "|    reward             | -60.732193 |\n",
      "|    std                | 1.26       |\n",
      "|    value_loss         | 1.11e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 76200      |\n",
      "|    time_elapsed       | 1823       |\n",
      "|    total_timesteps    | 381000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.1      |\n",
      "|    explained_variance | 0.044      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 76199      |\n",
      "|    policy_loss        | -3.33e+03  |\n",
      "|    reward             | -211.31644 |\n",
      "|    std                | 1.26       |\n",
      "|    value_loss         | 6.66e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 76300      |\n",
      "|    time_elapsed       | 1825       |\n",
      "|    total_timesteps    | 381500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.1      |\n",
      "|    explained_variance | 0.00592    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 76299      |\n",
      "|    policy_loss        | 5.05e+04   |\n",
      "|    reward             | -283.43283 |\n",
      "|    std                | 1.26       |\n",
      "|    value_loss         | 1.32e+06   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 76400     |\n",
      "|    time_elapsed       | 1828      |\n",
      "|    total_timesteps    | 382000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.1     |\n",
      "|    explained_variance | -0.221    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 76399     |\n",
      "|    policy_loss        | -2.25e+03 |\n",
      "|    reward             | 28.621996 |\n",
      "|    std                | 1.26      |\n",
      "|    value_loss         | 8.27e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 76500     |\n",
      "|    time_elapsed       | 1830      |\n",
      "|    total_timesteps    | 382500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.1     |\n",
      "|    explained_variance | 0.0167    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 76499     |\n",
      "|    policy_loss        | -5.67e+03 |\n",
      "|    reward             | 18.580242 |\n",
      "|    std                | 1.26      |\n",
      "|    value_loss         | 2.42e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 76600      |\n",
      "|    time_elapsed       | 1832       |\n",
      "|    total_timesteps    | 383000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.1      |\n",
      "|    explained_variance | -0.0236    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 76599      |\n",
      "|    policy_loss        | 1.43e+04   |\n",
      "|    reward             | -1.0464832 |\n",
      "|    std                | 1.26       |\n",
      "|    value_loss         | 8.34e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 76700     |\n",
      "|    time_elapsed       | 1835      |\n",
      "|    total_timesteps    | 383500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.2     |\n",
      "|    explained_variance | 0.0233    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 76699     |\n",
      "|    policy_loss        | -6.9e+03  |\n",
      "|    reward             | -258.0636 |\n",
      "|    std                | 1.26      |\n",
      "|    value_loss         | 5.62e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 76800     |\n",
      "|    time_elapsed       | 1837      |\n",
      "|    total_timesteps    | 384000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.2     |\n",
      "|    explained_variance | -0.0411   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 76799     |\n",
      "|    policy_loss        | -1.15e+04 |\n",
      "|    reward             | -781.0253 |\n",
      "|    std                | 1.27      |\n",
      "|    value_loss         | 5.89e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 76900     |\n",
      "|    time_elapsed       | 1839      |\n",
      "|    total_timesteps    | 384500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.3     |\n",
      "|    explained_variance | 0.00994   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 76899     |\n",
      "|    policy_loss        | -1.64e+04 |\n",
      "|    reward             | 338.5835  |\n",
      "|    std                | 1.27      |\n",
      "|    value_loss         | 2.49e+05  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 77000      |\n",
      "|    time_elapsed       | 1842       |\n",
      "|    total_timesteps    | 385000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.3      |\n",
      "|    explained_variance | 0.0602     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 76999      |\n",
      "|    policy_loss        | -1.42e+03  |\n",
      "|    reward             | -63.345715 |\n",
      "|    std                | 1.27       |\n",
      "|    value_loss         | 3.97e+03   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 77100    |\n",
      "|    time_elapsed       | 1844     |\n",
      "|    total_timesteps    | 385500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -49.3    |\n",
      "|    explained_variance | 0.0171   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77099    |\n",
      "|    policy_loss        | 1.25e+03 |\n",
      "|    reward             | 282.6441 |\n",
      "|    std                | 1.27     |\n",
      "|    value_loss         | 1.29e+04 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 77200     |\n",
      "|    time_elapsed       | 1846      |\n",
      "|    total_timesteps    | 386000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.3     |\n",
      "|    explained_variance | 0.0773    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 77199     |\n",
      "|    policy_loss        | -3.24e+03 |\n",
      "|    reward             | 206.04504 |\n",
      "|    std                | 1.27      |\n",
      "|    value_loss         | 1.3e+04   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 77300     |\n",
      "|    time_elapsed       | 1849      |\n",
      "|    total_timesteps    | 386500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.4     |\n",
      "|    explained_variance | 0.174     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 77299     |\n",
      "|    policy_loss        | 1.56e+03  |\n",
      "|    reward             | 222.14995 |\n",
      "|    std                | 1.27      |\n",
      "|    value_loss         | 8.07e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 77400      |\n",
      "|    time_elapsed       | 1851       |\n",
      "|    total_timesteps    | 387000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.4      |\n",
      "|    explained_variance | 0.00493    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 77399      |\n",
      "|    policy_loss        | -4.89e+03  |\n",
      "|    reward             | -84.191345 |\n",
      "|    std                | 1.27       |\n",
      "|    value_loss         | 4.14e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 77500      |\n",
      "|    time_elapsed       | 1853       |\n",
      "|    total_timesteps    | 387500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.4      |\n",
      "|    explained_variance | -0.027     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 77499      |\n",
      "|    policy_loss        | -2.25e+04  |\n",
      "|    reward             | -340.36322 |\n",
      "|    std                | 1.28       |\n",
      "|    value_loss         | 4.4e+05    |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 209      |\n",
      "|    iterations         | 77600    |\n",
      "|    time_elapsed       | 1856     |\n",
      "|    total_timesteps    | 388000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -49.4    |\n",
      "|    explained_variance | 0.112    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77599    |\n",
      "|    policy_loss        | 871      |\n",
      "|    reward             | 52.7614  |\n",
      "|    std                | 1.27     |\n",
      "|    value_loss         | 1.45e+03 |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 209         |\n",
      "|    iterations         | 77700       |\n",
      "|    time_elapsed       | 1858        |\n",
      "|    total_timesteps    | 388500      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -49.4       |\n",
      "|    explained_variance | -0.132      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 77699       |\n",
      "|    policy_loss        | 6.13e+03    |\n",
      "|    reward             | -114.718346 |\n",
      "|    std                | 1.27        |\n",
      "|    value_loss         | 2.34e+04    |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 77800     |\n",
      "|    time_elapsed       | 1860      |\n",
      "|    total_timesteps    | 389000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.4     |\n",
      "|    explained_variance | -0.164    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 77799     |\n",
      "|    policy_loss        | 2.42e+03  |\n",
      "|    reward             | 56.237343 |\n",
      "|    std                | 1.27      |\n",
      "|    value_loss         | 2.83e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 77900     |\n",
      "|    time_elapsed       | 1863      |\n",
      "|    total_timesteps    | 389500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.4     |\n",
      "|    explained_variance | 0.0124    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 77899     |\n",
      "|    policy_loss        | 1.37e+03  |\n",
      "|    reward             | -148.5086 |\n",
      "|    std                | 1.27      |\n",
      "|    value_loss         | 1.3e+04   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 78000     |\n",
      "|    time_elapsed       | 1865      |\n",
      "|    total_timesteps    | 390000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.5     |\n",
      "|    explained_variance | 0.0338    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 77999     |\n",
      "|    policy_loss        | -6.53e+03 |\n",
      "|    reward             | 57.80858  |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 2.45e+04  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 209      |\n",
      "|    iterations         | 78100    |\n",
      "|    time_elapsed       | 1867     |\n",
      "|    total_timesteps    | 390500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -49.5    |\n",
      "|    explained_variance | 0.0453   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78099    |\n",
      "|    policy_loss        | 1.87e+04 |\n",
      "|    reward             | 32.48891 |\n",
      "|    std                | 1.27     |\n",
      "|    value_loss         | 1.44e+05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 78200     |\n",
      "|    time_elapsed       | 1870      |\n",
      "|    total_timesteps    | 391000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.4     |\n",
      "|    explained_variance | -1.28     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 78199     |\n",
      "|    policy_loss        | -1.05e+03 |\n",
      "|    reward             | -81.54167 |\n",
      "|    std                | 1.27      |\n",
      "|    value_loss         | 862       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 209      |\n",
      "|    iterations         | 78300    |\n",
      "|    time_elapsed       | 1872     |\n",
      "|    total_timesteps    | 391500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -49.5    |\n",
      "|    explained_variance | 0.106    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78299    |\n",
      "|    policy_loss        | 858      |\n",
      "|    reward             | 9.289493 |\n",
      "|    std                | 1.27     |\n",
      "|    value_loss         | 1.61e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 78400     |\n",
      "|    time_elapsed       | 1874      |\n",
      "|    total_timesteps    | 392000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.5     |\n",
      "|    explained_variance | -0.061    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 78399     |\n",
      "|    policy_loss        | -4.29e+03 |\n",
      "|    reward             | 68.05341  |\n",
      "|    std                | 1.27      |\n",
      "|    value_loss         | 1e+04     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 78500      |\n",
      "|    time_elapsed       | 1877       |\n",
      "|    total_timesteps    | 392500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.4      |\n",
      "|    explained_variance | 0.0447     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 78499      |\n",
      "|    policy_loss        | -1.81e+03  |\n",
      "|    reward             | -171.78532 |\n",
      "|    std                | 1.27       |\n",
      "|    value_loss         | 5.84e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 78600      |\n",
      "|    time_elapsed       | 1879       |\n",
      "|    total_timesteps    | 393000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.4      |\n",
      "|    explained_variance | 0.0716     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 78599      |\n",
      "|    policy_loss        | -9.48e+03  |\n",
      "|    reward             | -267.31927 |\n",
      "|    std                | 1.27       |\n",
      "|    value_loss         | 6.02e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 78700      |\n",
      "|    time_elapsed       | 1881       |\n",
      "|    total_timesteps    | 393500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.4      |\n",
      "|    explained_variance | 0.09       |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 78699      |\n",
      "|    policy_loss        | -3.29e+03  |\n",
      "|    reward             | -274.60614 |\n",
      "|    std                | 1.27       |\n",
      "|    value_loss         | 1.66e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 78800      |\n",
      "|    time_elapsed       | 1884       |\n",
      "|    total_timesteps    | 394000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.4      |\n",
      "|    explained_variance | -0.0266    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 78799      |\n",
      "|    policy_loss        | -1.09e+04  |\n",
      "|    reward             | -479.22205 |\n",
      "|    std                | 1.27       |\n",
      "|    value_loss         | 1.77e+05   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 78900     |\n",
      "|    time_elapsed       | 1886      |\n",
      "|    total_timesteps    | 394500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.4     |\n",
      "|    explained_variance | -0.0825   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 78899     |\n",
      "|    policy_loss        | 1.26e+03  |\n",
      "|    reward             | -73.00325 |\n",
      "|    std                | 1.27      |\n",
      "|    value_loss         | 3.17e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 79000      |\n",
      "|    time_elapsed       | 1888       |\n",
      "|    total_timesteps    | 395000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.4      |\n",
      "|    explained_variance | 0.0658     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 78999      |\n",
      "|    policy_loss        | -4.31e+03  |\n",
      "|    reward             | -36.617393 |\n",
      "|    std                | 1.27       |\n",
      "|    value_loss         | 1.18e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 79100     |\n",
      "|    time_elapsed       | 1891      |\n",
      "|    total_timesteps    | 395500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.4     |\n",
      "|    explained_variance | 0.0857    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 79099     |\n",
      "|    policy_loss        | 1.54e+04  |\n",
      "|    reward             | 70.445045 |\n",
      "|    std                | 1.27      |\n",
      "|    value_loss         | 1.18e+05  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 79200      |\n",
      "|    time_elapsed       | 1893       |\n",
      "|    total_timesteps    | 396000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.4      |\n",
      "|    explained_variance | 0.169      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 79199      |\n",
      "|    policy_loss        | -7.64e+03  |\n",
      "|    reward             | -149.08241 |\n",
      "|    std                | 1.27       |\n",
      "|    value_loss         | 4.17e+04   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 209      |\n",
      "|    iterations         | 79300    |\n",
      "|    time_elapsed       | 1895     |\n",
      "|    total_timesteps    | 396500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -49.4    |\n",
      "|    explained_variance | -0.0911  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79299    |\n",
      "|    policy_loss        | 1.04e+04 |\n",
      "|    reward             | 62.53249 |\n",
      "|    std                | 1.27     |\n",
      "|    value_loss         | 7.82e+04 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 79400     |\n",
      "|    time_elapsed       | 1898      |\n",
      "|    total_timesteps    | 397000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.4     |\n",
      "|    explained_variance | -0.0312   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 79399     |\n",
      "|    policy_loss        | -1.93e+04 |\n",
      "|    reward             | 22.027937 |\n",
      "|    std                | 1.27      |\n",
      "|    value_loss         | 2.22e+05  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 79500      |\n",
      "|    time_elapsed       | 1900       |\n",
      "|    total_timesteps    | 397500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.4      |\n",
      "|    explained_variance | 0.193      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 79499      |\n",
      "|    policy_loss        | -1.79e+03  |\n",
      "|    reward             | -27.108494 |\n",
      "|    std                | 1.27       |\n",
      "|    value_loss         | 2.65e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 79600     |\n",
      "|    time_elapsed       | 1902      |\n",
      "|    total_timesteps    | 398000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.4     |\n",
      "|    explained_variance | -0.0342   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 79599     |\n",
      "|    policy_loss        | -2.47e+03 |\n",
      "|    reward             | -6.154063 |\n",
      "|    std                | 1.27      |\n",
      "|    value_loss         | 4.99e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 79700      |\n",
      "|    time_elapsed       | 1905       |\n",
      "|    total_timesteps    | 398500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.4      |\n",
      "|    explained_variance | 0.443      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 79699      |\n",
      "|    policy_loss        | 5.37e+03   |\n",
      "|    reward             | -180.75635 |\n",
      "|    std                | 1.27       |\n",
      "|    value_loss         | 1.31e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 79800     |\n",
      "|    time_elapsed       | 1907      |\n",
      "|    total_timesteps    | 399000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.5     |\n",
      "|    explained_variance | -0.316    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 79799     |\n",
      "|    policy_loss        | 9.92e+03  |\n",
      "|    reward             | 41.102173 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 4.04e+04  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 209      |\n",
      "|    iterations         | 79900    |\n",
      "|    time_elapsed       | 1909     |\n",
      "|    total_timesteps    | 399500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -49.4    |\n",
      "|    explained_variance | -0.0879  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79899    |\n",
      "|    policy_loss        | 984      |\n",
      "|    reward             | 528.2186 |\n",
      "|    std                | 1.28     |\n",
      "|    value_loss         | 1.15e+04 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 80000     |\n",
      "|    time_elapsed       | 1912      |\n",
      "|    total_timesteps    | 400000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.5     |\n",
      "|    explained_variance | 0.0507    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 79999     |\n",
      "|    policy_loss        | 3.32e+04  |\n",
      "|    reward             | 300.35754 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 4.69e+05  |\n",
      "-------------------------------------\n",
      "day: 3102, episode: 130\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5325542.73\n",
      "total_reward: 4325542.73\n",
      "total_cost: 58854.95\n",
      "total_trades: 46879\n",
      "Sharpe: 3.858\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 80100     |\n",
      "|    time_elapsed       | 1914      |\n",
      "|    total_timesteps    | 400500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.5     |\n",
      "|    explained_variance | 0.168     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 80099     |\n",
      "|    policy_loss        | -2.13e+03 |\n",
      "|    reward             | 70.05951  |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 7.31e+03  |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 208         |\n",
      "|    iterations         | 80200       |\n",
      "|    time_elapsed       | 1923        |\n",
      "|    total_timesteps    | 401000      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -49.5       |\n",
      "|    explained_variance | -0.129      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 80199       |\n",
      "|    policy_loss        | 6.82e+03    |\n",
      "|    reward             | -101.130646 |\n",
      "|    std                | 1.28        |\n",
      "|    value_loss         | 2.23e+04    |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 80300     |\n",
      "|    time_elapsed       | 1925      |\n",
      "|    total_timesteps    | 401500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.5     |\n",
      "|    explained_variance | -0.00911  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 80299     |\n",
      "|    policy_loss        | -5.45e+03 |\n",
      "|    reward             | 109.51209 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 1.56e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 80400      |\n",
      "|    time_elapsed       | 1928       |\n",
      "|    total_timesteps    | 402000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.4      |\n",
      "|    explained_variance | 0.0446     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 80399      |\n",
      "|    policy_loss        | -4.39e+03  |\n",
      "|    reward             | -237.83942 |\n",
      "|    std                | 1.27       |\n",
      "|    value_loss         | 1.59e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 80500      |\n",
      "|    time_elapsed       | 1930       |\n",
      "|    total_timesteps    | 402500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.4      |\n",
      "|    explained_variance | 0.0975     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 80499      |\n",
      "|    policy_loss        | 1.86e+03   |\n",
      "|    reward             | -273.74213 |\n",
      "|    std                | 1.27       |\n",
      "|    value_loss         | 1.74e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 80600     |\n",
      "|    time_elapsed       | 1932      |\n",
      "|    total_timesteps    | 403000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.4     |\n",
      "|    explained_variance | 0.102     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 80599     |\n",
      "|    policy_loss        | 2.35e+03  |\n",
      "|    reward             | 200.92535 |\n",
      "|    std                | 1.27      |\n",
      "|    value_loss         | 8.95e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 80700    |\n",
      "|    time_elapsed       | 1935     |\n",
      "|    total_timesteps    | 403500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -49.4    |\n",
      "|    explained_variance | 0.532    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80699    |\n",
      "|    policy_loss        | -483     |\n",
      "|    reward             | 97.63139 |\n",
      "|    std                | 1.27     |\n",
      "|    value_loss         | 278      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 80800      |\n",
      "|    time_elapsed       | 1937       |\n",
      "|    total_timesteps    | 404000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.4      |\n",
      "|    explained_variance | 0.179      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 80799      |\n",
      "|    policy_loss        | -566       |\n",
      "|    reward             | -41.123684 |\n",
      "|    std                | 1.27       |\n",
      "|    value_loss         | 1.46e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 80900      |\n",
      "|    time_elapsed       | 1939       |\n",
      "|    total_timesteps    | 404500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.5      |\n",
      "|    explained_variance | 0.241      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 80899      |\n",
      "|    policy_loss        | -2.02e+03  |\n",
      "|    reward             | -56.121716 |\n",
      "|    std                | 1.28       |\n",
      "|    value_loss         | 2.36e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 81000     |\n",
      "|    time_elapsed       | 1942      |\n",
      "|    total_timesteps    | 405000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.5     |\n",
      "|    explained_variance | -0.339    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 80999     |\n",
      "|    policy_loss        | 1.3e+03   |\n",
      "|    reward             | -70.05927 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 4.15e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 81100     |\n",
      "|    time_elapsed       | 1944      |\n",
      "|    total_timesteps    | 405500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.5     |\n",
      "|    explained_variance | 0.162     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 81099     |\n",
      "|    policy_loss        | 2.13e+03  |\n",
      "|    reward             | -161.1952 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 4.83e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 81200     |\n",
      "|    time_elapsed       | 1946      |\n",
      "|    total_timesteps    | 406000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.5     |\n",
      "|    explained_variance | 0.0436    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 81199     |\n",
      "|    policy_loss        | -6.24e+03 |\n",
      "|    reward             | 86.342445 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 3.48e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 81300      |\n",
      "|    time_elapsed       | 1949       |\n",
      "|    total_timesteps    | 406500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.6      |\n",
      "|    explained_variance | -0.0274    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 81299      |\n",
      "|    policy_loss        | 6.8e+03    |\n",
      "|    reward             | -7.9053073 |\n",
      "|    std                | 1.28       |\n",
      "|    value_loss         | 3.22e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 81400     |\n",
      "|    time_elapsed       | 1951      |\n",
      "|    total_timesteps    | 407000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.6     |\n",
      "|    explained_variance | 0.0331    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 81399     |\n",
      "|    policy_loss        | -2.18e+03 |\n",
      "|    reward             | -68.79211 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 4.56e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 81500     |\n",
      "|    time_elapsed       | 1953      |\n",
      "|    total_timesteps    | 407500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.6     |\n",
      "|    explained_variance | -0.3      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 81499     |\n",
      "|    policy_loss        | -450      |\n",
      "|    reward             | 19.631607 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 3.93e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 81600     |\n",
      "|    time_elapsed       | 1956      |\n",
      "|    total_timesteps    | 408000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.6     |\n",
      "|    explained_variance | 0.362     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 81599     |\n",
      "|    policy_loss        | -641      |\n",
      "|    reward             | 6.8876734 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 900       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 81700     |\n",
      "|    time_elapsed       | 1958      |\n",
      "|    total_timesteps    | 408500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.6     |\n",
      "|    explained_variance | 0.113     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 81699     |\n",
      "|    policy_loss        | 187       |\n",
      "|    reward             | 210.30518 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 5.84e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 81800     |\n",
      "|    time_elapsed       | 1960      |\n",
      "|    total_timesteps    | 409000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.5     |\n",
      "|    explained_variance | -0.0628   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 81799     |\n",
      "|    policy_loss        | -4.05e+03 |\n",
      "|    reward             | 200.06058 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 8.09e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 81900      |\n",
      "|    time_elapsed       | 1963       |\n",
      "|    total_timesteps    | 409500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.6      |\n",
      "|    explained_variance | -0.0336    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 81899      |\n",
      "|    policy_loss        | 2.78e+03   |\n",
      "|    reward             | -184.48499 |\n",
      "|    std                | 1.28       |\n",
      "|    value_loss         | 2.84e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 82000     |\n",
      "|    time_elapsed       | 1965      |\n",
      "|    total_timesteps    | 410000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.6     |\n",
      "|    explained_variance | 0.14      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 81999     |\n",
      "|    policy_loss        | 853       |\n",
      "|    reward             | -41.34206 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 1.31e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 82100     |\n",
      "|    time_elapsed       | 1967      |\n",
      "|    total_timesteps    | 410500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.6     |\n",
      "|    explained_variance | -0.222    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 82099     |\n",
      "|    policy_loss        | 441       |\n",
      "|    reward             | 58.454338 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 437       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 82200      |\n",
      "|    time_elapsed       | 1970       |\n",
      "|    total_timesteps    | 411000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.7      |\n",
      "|    explained_variance | 0.0777     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 82199      |\n",
      "|    policy_loss        | -6.4e+03   |\n",
      "|    reward             | -0.2001564 |\n",
      "|    std                | 1.28       |\n",
      "|    value_loss         | 2.88e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 82300     |\n",
      "|    time_elapsed       | 1972      |\n",
      "|    total_timesteps    | 411500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.7     |\n",
      "|    explained_variance | -0.152    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 82299     |\n",
      "|    policy_loss        | -7.62e+03 |\n",
      "|    reward             | -62.14979 |\n",
      "|    std                | 1.29      |\n",
      "|    value_loss         | 2.89e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 82400      |\n",
      "|    time_elapsed       | 1974       |\n",
      "|    total_timesteps    | 412000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.7      |\n",
      "|    explained_variance | 0.0696     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 82399      |\n",
      "|    policy_loss        | -3.39e+03  |\n",
      "|    reward             | -144.61093 |\n",
      "|    std                | 1.28       |\n",
      "|    value_loss         | 3.12e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 82500      |\n",
      "|    time_elapsed       | 1977       |\n",
      "|    total_timesteps    | 412500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.7      |\n",
      "|    explained_variance | -0.0143    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 82499      |\n",
      "|    policy_loss        | 235        |\n",
      "|    reward             | -52.287098 |\n",
      "|    std                | 1.28       |\n",
      "|    value_loss         | 2.1e+04    |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 82600     |\n",
      "|    time_elapsed       | 1979      |\n",
      "|    total_timesteps    | 413000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.6     |\n",
      "|    explained_variance | 0.396     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 82599     |\n",
      "|    policy_loss        | -2.69e+03 |\n",
      "|    reward             | -81.16201 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 3.83e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 82700     |\n",
      "|    time_elapsed       | 1981      |\n",
      "|    total_timesteps    | 413500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.6     |\n",
      "|    explained_variance | -0.143    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 82699     |\n",
      "|    policy_loss        | 3.9e+03   |\n",
      "|    reward             | -26.47199 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 1.17e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 82800     |\n",
      "|    time_elapsed       | 1984      |\n",
      "|    total_timesteps    | 414000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.7     |\n",
      "|    explained_variance | 0.216     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 82799     |\n",
      "|    policy_loss        | -3.67e+03 |\n",
      "|    reward             | 3.4431336 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 1.46e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 82900     |\n",
      "|    time_elapsed       | 1986      |\n",
      "|    total_timesteps    | 414500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.6     |\n",
      "|    explained_variance | 0.147     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 82899     |\n",
      "|    policy_loss        | -4.09e+03 |\n",
      "|    reward             | 135.13249 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 2.62e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 83000     |\n",
      "|    time_elapsed       | 1988      |\n",
      "|    total_timesteps    | 415000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.6     |\n",
      "|    explained_variance | -0.0211   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 82999     |\n",
      "|    policy_loss        | -9.87e+03 |\n",
      "|    reward             | 233.47308 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 7.07e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 83100     |\n",
      "|    time_elapsed       | 1991      |\n",
      "|    total_timesteps    | 415500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.6     |\n",
      "|    explained_variance | -0.0394   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 83099     |\n",
      "|    policy_loss        | -4.34e+03 |\n",
      "|    reward             | 62.518066 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 4.29e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 83200     |\n",
      "|    time_elapsed       | 1993      |\n",
      "|    total_timesteps    | 416000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.6     |\n",
      "|    explained_variance | 0.0684    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 83199     |\n",
      "|    policy_loss        | 3.64e+03  |\n",
      "|    reward             | 28.920795 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 6.02e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 83300    |\n",
      "|    time_elapsed       | 1996     |\n",
      "|    total_timesteps    | 416500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -49.6    |\n",
      "|    explained_variance | -0.0891  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83299    |\n",
      "|    policy_loss        | 2.82e+03 |\n",
      "|    reward             | 5.355794 |\n",
      "|    std                | 1.28     |\n",
      "|    value_loss         | 1.01e+04 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 83400      |\n",
      "|    time_elapsed       | 1998       |\n",
      "|    total_timesteps    | 417000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.7      |\n",
      "|    explained_variance | 0.182      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 83399      |\n",
      "|    policy_loss        | -1.2e+03   |\n",
      "|    reward             | -139.53961 |\n",
      "|    std                | 1.28       |\n",
      "|    value_loss         | 1.47e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 83500     |\n",
      "|    time_elapsed       | 2000      |\n",
      "|    total_timesteps    | 417500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.7     |\n",
      "|    explained_variance | -0.0652   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 83499     |\n",
      "|    policy_loss        | -2.88e+03 |\n",
      "|    reward             | -85.71265 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 1.27e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 83600      |\n",
      "|    time_elapsed       | 2003       |\n",
      "|    total_timesteps    | 418000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.8      |\n",
      "|    explained_variance | 0.0319     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 83599      |\n",
      "|    policy_loss        | -4.63e+03  |\n",
      "|    reward             | -175.87119 |\n",
      "|    std                | 1.29       |\n",
      "|    value_loss         | 2.01e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 83700     |\n",
      "|    time_elapsed       | 2005      |\n",
      "|    total_timesteps    | 418500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.8     |\n",
      "|    explained_variance | -0.0153   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 83699     |\n",
      "|    policy_loss        | -2.21e+03 |\n",
      "|    reward             | 54.082134 |\n",
      "|    std                | 1.29      |\n",
      "|    value_loss         | 3.12e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 83800     |\n",
      "|    time_elapsed       | 2007      |\n",
      "|    total_timesteps    | 419000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.9     |\n",
      "|    explained_variance | -0.342    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 83799     |\n",
      "|    policy_loss        | 1.95e+03  |\n",
      "|    reward             | 63.523605 |\n",
      "|    std                | 1.29      |\n",
      "|    value_loss         | 2.23e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 83900      |\n",
      "|    time_elapsed       | 2010       |\n",
      "|    total_timesteps    | 419500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.9      |\n",
      "|    explained_variance | 0.215      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 83899      |\n",
      "|    policy_loss        | 1.55e+03   |\n",
      "|    reward             | -25.716368 |\n",
      "|    std                | 1.29       |\n",
      "|    value_loss         | 2.52e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 84000      |\n",
      "|    time_elapsed       | 2012       |\n",
      "|    total_timesteps    | 420000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.8      |\n",
      "|    explained_variance | -0.097     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 83999      |\n",
      "|    policy_loss        | -3.55e+03  |\n",
      "|    reward             | -118.44095 |\n",
      "|    std                | 1.29       |\n",
      "|    value_loss         | 6.44e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 84100      |\n",
      "|    time_elapsed       | 2014       |\n",
      "|    total_timesteps    | 420500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.9      |\n",
      "|    explained_variance | 0.225      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 84099      |\n",
      "|    policy_loss        | 182        |\n",
      "|    reward             | -0.9405422 |\n",
      "|    std                | 1.29       |\n",
      "|    value_loss         | 926        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 84200      |\n",
      "|    time_elapsed       | 2017       |\n",
      "|    total_timesteps    | 421000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.9      |\n",
      "|    explained_variance | 0.00645    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 84199      |\n",
      "|    policy_loss        | 1.33e+03   |\n",
      "|    reward             | -116.34984 |\n",
      "|    std                | 1.29       |\n",
      "|    value_loss         | 7.73e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 84300     |\n",
      "|    time_elapsed       | 2019      |\n",
      "|    total_timesteps    | 421500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50       |\n",
      "|    explained_variance | 0.127     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 84299     |\n",
      "|    policy_loss        | -1.52e+03 |\n",
      "|    reward             | 117.47822 |\n",
      "|    std                | 1.3       |\n",
      "|    value_loss         | 1.23e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 84400     |\n",
      "|    time_elapsed       | 2022      |\n",
      "|    total_timesteps    | 422000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50       |\n",
      "|    explained_variance | 0.00995   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 84399     |\n",
      "|    policy_loss        | -1.28e+03 |\n",
      "|    reward             | 156.26361 |\n",
      "|    std                | 1.3       |\n",
      "|    value_loss         | 3.49e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 84500      |\n",
      "|    time_elapsed       | 2024       |\n",
      "|    total_timesteps    | 422500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.9      |\n",
      "|    explained_variance | 0.0954     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 84499      |\n",
      "|    policy_loss        | 170        |\n",
      "|    reward             | -44.063576 |\n",
      "|    std                | 1.3        |\n",
      "|    value_loss         | 3.95e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 84600      |\n",
      "|    time_elapsed       | 2026       |\n",
      "|    total_timesteps    | 423000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.9      |\n",
      "|    explained_variance | 0.437      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 84599      |\n",
      "|    policy_loss        | 4.89e+03   |\n",
      "|    reward             | -108.20624 |\n",
      "|    std                | 1.29       |\n",
      "|    value_loss         | 9.17e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 84700     |\n",
      "|    time_elapsed       | 2029      |\n",
      "|    total_timesteps    | 423500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.9     |\n",
      "|    explained_variance | -0.0415   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 84699     |\n",
      "|    policy_loss        | 1.63e+03  |\n",
      "|    reward             | 19.413162 |\n",
      "|    std                | 1.29      |\n",
      "|    value_loss         | 2.23e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 84800     |\n",
      "|    time_elapsed       | 2031      |\n",
      "|    total_timesteps    | 424000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.9     |\n",
      "|    explained_variance | 0.0995    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 84799     |\n",
      "|    policy_loss        | -3.76e+03 |\n",
      "|    reward             | 123.37244 |\n",
      "|    std                | 1.29      |\n",
      "|    value_loss         | 7.83e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 84900     |\n",
      "|    time_elapsed       | 2033      |\n",
      "|    total_timesteps    | 424500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.9     |\n",
      "|    explained_variance | -0.000417 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 84899     |\n",
      "|    policy_loss        | -1.16e+03 |\n",
      "|    reward             | -83.22428 |\n",
      "|    std                | 1.29      |\n",
      "|    value_loss         | 1.34e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 85000      |\n",
      "|    time_elapsed       | 2036       |\n",
      "|    total_timesteps    | 425000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.9      |\n",
      "|    explained_variance | 0.0489     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 84999      |\n",
      "|    policy_loss        | -8.42e+03  |\n",
      "|    reward             | -62.500954 |\n",
      "|    std                | 1.29       |\n",
      "|    value_loss         | 5.2e+04    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 85100      |\n",
      "|    time_elapsed       | 2038       |\n",
      "|    total_timesteps    | 425500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.9      |\n",
      "|    explained_variance | -0.209     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 85099      |\n",
      "|    policy_loss        | 235        |\n",
      "|    reward             | -1.5212352 |\n",
      "|    std                | 1.3        |\n",
      "|    value_loss         | 1.05e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 85200      |\n",
      "|    time_elapsed       | 2040       |\n",
      "|    total_timesteps    | 426000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.9      |\n",
      "|    explained_variance | -0.11      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 85199      |\n",
      "|    policy_loss        | 3.53e+03   |\n",
      "|    reward             | -64.539116 |\n",
      "|    std                | 1.29       |\n",
      "|    value_loss         | 9.78e+03   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 85300    |\n",
      "|    time_elapsed       | 2043     |\n",
      "|    total_timesteps    | 426500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -49.9    |\n",
      "|    explained_variance | 0.0302   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85299    |\n",
      "|    policy_loss        | 4.35e+03 |\n",
      "|    reward             | 87.29037 |\n",
      "|    std                | 1.3      |\n",
      "|    value_loss         | 1.48e+04 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 85400     |\n",
      "|    time_elapsed       | 2045      |\n",
      "|    total_timesteps    | 427000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -49.9     |\n",
      "|    explained_variance | 0.0746    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 85399     |\n",
      "|    policy_loss        | -4.04e+03 |\n",
      "|    reward             | 23.942469 |\n",
      "|    std                | 1.3       |\n",
      "|    value_loss         | 1.04e+04  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 85500    |\n",
      "|    time_elapsed       | 2047     |\n",
      "|    total_timesteps    | 427500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -49.9    |\n",
      "|    explained_variance | -0.0771  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85499    |\n",
      "|    policy_loss        | 1.28e+04 |\n",
      "|    reward             | 48.58528 |\n",
      "|    std                | 1.3      |\n",
      "|    value_loss         | 6.18e+04 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 85600     |\n",
      "|    time_elapsed       | 2050      |\n",
      "|    total_timesteps    | 428000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50       |\n",
      "|    explained_variance | -0.138    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 85599     |\n",
      "|    policy_loss        | 4.03e+03  |\n",
      "|    reward             | -3.641321 |\n",
      "|    std                | 1.3       |\n",
      "|    value_loss         | 1.25e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 85700      |\n",
      "|    time_elapsed       | 2052       |\n",
      "|    total_timesteps    | 428500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50        |\n",
      "|    explained_variance | 0.622      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 85699      |\n",
      "|    policy_loss        | 1.44e+03   |\n",
      "|    reward             | -27.822649 |\n",
      "|    std                | 1.3        |\n",
      "|    value_loss         | 1.13e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 85800      |\n",
      "|    time_elapsed       | 2054       |\n",
      "|    total_timesteps    | 429000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50        |\n",
      "|    explained_variance | 0.0215     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 85799      |\n",
      "|    policy_loss        | -778       |\n",
      "|    reward             | -10.960166 |\n",
      "|    std                | 1.3        |\n",
      "|    value_loss         | 2.37e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 85900      |\n",
      "|    time_elapsed       | 2057       |\n",
      "|    total_timesteps    | 429500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50        |\n",
      "|    explained_variance | -0.0303    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 85899      |\n",
      "|    policy_loss        | -5.25e+03  |\n",
      "|    reward             | -245.48605 |\n",
      "|    std                | 1.3        |\n",
      "|    value_loss         | 1.2e+04    |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 86000     |\n",
      "|    time_elapsed       | 2059      |\n",
      "|    total_timesteps    | 430000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.1     |\n",
      "|    explained_variance | 0.21      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 85999     |\n",
      "|    policy_loss        | 6.38e+03  |\n",
      "|    reward             | 102.09603 |\n",
      "|    std                | 1.3       |\n",
      "|    value_loss         | 1.98e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 86100     |\n",
      "|    time_elapsed       | 2061      |\n",
      "|    total_timesteps    | 430500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.2     |\n",
      "|    explained_variance | 0.0669    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 86099     |\n",
      "|    policy_loss        | -133      |\n",
      "|    reward             | 161.59833 |\n",
      "|    std                | 1.31      |\n",
      "|    value_loss         | 4.47e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 86200      |\n",
      "|    time_elapsed       | 2064       |\n",
      "|    total_timesteps    | 431000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.2      |\n",
      "|    explained_variance | 0.075      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 86199      |\n",
      "|    policy_loss        | -5.62e+03  |\n",
      "|    reward             | -93.796425 |\n",
      "|    std                | 1.31       |\n",
      "|    value_loss         | 5.72e+04   |\n",
      "--------------------------------------\n",
      "day: 3102, episode: 140\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7073280.89\n",
      "total_reward: 6073280.89\n",
      "total_cost: 45610.44\n",
      "total_trades: 52911\n",
      "Sharpe: 2.981\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 86300     |\n",
      "|    time_elapsed       | 2066      |\n",
      "|    total_timesteps    | 431500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.2     |\n",
      "|    explained_variance | 0.0244    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 86299     |\n",
      "|    policy_loss        | -746      |\n",
      "|    reward             | 10.357142 |\n",
      "|    std                | 1.31      |\n",
      "|    value_loss         | 2.82e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 86400     |\n",
      "|    time_elapsed       | 2068      |\n",
      "|    total_timesteps    | 432000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.1     |\n",
      "|    explained_variance | 0.048     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 86399     |\n",
      "|    policy_loss        | 2.17e+03  |\n",
      "|    reward             | 132.64793 |\n",
      "|    std                | 1.3       |\n",
      "|    value_loss         | 4.33e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 86500    |\n",
      "|    time_elapsed       | 2071     |\n",
      "|    total_timesteps    | 432500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -50.2    |\n",
      "|    explained_variance | 0.14     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86499    |\n",
      "|    policy_loss        | 4.73e+03 |\n",
      "|    reward             | 75.68504 |\n",
      "|    std                | 1.31     |\n",
      "|    value_loss         | 1.21e+04 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 86600     |\n",
      "|    time_elapsed       | 2073      |\n",
      "|    total_timesteps    | 433000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.2     |\n",
      "|    explained_variance | -0.14     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 86599     |\n",
      "|    policy_loss        | 562       |\n",
      "|    reward             | 51.032383 |\n",
      "|    std                | 1.31      |\n",
      "|    value_loss         | 471       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 86700    |\n",
      "|    time_elapsed       | 2075     |\n",
      "|    total_timesteps    | 433500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -50.2    |\n",
      "|    explained_variance | 0.0243   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86699    |\n",
      "|    policy_loss        | 291      |\n",
      "|    reward             | 133.3904 |\n",
      "|    std                | 1.31     |\n",
      "|    value_loss         | 2.12e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 86800     |\n",
      "|    time_elapsed       | 2078      |\n",
      "|    total_timesteps    | 434000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.3     |\n",
      "|    explained_variance | 0.0537    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 86799     |\n",
      "|    policy_loss        | 2.54e+03  |\n",
      "|    reward             | 26.969622 |\n",
      "|    std                | 1.31      |\n",
      "|    value_loss         | 4.13e+04  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 86900    |\n",
      "|    time_elapsed       | 2080     |\n",
      "|    total_timesteps    | 434500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -50.3    |\n",
      "|    explained_variance | 0.202    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86899    |\n",
      "|    policy_loss        | 1.7e+03  |\n",
      "|    reward             | 14.22303 |\n",
      "|    std                | 1.31     |\n",
      "|    value_loss         | 1.25e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 87000     |\n",
      "|    time_elapsed       | 2082      |\n",
      "|    total_timesteps    | 435000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.3     |\n",
      "|    explained_variance | 0.226     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 86999     |\n",
      "|    policy_loss        | 3.43e+03  |\n",
      "|    reward             | 33.478657 |\n",
      "|    std                | 1.31      |\n",
      "|    value_loss         | 7.63e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 87100     |\n",
      "|    time_elapsed       | 2085      |\n",
      "|    total_timesteps    | 435500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.3     |\n",
      "|    explained_variance | 0.514     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 87099     |\n",
      "|    policy_loss        | -1.07e+03 |\n",
      "|    reward             | 112.57759 |\n",
      "|    std                | 1.31      |\n",
      "|    value_loss         | 1.26e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 87200      |\n",
      "|    time_elapsed       | 2087       |\n",
      "|    total_timesteps    | 436000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.3      |\n",
      "|    explained_variance | 0.0154     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 87199      |\n",
      "|    policy_loss        | -9.36e+03  |\n",
      "|    reward             | -24.362984 |\n",
      "|    std                | 1.31       |\n",
      "|    value_loss         | 4.25e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 87300     |\n",
      "|    time_elapsed       | 2089      |\n",
      "|    total_timesteps    | 436500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.2     |\n",
      "|    explained_variance | 0.101     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 87299     |\n",
      "|    policy_loss        | 1.52e+04  |\n",
      "|    reward             | -71.72184 |\n",
      "|    std                | 1.31      |\n",
      "|    value_loss         | 9.35e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 87400      |\n",
      "|    time_elapsed       | 2092       |\n",
      "|    total_timesteps    | 437000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.2      |\n",
      "|    explained_variance | 0.0239     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 87399      |\n",
      "|    policy_loss        | -1.04e+04  |\n",
      "|    reward             | -173.10497 |\n",
      "|    std                | 1.31       |\n",
      "|    value_loss         | 6.04e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 87500     |\n",
      "|    time_elapsed       | 2094      |\n",
      "|    total_timesteps    | 437500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.2     |\n",
      "|    explained_variance | 0.0204    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 87499     |\n",
      "|    policy_loss        | -1.58e+04 |\n",
      "|    reward             | 199.119   |\n",
      "|    std                | 1.31      |\n",
      "|    value_loss         | 1.24e+05  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 87600      |\n",
      "|    time_elapsed       | 2096       |\n",
      "|    total_timesteps    | 438000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.3      |\n",
      "|    explained_variance | 0.248      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 87599      |\n",
      "|    policy_loss        | -4.99e+03  |\n",
      "|    reward             | -24.694635 |\n",
      "|    std                | 1.31       |\n",
      "|    value_loss         | 1.35e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 87700      |\n",
      "|    time_elapsed       | 2099       |\n",
      "|    total_timesteps    | 438500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.3      |\n",
      "|    explained_variance | 0.0676     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 87699      |\n",
      "|    policy_loss        | -3e+03     |\n",
      "|    reward             | -49.913456 |\n",
      "|    std                | 1.31       |\n",
      "|    value_loss         | 5.05e+03   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 87800    |\n",
      "|    time_elapsed       | 2101     |\n",
      "|    total_timesteps    | 439000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -50.3    |\n",
      "|    explained_variance | 0.0425   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87799    |\n",
      "|    policy_loss        | 1.88e+03 |\n",
      "|    reward             | 82.12434 |\n",
      "|    std                | 1.31     |\n",
      "|    value_loss         | 1.99e+03 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 87900      |\n",
      "|    time_elapsed       | 2103       |\n",
      "|    total_timesteps    | 439500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.3      |\n",
      "|    explained_variance | 0.142      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 87899      |\n",
      "|    policy_loss        | 767        |\n",
      "|    reward             | -54.706173 |\n",
      "|    std                | 1.31       |\n",
      "|    value_loss         | 8.13e+03   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 88000    |\n",
      "|    time_elapsed       | 2105     |\n",
      "|    total_timesteps    | 440000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -50.3    |\n",
      "|    explained_variance | 0.0305   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87999    |\n",
      "|    policy_loss        | 1.2e+04  |\n",
      "|    reward             | 97.20416 |\n",
      "|    std                | 1.31     |\n",
      "|    value_loss         | 6.23e+04 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 88100     |\n",
      "|    time_elapsed       | 2108      |\n",
      "|    total_timesteps    | 440500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.3     |\n",
      "|    explained_variance | -0.034    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 88099     |\n",
      "|    policy_loss        | -8.29e+03 |\n",
      "|    reward             | 76.03492  |\n",
      "|    std                | 1.31      |\n",
      "|    value_loss         | 3.1e+04   |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 88200      |\n",
      "|    time_elapsed       | 2110       |\n",
      "|    total_timesteps    | 441000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.3      |\n",
      "|    explained_variance | 0.136      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 88199      |\n",
      "|    policy_loss        | 10.3       |\n",
      "|    reward             | -14.184921 |\n",
      "|    std                | 1.31       |\n",
      "|    value_loss         | 1.63e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 88300     |\n",
      "|    time_elapsed       | 2112      |\n",
      "|    total_timesteps    | 441500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.4     |\n",
      "|    explained_variance | -0.0339   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 88299     |\n",
      "|    policy_loss        | -156      |\n",
      "|    reward             | -68.89541 |\n",
      "|    std                | 1.31      |\n",
      "|    value_loss         | 2.36e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 88400      |\n",
      "|    time_elapsed       | 2115       |\n",
      "|    total_timesteps    | 442000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.4      |\n",
      "|    explained_variance | 0.00951    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 88399      |\n",
      "|    policy_loss        | 1.94e+03   |\n",
      "|    reward             | -30.543648 |\n",
      "|    std                | 1.31       |\n",
      "|    value_loss         | 2.03e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 88500     |\n",
      "|    time_elapsed       | 2117      |\n",
      "|    total_timesteps    | 442500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.4     |\n",
      "|    explained_variance | 0.144     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 88499     |\n",
      "|    policy_loss        | 473       |\n",
      "|    reward             | -111.5149 |\n",
      "|    std                | 1.32      |\n",
      "|    value_loss         | 4.81e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 88600     |\n",
      "|    time_elapsed       | 2119      |\n",
      "|    total_timesteps    | 443000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.4     |\n",
      "|    explained_variance | -0.066    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 88599     |\n",
      "|    policy_loss        | -1.11e+04 |\n",
      "|    reward             | 0.1490551 |\n",
      "|    std                | 1.32      |\n",
      "|    value_loss         | 6.56e+04  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 88700    |\n",
      "|    time_elapsed       | 2122     |\n",
      "|    total_timesteps    | 443500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -50.4    |\n",
      "|    explained_variance | -0.0187  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88699    |\n",
      "|    policy_loss        | 573      |\n",
      "|    reward             | 467.3026 |\n",
      "|    std                | 1.32     |\n",
      "|    value_loss         | 7.86e+04 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 88800      |\n",
      "|    time_elapsed       | 2124       |\n",
      "|    total_timesteps    | 444000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.4      |\n",
      "|    explained_variance | -0.161     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 88799      |\n",
      "|    policy_loss        | -848       |\n",
      "|    reward             | -28.363071 |\n",
      "|    std                | 1.32       |\n",
      "|    value_loss         | 655        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 208      |\n",
      "|    iterations         | 88900    |\n",
      "|    time_elapsed       | 2126     |\n",
      "|    total_timesteps    | 444500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -50.4    |\n",
      "|    explained_variance | 0.0783   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88899    |\n",
      "|    policy_loss        | 2.55e+03 |\n",
      "|    reward             | 57.05891 |\n",
      "|    std                | 1.32     |\n",
      "|    value_loss         | 3.59e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 208       |\n",
      "|    iterations         | 89000     |\n",
      "|    time_elapsed       | 2129      |\n",
      "|    total_timesteps    | 445000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.4     |\n",
      "|    explained_variance | 0.0256    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 88999     |\n",
      "|    policy_loss        | -3.35e+03 |\n",
      "|    reward             | 102.43422 |\n",
      "|    std                | 1.32      |\n",
      "|    value_loss         | 8.84e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 208        |\n",
      "|    iterations         | 89100      |\n",
      "|    time_elapsed       | 2131       |\n",
      "|    total_timesteps    | 445500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.4      |\n",
      "|    explained_variance | 0.227      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 89099      |\n",
      "|    policy_loss        | -1.05e+03  |\n",
      "|    reward             | -6.1743336 |\n",
      "|    std                | 1.32       |\n",
      "|    value_loss         | 7.23e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 89200     |\n",
      "|    time_elapsed       | 2133      |\n",
      "|    total_timesteps    | 446000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.5     |\n",
      "|    explained_variance | 0.053     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 89199     |\n",
      "|    policy_loss        | 823       |\n",
      "|    reward             | 58.128025 |\n",
      "|    std                | 1.32      |\n",
      "|    value_loss         | 4.69e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 89300     |\n",
      "|    time_elapsed       | 2136      |\n",
      "|    total_timesteps    | 446500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.5     |\n",
      "|    explained_variance | -0.0131   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 89299     |\n",
      "|    policy_loss        | -1.24e+04 |\n",
      "|    reward             | 100.89925 |\n",
      "|    std                | 1.32      |\n",
      "|    value_loss         | 1.27e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 89400     |\n",
      "|    time_elapsed       | 2138      |\n",
      "|    total_timesteps    | 447000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.5     |\n",
      "|    explained_variance | -0.312    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 89399     |\n",
      "|    policy_loss        | -894      |\n",
      "|    reward             | 21.288721 |\n",
      "|    std                | 1.32      |\n",
      "|    value_loss         | 1.05e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 209      |\n",
      "|    iterations         | 89500    |\n",
      "|    time_elapsed       | 2140     |\n",
      "|    total_timesteps    | 447500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -50.5    |\n",
      "|    explained_variance | 0.132    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89499    |\n",
      "|    policy_loss        | 1.79e+03 |\n",
      "|    reward             | -95.314  |\n",
      "|    std                | 1.32     |\n",
      "|    value_loss         | 2.01e+03 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 89600      |\n",
      "|    time_elapsed       | 2143       |\n",
      "|    total_timesteps    | 448000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.5      |\n",
      "|    explained_variance | -0.0768    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 89599      |\n",
      "|    policy_loss        | 4.1e+03    |\n",
      "|    reward             | -21.439005 |\n",
      "|    std                | 1.32       |\n",
      "|    value_loss         | 1.02e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 89700      |\n",
      "|    time_elapsed       | 2145       |\n",
      "|    total_timesteps    | 448500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.6      |\n",
      "|    explained_variance | 0.0366     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 89699      |\n",
      "|    policy_loss        | 3.05e+03   |\n",
      "|    reward             | -126.53049 |\n",
      "|    std                | 1.33       |\n",
      "|    value_loss         | 4.89e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 89800     |\n",
      "|    time_elapsed       | 2147      |\n",
      "|    total_timesteps    | 449000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.6     |\n",
      "|    explained_variance | -0.0947   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 89799     |\n",
      "|    policy_loss        | 6.13e+03  |\n",
      "|    reward             | 161.98984 |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 2.47e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 89900     |\n",
      "|    time_elapsed       | 2150      |\n",
      "|    total_timesteps    | 449500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.6     |\n",
      "|    explained_variance | 0.0145    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 89899     |\n",
      "|    policy_loss        | 1.97e+03  |\n",
      "|    reward             | 72.863525 |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 3.96e+04  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 209      |\n",
      "|    iterations         | 90000    |\n",
      "|    time_elapsed       | 2152     |\n",
      "|    total_timesteps    | 450000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -50.6    |\n",
      "|    explained_variance | 0.0769   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89999    |\n",
      "|    policy_loss        | -204     |\n",
      "|    reward             | 4.308685 |\n",
      "|    std                | 1.33     |\n",
      "|    value_loss         | 1.24e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 209      |\n",
      "|    iterations         | 90100    |\n",
      "|    time_elapsed       | 2154     |\n",
      "|    total_timesteps    | 450500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -50.7    |\n",
      "|    explained_variance | 0.234    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90099    |\n",
      "|    policy_loss        | 2.9e+03  |\n",
      "|    reward             | 132.4105 |\n",
      "|    std                | 1.33     |\n",
      "|    value_loss         | 4.64e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 90200     |\n",
      "|    time_elapsed       | 2157      |\n",
      "|    total_timesteps    | 451000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.7     |\n",
      "|    explained_variance | 0.172     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 90199     |\n",
      "|    policy_loss        | -807      |\n",
      "|    reward             | 17.467749 |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 2.78e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 90300      |\n",
      "|    time_elapsed       | 2159       |\n",
      "|    total_timesteps    | 451500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.7      |\n",
      "|    explained_variance | 0.0819     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 90299      |\n",
      "|    policy_loss        | -2.7e+03   |\n",
      "|    reward             | -112.21902 |\n",
      "|    std                | 1.33       |\n",
      "|    value_loss         | 4.11e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 90400     |\n",
      "|    time_elapsed       | 2161      |\n",
      "|    total_timesteps    | 452000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.8     |\n",
      "|    explained_variance | 0.0988    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 90399     |\n",
      "|    policy_loss        | -1.82e+03 |\n",
      "|    reward             | -8.935719 |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 7.83e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 90500      |\n",
      "|    time_elapsed       | 2164       |\n",
      "|    total_timesteps    | 452500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.7      |\n",
      "|    explained_variance | 0.0633     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 90499      |\n",
      "|    policy_loss        | 7.26e+03   |\n",
      "|    reward             | -40.274143 |\n",
      "|    std                | 1.33       |\n",
      "|    value_loss         | 2.83e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 90600     |\n",
      "|    time_elapsed       | 2166      |\n",
      "|    total_timesteps    | 453000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.7     |\n",
      "|    explained_variance | -0.0657   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 90599     |\n",
      "|    policy_loss        | -7.17e+03 |\n",
      "|    reward             | -241.6261 |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 2.31e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 90700      |\n",
      "|    time_elapsed       | 2168       |\n",
      "|    total_timesteps    | 453500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.7      |\n",
      "|    explained_variance | 0.113      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 90699      |\n",
      "|    policy_loss        | -3.05e+03  |\n",
      "|    reward             | -31.141394 |\n",
      "|    std                | 1.33       |\n",
      "|    value_loss         | 6.32e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 90800     |\n",
      "|    time_elapsed       | 2171      |\n",
      "|    total_timesteps    | 454000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.7     |\n",
      "|    explained_variance | -0.534    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 90799     |\n",
      "|    policy_loss        | -1.2e+03  |\n",
      "|    reward             | -62.74842 |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 989       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 90900      |\n",
      "|    time_elapsed       | 2173       |\n",
      "|    total_timesteps    | 454500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.7      |\n",
      "|    explained_variance | 0.226      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 90899      |\n",
      "|    policy_loss        | 6.59e+03   |\n",
      "|    reward             | -79.369965 |\n",
      "|    std                | 1.33       |\n",
      "|    value_loss         | 1.73e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 91000     |\n",
      "|    time_elapsed       | 2175      |\n",
      "|    total_timesteps    | 455000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.6     |\n",
      "|    explained_variance | -0.0362   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 90999     |\n",
      "|    policy_loss        | 3.48e+03  |\n",
      "|    reward             | 264.91467 |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 1.29e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 91100     |\n",
      "|    time_elapsed       | 2178      |\n",
      "|    total_timesteps    | 455500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.7     |\n",
      "|    explained_variance | 0.0562    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 91099     |\n",
      "|    policy_loss        | -53.9     |\n",
      "|    reward             | -6.760493 |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 5.39e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 91200     |\n",
      "|    time_elapsed       | 2180      |\n",
      "|    total_timesteps    | 456000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.7     |\n",
      "|    explained_variance | -0.0106   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 91199     |\n",
      "|    policy_loss        | -1.79e+04 |\n",
      "|    reward             | 88.383995 |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 1.44e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 91300     |\n",
      "|    time_elapsed       | 2182      |\n",
      "|    total_timesteps    | 456500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.7     |\n",
      "|    explained_variance | 0.461     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 91299     |\n",
      "|    policy_loss        | -1.1e+03  |\n",
      "|    reward             | -8.631214 |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 1.07e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 91400     |\n",
      "|    time_elapsed       | 2185      |\n",
      "|    total_timesteps    | 457000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.7     |\n",
      "|    explained_variance | -0.0203   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 91399     |\n",
      "|    policy_loss        | 384       |\n",
      "|    reward             | -159.1964 |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 1.93e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 91500      |\n",
      "|    time_elapsed       | 2187       |\n",
      "|    total_timesteps    | 457500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.8      |\n",
      "|    explained_variance | 0.0344     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 91499      |\n",
      "|    policy_loss        | -1.31e+03  |\n",
      "|    reward             | -77.053566 |\n",
      "|    std                | 1.33       |\n",
      "|    value_loss         | 5.27e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 91600      |\n",
      "|    time_elapsed       | 2189       |\n",
      "|    total_timesteps    | 458000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.8      |\n",
      "|    explained_variance | -0.217     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 91599      |\n",
      "|    policy_loss        | -3.64e+03  |\n",
      "|    reward             | -151.32791 |\n",
      "|    std                | 1.33       |\n",
      "|    value_loss         | 6.98e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 91700     |\n",
      "|    time_elapsed       | 2192      |\n",
      "|    total_timesteps    | 458500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.8     |\n",
      "|    explained_variance | 0.0415    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 91699     |\n",
      "|    policy_loss        | -7.11e+03 |\n",
      "|    reward             | -18.43494 |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 2.36e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 91800     |\n",
      "|    time_elapsed       | 2194      |\n",
      "|    total_timesteps    | 459000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.7     |\n",
      "|    explained_variance | 0.0631    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 91799     |\n",
      "|    policy_loss        | 6.25e+03  |\n",
      "|    reward             | 72.221436 |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 2.36e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 91900     |\n",
      "|    time_elapsed       | 2196      |\n",
      "|    total_timesteps    | 459500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.7     |\n",
      "|    explained_variance | 0.145     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 91899     |\n",
      "|    policy_loss        | -152      |\n",
      "|    reward             | -82.32063 |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 5.73e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 92000     |\n",
      "|    time_elapsed       | 2199      |\n",
      "|    total_timesteps    | 460000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.7     |\n",
      "|    explained_variance | -0.0778   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 91999     |\n",
      "|    policy_loss        | -1.56e+03 |\n",
      "|    reward             | 50.91053  |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 5.75e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 92100     |\n",
      "|    time_elapsed       | 2201      |\n",
      "|    total_timesteps    | 460500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.7     |\n",
      "|    explained_variance | 0.0408    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 92099     |\n",
      "|    policy_loss        | -1.05e+04 |\n",
      "|    reward             | 267.34335 |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 7.41e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 92200     |\n",
      "|    time_elapsed       | 2203      |\n",
      "|    total_timesteps    | 461000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.8     |\n",
      "|    explained_variance | 0.129     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 92199     |\n",
      "|    policy_loss        | 3.1e+03   |\n",
      "|    reward             | 161.44357 |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 3.28e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 92300      |\n",
      "|    time_elapsed       | 2205       |\n",
      "|    total_timesteps    | 461500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.8      |\n",
      "|    explained_variance | 0.0581     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 92299      |\n",
      "|    policy_loss        | -1.51e+03  |\n",
      "|    reward             | -125.99718 |\n",
      "|    std                | 1.33       |\n",
      "|    value_loss         | 1.68e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 92400     |\n",
      "|    time_elapsed       | 2208      |\n",
      "|    total_timesteps    | 462000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.8     |\n",
      "|    explained_variance | 0.0589    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 92399     |\n",
      "|    policy_loss        | 4.51e+03  |\n",
      "|    reward             | -86.54228 |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 4.95e+04  |\n",
      "-------------------------------------\n",
      "day: 3102, episode: 150\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11841221.26\n",
      "total_reward: 10841221.26\n",
      "total_cost: 60070.40\n",
      "total_trades: 44156\n",
      "Sharpe: 3.033\n",
      "=================================\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 209      |\n",
      "|    iterations         | 92500    |\n",
      "|    time_elapsed       | 2210     |\n",
      "|    total_timesteps    | 462500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -50.8    |\n",
      "|    explained_variance | 0.125    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92499    |\n",
      "|    policy_loss        | 1.34e+03 |\n",
      "|    reward             | 36.32259 |\n",
      "|    std                | 1.33     |\n",
      "|    value_loss         | 1.02e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 92600     |\n",
      "|    time_elapsed       | 2212      |\n",
      "|    total_timesteps    | 463000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.8     |\n",
      "|    explained_variance | 0.0661    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 92599     |\n",
      "|    policy_loss        | -3.28e+03 |\n",
      "|    reward             | 155.9666  |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 8.58e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 92700      |\n",
      "|    time_elapsed       | 2215       |\n",
      "|    total_timesteps    | 463500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.7      |\n",
      "|    explained_variance | 0.099      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 92699      |\n",
      "|    policy_loss        | 5.97e+03   |\n",
      "|    reward             | -153.22131 |\n",
      "|    std                | 1.33       |\n",
      "|    value_loss         | 2.04e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 92800      |\n",
      "|    time_elapsed       | 2217       |\n",
      "|    total_timesteps    | 464000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.7      |\n",
      "|    explained_variance | 0.0955     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 92799      |\n",
      "|    policy_loss        | -4.99e+03  |\n",
      "|    reward             | -127.94081 |\n",
      "|    std                | 1.33       |\n",
      "|    value_loss         | 2.35e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 92900     |\n",
      "|    time_elapsed       | 2219      |\n",
      "|    total_timesteps    | 464500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.8     |\n",
      "|    explained_variance | 0.00651   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 92899     |\n",
      "|    policy_loss        | 2.08e+04  |\n",
      "|    reward             | 348.08133 |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 1.79e+05  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 93000      |\n",
      "|    time_elapsed       | 2222       |\n",
      "|    total_timesteps    | 465000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.8      |\n",
      "|    explained_variance | -0.0394    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 92999      |\n",
      "|    policy_loss        | 6.43e+03   |\n",
      "|    reward             | -278.41553 |\n",
      "|    std                | 1.34       |\n",
      "|    value_loss         | 7.08e+04   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 209      |\n",
      "|    iterations         | 93100    |\n",
      "|    time_elapsed       | 2224     |\n",
      "|    total_timesteps    | 465500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -50.8    |\n",
      "|    explained_variance | 0.217    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93099    |\n",
      "|    policy_loss        | 3.32e+03 |\n",
      "|    reward             | 34.05478 |\n",
      "|    std                | 1.34     |\n",
      "|    value_loss         | 4.64e+03 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 93200      |\n",
      "|    time_elapsed       | 2226       |\n",
      "|    total_timesteps    | 466000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.7      |\n",
      "|    explained_variance | 0.171      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 93199      |\n",
      "|    policy_loss        | 585        |\n",
      "|    reward             | -51.306694 |\n",
      "|    std                | 1.33       |\n",
      "|    value_loss         | 4.66e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 93300     |\n",
      "|    time_elapsed       | 2229      |\n",
      "|    total_timesteps    | 466500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.8     |\n",
      "|    explained_variance | 0.254     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 93299     |\n",
      "|    policy_loss        | -2.31e+03 |\n",
      "|    reward             | 11.818956 |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 6.79e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 93400     |\n",
      "|    time_elapsed       | 2231      |\n",
      "|    total_timesteps    | 467000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.8     |\n",
      "|    explained_variance | -0.0315   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 93399     |\n",
      "|    policy_loss        | -404      |\n",
      "|    reward             | 5.7151294 |\n",
      "|    std                | 1.34      |\n",
      "|    value_loss         | 8.24e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 93500     |\n",
      "|    time_elapsed       | 2233      |\n",
      "|    total_timesteps    | 467500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.8     |\n",
      "|    explained_variance | 0.0138    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 93499     |\n",
      "|    policy_loss        | -5.71e+03 |\n",
      "|    reward             | -64.19304 |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 1.47e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 93600     |\n",
      "|    time_elapsed       | 2236      |\n",
      "|    total_timesteps    | 468000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.8     |\n",
      "|    explained_variance | 0.0502    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 93599     |\n",
      "|    policy_loss        | -8.78e+03 |\n",
      "|    reward             | 231.841   |\n",
      "|    std                | 1.34      |\n",
      "|    value_loss         | 5.69e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 93700      |\n",
      "|    time_elapsed       | 2238       |\n",
      "|    total_timesteps    | 468500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.8      |\n",
      "|    explained_variance | 0.049      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 93699      |\n",
      "|    policy_loss        | 1.55e+04   |\n",
      "|    reward             | -276.84927 |\n",
      "|    std                | 1.33       |\n",
      "|    value_loss         | 1.93e+05   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 209      |\n",
      "|    iterations         | 93800    |\n",
      "|    time_elapsed       | 2240     |\n",
      "|    total_timesteps    | 469000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -50.8    |\n",
      "|    explained_variance | -0.0141  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93799    |\n",
      "|    policy_loss        | 7.01e+03 |\n",
      "|    reward             | 83.53432 |\n",
      "|    std                | 1.33     |\n",
      "|    value_loss         | 2.01e+04 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 93900      |\n",
      "|    time_elapsed       | 2243       |\n",
      "|    total_timesteps    | 469500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.7      |\n",
      "|    explained_variance | 0.156      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 93899      |\n",
      "|    policy_loss        | 2.52e+03   |\n",
      "|    reward             | -52.679607 |\n",
      "|    std                | 1.33       |\n",
      "|    value_loss         | 2.99e+03   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 209      |\n",
      "|    iterations         | 94000    |\n",
      "|    time_elapsed       | 2245     |\n",
      "|    total_timesteps    | 470000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -50.8    |\n",
      "|    explained_variance | 0.143    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93999    |\n",
      "|    policy_loss        | -5.7e+03 |\n",
      "|    reward             | 177.9612 |\n",
      "|    std                | 1.34     |\n",
      "|    value_loss         | 1.45e+04 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 94100     |\n",
      "|    time_elapsed       | 2247      |\n",
      "|    total_timesteps    | 470500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.8     |\n",
      "|    explained_variance | 0.0377    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 94099     |\n",
      "|    policy_loss        | 161       |\n",
      "|    reward             | 46.919243 |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 2.48e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 94200     |\n",
      "|    time_elapsed       | 2250      |\n",
      "|    total_timesteps    | 471000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.8     |\n",
      "|    explained_variance | -0.101    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 94199     |\n",
      "|    policy_loss        | -6.69e+03 |\n",
      "|    reward             | 205.11246 |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 3.54e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 94300     |\n",
      "|    time_elapsed       | 2252      |\n",
      "|    total_timesteps    | 471500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.8     |\n",
      "|    explained_variance | 0.0708    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 94299     |\n",
      "|    policy_loss        | -3.94e+03 |\n",
      "|    reward             | 553.90814 |\n",
      "|    std                | 1.34      |\n",
      "|    value_loss         | 4.6e+04   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 209      |\n",
      "|    iterations         | 94400    |\n",
      "|    time_elapsed       | 2254     |\n",
      "|    total_timesteps    | 472000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -50.8    |\n",
      "|    explained_variance | -0.173   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94399    |\n",
      "|    policy_loss        | 5.31e+03 |\n",
      "|    reward             | 67.84458 |\n",
      "|    std                | 1.34     |\n",
      "|    value_loss         | 1.43e+04 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 94500      |\n",
      "|    time_elapsed       | 2257       |\n",
      "|    total_timesteps    | 472500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.8      |\n",
      "|    explained_variance | 0.0523     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 94499      |\n",
      "|    policy_loss        | 1.63e+03   |\n",
      "|    reward             | -12.597783 |\n",
      "|    std                | 1.33       |\n",
      "|    value_loss         | 3.96e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 94600      |\n",
      "|    time_elapsed       | 2259       |\n",
      "|    total_timesteps    | 473000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.7      |\n",
      "|    explained_variance | 0.0595     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 94599      |\n",
      "|    policy_loss        | -1.06e+03  |\n",
      "|    reward             | -60.514957 |\n",
      "|    std                | 1.33       |\n",
      "|    value_loss         | 3.15e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 94700      |\n",
      "|    time_elapsed       | 2261       |\n",
      "|    total_timesteps    | 473500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.8      |\n",
      "|    explained_variance | 0.242      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 94699      |\n",
      "|    policy_loss        | -909       |\n",
      "|    reward             | -113.00658 |\n",
      "|    std                | 1.33       |\n",
      "|    value_loss         | 5.76e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 94800     |\n",
      "|    time_elapsed       | 2264      |\n",
      "|    total_timesteps    | 474000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.8     |\n",
      "|    explained_variance | 0.106     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 94799     |\n",
      "|    policy_loss        | -6.97e+03 |\n",
      "|    reward             | 179.50536 |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 3.4e+04   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 94900     |\n",
      "|    time_elapsed       | 2266      |\n",
      "|    total_timesteps    | 474500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.8     |\n",
      "|    explained_variance | -0.0935   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 94899     |\n",
      "|    policy_loss        | 6.29e+03  |\n",
      "|    reward             | 23.736904 |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 2.22e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 95000      |\n",
      "|    time_elapsed       | 2268       |\n",
      "|    total_timesteps    | 475000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.8      |\n",
      "|    explained_variance | 0.389      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 94999      |\n",
      "|    policy_loss        | 1.74e+03   |\n",
      "|    reward             | -72.940094 |\n",
      "|    std                | 1.33       |\n",
      "|    value_loss         | 1.5e+03    |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 95100     |\n",
      "|    time_elapsed       | 2271      |\n",
      "|    total_timesteps    | 475500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.8     |\n",
      "|    explained_variance | 0.299     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 95099     |\n",
      "|    policy_loss        | -547      |\n",
      "|    reward             | 16.187141 |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 852       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 95200     |\n",
      "|    time_elapsed       | 2273      |\n",
      "|    total_timesteps    | 476000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.8     |\n",
      "|    explained_variance | 0.0895    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 95199     |\n",
      "|    policy_loss        | 2.97e+03  |\n",
      "|    reward             | -86.26785 |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 6.35e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 95300      |\n",
      "|    time_elapsed       | 2275       |\n",
      "|    total_timesteps    | 476500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.8      |\n",
      "|    explained_variance | 0.0385     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 95299      |\n",
      "|    policy_loss        | 263        |\n",
      "|    reward             | -19.574774 |\n",
      "|    std                | 1.34       |\n",
      "|    value_loss         | 6.87e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 95400      |\n",
      "|    time_elapsed       | 2278       |\n",
      "|    total_timesteps    | 477000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.9      |\n",
      "|    explained_variance | 0.0733     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 95399      |\n",
      "|    policy_loss        | 4.62e+03   |\n",
      "|    reward             | -94.069565 |\n",
      "|    std                | 1.34       |\n",
      "|    value_loss         | 2.01e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 95500     |\n",
      "|    time_elapsed       | 2280      |\n",
      "|    total_timesteps    | 477500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.8     |\n",
      "|    explained_variance | 0.263     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 95499     |\n",
      "|    policy_loss        | 2.03e+03  |\n",
      "|    reward             | -81.36868 |\n",
      "|    std                | 1.34      |\n",
      "|    value_loss         | 4.89e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 95600     |\n",
      "|    time_elapsed       | 2282      |\n",
      "|    total_timesteps    | 478000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.9     |\n",
      "|    explained_variance | -0.0111   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 95599     |\n",
      "|    policy_loss        | -1.28e+03 |\n",
      "|    reward             | 30.22548  |\n",
      "|    std                | 1.34      |\n",
      "|    value_loss         | 1.4e+03   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 95700     |\n",
      "|    time_elapsed       | 2285      |\n",
      "|    total_timesteps    | 478500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.9     |\n",
      "|    explained_variance | 0.327     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 95699     |\n",
      "|    policy_loss        | -2.54e+03 |\n",
      "|    reward             | 4.9084816 |\n",
      "|    std                | 1.34      |\n",
      "|    value_loss         | 3.7e+03   |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 95800      |\n",
      "|    time_elapsed       | 2287       |\n",
      "|    total_timesteps    | 479000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -51        |\n",
      "|    explained_variance | 0.303      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 95799      |\n",
      "|    policy_loss        | 135        |\n",
      "|    reward             | -48.018345 |\n",
      "|    std                | 1.34       |\n",
      "|    value_loss         | 1.45e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 95900     |\n",
      "|    time_elapsed       | 2289      |\n",
      "|    total_timesteps    | 479500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.9     |\n",
      "|    explained_variance | 0.258     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 95899     |\n",
      "|    policy_loss        | -4.19e+03 |\n",
      "|    reward             | 151.07224 |\n",
      "|    std                | 1.34      |\n",
      "|    value_loss         | 1.32e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 96000     |\n",
      "|    time_elapsed       | 2292      |\n",
      "|    total_timesteps    | 480000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.9     |\n",
      "|    explained_variance | 0.14      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 95999     |\n",
      "|    policy_loss        | 953       |\n",
      "|    reward             | 310.51376 |\n",
      "|    std                | 1.34      |\n",
      "|    value_loss         | 1.77e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 96100     |\n",
      "|    time_elapsed       | 2294      |\n",
      "|    total_timesteps    | 480500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.9     |\n",
      "|    explained_variance | -0.00511  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 96099     |\n",
      "|    policy_loss        | 9.7e+03   |\n",
      "|    reward             | 215.46179 |\n",
      "|    std                | 1.34      |\n",
      "|    value_loss         | 5.11e+04  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 209      |\n",
      "|    iterations         | 96200    |\n",
      "|    time_elapsed       | 2296     |\n",
      "|    total_timesteps    | 481000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -50.9    |\n",
      "|    explained_variance | 0.347    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96199    |\n",
      "|    policy_loss        | 1.11e+03 |\n",
      "|    reward             | 5.317072 |\n",
      "|    std                | 1.34     |\n",
      "|    value_loss         | 516      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 96300     |\n",
      "|    time_elapsed       | 2299      |\n",
      "|    total_timesteps    | 481500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.9     |\n",
      "|    explained_variance | 0.162     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 96299     |\n",
      "|    policy_loss        | -1.69e+03 |\n",
      "|    reward             | 44.146904 |\n",
      "|    std                | 1.34      |\n",
      "|    value_loss         | 2.41e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 96400      |\n",
      "|    time_elapsed       | 2301       |\n",
      "|    total_timesteps    | 482000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -51        |\n",
      "|    explained_variance | -0.528     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 96399      |\n",
      "|    policy_loss        | 1.05e+03   |\n",
      "|    reward             | -32.240696 |\n",
      "|    std                | 1.34       |\n",
      "|    value_loss         | 1.58e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 96500      |\n",
      "|    time_elapsed       | 2303       |\n",
      "|    total_timesteps    | 482500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -51        |\n",
      "|    explained_variance | 0.24       |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 96499      |\n",
      "|    policy_loss        | -5.17e+03  |\n",
      "|    reward             | -122.37332 |\n",
      "|    std                | 1.34       |\n",
      "|    value_loss         | 1.54e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 96600     |\n",
      "|    time_elapsed       | 2306      |\n",
      "|    total_timesteps    | 483000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -51       |\n",
      "|    explained_variance | 0.174     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 96599     |\n",
      "|    policy_loss        | 3.1e+03   |\n",
      "|    reward             | 279.28198 |\n",
      "|    std                | 1.34      |\n",
      "|    value_loss         | 1.58e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 96700      |\n",
      "|    time_elapsed       | 2308       |\n",
      "|    total_timesteps    | 483500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -51        |\n",
      "|    explained_variance | 0.126      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 96699      |\n",
      "|    policy_loss        | -7.19e+03  |\n",
      "|    reward             | -129.17384 |\n",
      "|    std                | 1.34       |\n",
      "|    value_loss         | 4.47e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 96800      |\n",
      "|    time_elapsed       | 2310       |\n",
      "|    total_timesteps    | 484000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -51        |\n",
      "|    explained_variance | 0.0557     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 96799      |\n",
      "|    policy_loss        | -9.96e+03  |\n",
      "|    reward             | -355.55542 |\n",
      "|    std                | 1.34       |\n",
      "|    value_loss         | 5.5e+04    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 96900      |\n",
      "|    time_elapsed       | 2313       |\n",
      "|    total_timesteps    | 484500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -51        |\n",
      "|    explained_variance | -0.0445    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 96899      |\n",
      "|    policy_loss        | 2.38e+03   |\n",
      "|    reward             | -57.976864 |\n",
      "|    std                | 1.34       |\n",
      "|    value_loss         | 2.77e+03   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 209      |\n",
      "|    iterations         | 97000    |\n",
      "|    time_elapsed       | 2315     |\n",
      "|    total_timesteps    | 485000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -50.9    |\n",
      "|    explained_variance | -0.274   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96999    |\n",
      "|    policy_loss        | 4.67e+03 |\n",
      "|    reward             | 74.90084 |\n",
      "|    std                | 1.34     |\n",
      "|    value_loss         | 1.15e+04 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 97100     |\n",
      "|    time_elapsed       | 2318      |\n",
      "|    total_timesteps    | 485500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.9     |\n",
      "|    explained_variance | 0.316     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 97099     |\n",
      "|    policy_loss        | -2.71e+03 |\n",
      "|    reward             | 151.25008 |\n",
      "|    std                | 1.34      |\n",
      "|    value_loss         | 4.4e+03   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 97200     |\n",
      "|    time_elapsed       | 2320      |\n",
      "|    total_timesteps    | 486000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.9     |\n",
      "|    explained_variance | 0.261     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 97199     |\n",
      "|    policy_loss        | 8.12e+03  |\n",
      "|    reward             | 99.231544 |\n",
      "|    std                | 1.34      |\n",
      "|    value_loss         | 2.61e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 97300     |\n",
      "|    time_elapsed       | 2322      |\n",
      "|    total_timesteps    | 486500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.8     |\n",
      "|    explained_variance | 0.0783    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 97299     |\n",
      "|    policy_loss        | -1.16e+04 |\n",
      "|    reward             | -33.99654 |\n",
      "|    std                | 1.34      |\n",
      "|    value_loss         | 5.23e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 97400      |\n",
      "|    time_elapsed       | 2325       |\n",
      "|    total_timesteps    | 487000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.9      |\n",
      "|    explained_variance | 0.0781     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 97399      |\n",
      "|    policy_loss        | -6.37e+03  |\n",
      "|    reward             | -210.40453 |\n",
      "|    std                | 1.34       |\n",
      "|    value_loss         | 3.65e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 97500     |\n",
      "|    time_elapsed       | 2327      |\n",
      "|    total_timesteps    | 487500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.9     |\n",
      "|    explained_variance | -0.133    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 97499     |\n",
      "|    policy_loss        | -8.28     |\n",
      "|    reward             | 2.7676284 |\n",
      "|    std                | 1.34      |\n",
      "|    value_loss         | 1.28e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 97600     |\n",
      "|    time_elapsed       | 2329      |\n",
      "|    total_timesteps    | 488000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.8     |\n",
      "|    explained_variance | -2.98     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 97599     |\n",
      "|    policy_loss        | 993       |\n",
      "|    reward             | -90.77629 |\n",
      "|    std                | 1.34      |\n",
      "|    value_loss         | 1.03e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 97700      |\n",
      "|    time_elapsed       | 2332       |\n",
      "|    total_timesteps    | 488500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.9      |\n",
      "|    explained_variance | -0.146     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 97699      |\n",
      "|    policy_loss        | -4.27e+03  |\n",
      "|    reward             | -62.087685 |\n",
      "|    std                | 1.34       |\n",
      "|    value_loss         | 1.05e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 97800     |\n",
      "|    time_elapsed       | 2334      |\n",
      "|    total_timesteps    | 489000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.9     |\n",
      "|    explained_variance | 0.161     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 97799     |\n",
      "|    policy_loss        | -4.97e+03 |\n",
      "|    reward             | 223.71336 |\n",
      "|    std                | 1.34      |\n",
      "|    value_loss         | 1.09e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 97900     |\n",
      "|    time_elapsed       | 2336      |\n",
      "|    total_timesteps    | 489500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.9     |\n",
      "|    explained_variance | 0.0193    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 97899     |\n",
      "|    policy_loss        | -586      |\n",
      "|    reward             | 133.87776 |\n",
      "|    std                | 1.34      |\n",
      "|    value_loss         | 2.89e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 98000     |\n",
      "|    time_elapsed       | 2339      |\n",
      "|    total_timesteps    | 490000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.9     |\n",
      "|    explained_variance | 0.0937    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 97999     |\n",
      "|    policy_loss        | -7.23e+03 |\n",
      "|    reward             | 314.44254 |\n",
      "|    std                | 1.34      |\n",
      "|    value_loss         | 3.81e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 98100      |\n",
      "|    time_elapsed       | 2341       |\n",
      "|    total_timesteps    | 490500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.9      |\n",
      "|    explained_variance | 0.0921     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 98099      |\n",
      "|    policy_loss        | 314        |\n",
      "|    reward             | -16.652637 |\n",
      "|    std                | 1.34       |\n",
      "|    value_loss         | 3.43e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 98200     |\n",
      "|    time_elapsed       | 2344      |\n",
      "|    total_timesteps    | 491000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.9     |\n",
      "|    explained_variance | -0.271    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 98199     |\n",
      "|    policy_loss        | 1.9e+03   |\n",
      "|    reward             | -72.43723 |\n",
      "|    std                | 1.34      |\n",
      "|    value_loss         | 2.74e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 98300     |\n",
      "|    time_elapsed       | 2346      |\n",
      "|    total_timesteps    | 491500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.9     |\n",
      "|    explained_variance | -0.113    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 98299     |\n",
      "|    policy_loss        | 3.72e+03  |\n",
      "|    reward             | 132.73367 |\n",
      "|    std                | 1.34      |\n",
      "|    value_loss         | 5.88e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 209      |\n",
      "|    iterations         | 98400    |\n",
      "|    time_elapsed       | 2348     |\n",
      "|    total_timesteps    | 492000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -50.9    |\n",
      "|    explained_variance | 0.176    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98399    |\n",
      "|    policy_loss        | 655      |\n",
      "|    reward             | 70.54486 |\n",
      "|    std                | 1.34     |\n",
      "|    value_loss         | 9.45e+03 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 98500      |\n",
      "|    time_elapsed       | 2351       |\n",
      "|    total_timesteps    | 492500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.9      |\n",
      "|    explained_variance | -0.0445    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 98499      |\n",
      "|    policy_loss        | 3.56e+03   |\n",
      "|    reward             | -91.080864 |\n",
      "|    std                | 1.34       |\n",
      "|    value_loss         | 7.85e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 98600     |\n",
      "|    time_elapsed       | 2353      |\n",
      "|    total_timesteps    | 493000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.9     |\n",
      "|    explained_variance | -0.0106   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 98599     |\n",
      "|    policy_loss        | -1.38e+04 |\n",
      "|    reward             | -44.62618 |\n",
      "|    std                | 1.34      |\n",
      "|    value_loss         | 7.59e+04  |\n",
      "-------------------------------------\n",
      "day: 3102, episode: 160\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 10774571.82\n",
      "total_reward: 9774571.82\n",
      "total_cost: 52904.64\n",
      "total_trades: 48096\n",
      "Sharpe: 2.620\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 98700     |\n",
      "|    time_elapsed       | 2355      |\n",
      "|    total_timesteps    | 493500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -51       |\n",
      "|    explained_variance | -0.484    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 98699     |\n",
      "|    policy_loss        | -1.03e+03 |\n",
      "|    reward             | 30.76152  |\n",
      "|    std                | 1.34      |\n",
      "|    value_loss         | 1.06e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 98800      |\n",
      "|    time_elapsed       | 2358       |\n",
      "|    total_timesteps    | 494000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.9      |\n",
      "|    explained_variance | 0.22       |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 98799      |\n",
      "|    policy_loss        | 5.99e+03   |\n",
      "|    reward             | -28.839468 |\n",
      "|    std                | 1.34       |\n",
      "|    value_loss         | 1.41e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 98900     |\n",
      "|    time_elapsed       | 2360      |\n",
      "|    total_timesteps    | 494500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -50.9     |\n",
      "|    explained_variance | 0.405     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 98899     |\n",
      "|    policy_loss        | -210      |\n",
      "|    reward             | 14.112892 |\n",
      "|    std                | 1.34      |\n",
      "|    value_loss         | 283       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 209         |\n",
      "|    iterations         | 99000       |\n",
      "|    time_elapsed       | 2362        |\n",
      "|    total_timesteps    | 495000      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -51         |\n",
      "|    explained_variance | 0.128       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 98999       |\n",
      "|    policy_loss        | -6.85e+03   |\n",
      "|    reward             | -107.540344 |\n",
      "|    std                | 1.34        |\n",
      "|    value_loss         | 2.6e+04     |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 99100     |\n",
      "|    time_elapsed       | 2365      |\n",
      "|    total_timesteps    | 495500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -51       |\n",
      "|    explained_variance | 0.183     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99099     |\n",
      "|    policy_loss        | 156       |\n",
      "|    reward             | -66.81347 |\n",
      "|    std                | 1.34      |\n",
      "|    value_loss         | 3.1e+03   |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 99200      |\n",
      "|    time_elapsed       | 2367       |\n",
      "|    total_timesteps    | 496000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -51        |\n",
      "|    explained_variance | 0.103      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99199      |\n",
      "|    policy_loss        | -2.93e+03  |\n",
      "|    reward             | -221.79678 |\n",
      "|    std                | 1.34       |\n",
      "|    value_loss         | 3e+04      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 99300      |\n",
      "|    time_elapsed       | 2369       |\n",
      "|    total_timesteps    | 496500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -51        |\n",
      "|    explained_variance | 0.427      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99299      |\n",
      "|    policy_loss        | 1.75e+03   |\n",
      "|    reward             | -6.7605066 |\n",
      "|    std                | 1.35       |\n",
      "|    value_loss         | 1.2e+03    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 99400      |\n",
      "|    time_elapsed       | 2372       |\n",
      "|    total_timesteps    | 497000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -51        |\n",
      "|    explained_variance | 0.155      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99399      |\n",
      "|    policy_loss        | -1.47e+03  |\n",
      "|    reward             | -70.518616 |\n",
      "|    std                | 1.34       |\n",
      "|    value_loss         | 1.45e+03   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 209      |\n",
      "|    iterations         | 99500    |\n",
      "|    time_elapsed       | 2374     |\n",
      "|    total_timesteps    | 497500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -51      |\n",
      "|    explained_variance | 0.641    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99499    |\n",
      "|    policy_loss        | 639      |\n",
      "|    reward             | 65.6776  |\n",
      "|    std                | 1.34     |\n",
      "|    value_loss         | 290      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 99600     |\n",
      "|    time_elapsed       | 2376      |\n",
      "|    total_timesteps    | 498000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -51       |\n",
      "|    explained_variance | 0.119     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99599     |\n",
      "|    policy_loss        | 5.3e+03   |\n",
      "|    reward             | 43.498363 |\n",
      "|    std                | 1.34      |\n",
      "|    value_loss         | 1.41e+04  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 209      |\n",
      "|    iterations         | 99700    |\n",
      "|    time_elapsed       | 2379     |\n",
      "|    total_timesteps    | 498500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -51.1    |\n",
      "|    explained_variance | 0.411    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99699    |\n",
      "|    policy_loss        | -587     |\n",
      "|    reward             | 92.57175 |\n",
      "|    std                | 1.35     |\n",
      "|    value_loss         | 3.14e+03 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 99800      |\n",
      "|    time_elapsed       | 2381       |\n",
      "|    total_timesteps    | 499000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -51        |\n",
      "|    explained_variance | 0.121      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99799      |\n",
      "|    policy_loss        | -1.17e+03  |\n",
      "|    reward             | -19.672823 |\n",
      "|    std                | 1.35       |\n",
      "|    value_loss         | 7.29e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 209       |\n",
      "|    iterations         | 99900     |\n",
      "|    time_elapsed       | 2383      |\n",
      "|    total_timesteps    | 499500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -51       |\n",
      "|    explained_variance | 0.111     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99899     |\n",
      "|    policy_loss        | 7.78e+03  |\n",
      "|    reward             | 193.26938 |\n",
      "|    std                | 1.35      |\n",
      "|    value_loss         | 4.84e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 209        |\n",
      "|    iterations         | 100000     |\n",
      "|    time_elapsed       | 2386       |\n",
      "|    total_timesteps    | 500000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -51        |\n",
      "|    explained_variance | 0.044      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99999      |\n",
      "|    policy_loss        | -557       |\n",
      "|    reward             | -0.5226972 |\n",
      "|    std                | 1.34       |\n",
      "|    value_loss         | 1.48e+03   |\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_a2c = agent.train_model(model=model_a2c, \n",
    "                             tb_log_name='a2c',\n",
    "                             total_timesteps=500000) if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "zjCWfgsg3sVa"
   },
   "outputs": [],
   "source": [
    "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRiOtrywfAo1"
   },
   "source": [
    "### Agent 2: DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "M2YadjfnLwgt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
      "Using cpu device\n",
      "Logging to results/ddpg\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_ddpg = agent.get_model(\"ddpg\")\n",
    "\n",
    "if if_using_ddpg:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ddpg'\n",
    "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ddpg.set_logger(new_logger_ddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tCDa78rqfO_a"
   },
   "outputs": [],
   "source": [
    "trained_ddpg = agent.train_model(model=model_ddpg, \n",
    "                             tb_log_name='ddpg',\n",
    "                             total_timesteps=50) if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ne6M2R-WvrUQ"
   },
   "outputs": [],
   "source": [
    "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gDkU-j-fCmZ"
   },
   "source": [
    "### Agent 3: PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "y5D5PFUhMzSV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to results/ppo\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "\n",
    "if if_using_ppo:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ppo'\n",
    "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ppo.set_logger(new_logger_ppo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gt8eIQKYM4G3",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "trained_ppo = agent.train_model(model=model_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=200000) if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "C6AidlWyvwzm"
   },
   "outputs": [],
   "source": [
    "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Zpv4S0-fDBv"
   },
   "source": [
    "### Agent 4: TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "JSAHhV4Xc-bh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
      "Using cpu device\n",
      "Logging to results/td3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lolo/Desktop/stock/lib/python3.10/site-packages/stable_baselines3/common/buffers.py:229: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 4.46GB > 3.08GB\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "TD3_PARAMS = {\"batch_size\": 100, \n",
    "              \"buffer_size\": 1000000, \n",
    "              \"learning_rate\": 0.001}\n",
    "\n",
    "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
    "\n",
    "if if_using_td3:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/td3'\n",
    "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_td3.set_logger(new_logger_td3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "OSRxNYAxdKpU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 114       |\n",
      "|    time_elapsed    | 108       |\n",
      "|    total_timesteps | 12412     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 3.08e+04  |\n",
      "|    critic_loss     | 5.72e+06  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 9309      |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "day: 3102, episode: 170\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1019754.19\n",
      "total_reward: 19754.19\n",
      "total_cost: 1150.29\n",
      "total_trades: 58900\n",
      "Sharpe: 1.186\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 112       |\n",
      "|    time_elapsed    | 221       |\n",
      "|    total_timesteps | 24824     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.24e+04  |\n",
      "|    critic_loss     | 1.47e+06  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 21721     |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 112       |\n",
      "|    time_elapsed    | 330       |\n",
      "|    total_timesteps | 37236     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.7e+04   |\n",
      "|    critic_loss     | 5.47e+05  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 34133     |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 112       |\n",
      "|    time_elapsed    | 440       |\n",
      "|    total_timesteps | 49648     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.3e+04   |\n",
      "|    critic_loss     | 2.92e+05  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 46545     |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "day: 3102, episode: 180\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1019754.19\n",
      "total_reward: 19754.19\n",
      "total_cost: 1150.29\n",
      "total_trades: 58900\n",
      "Sharpe: 1.186\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 112       |\n",
      "|    time_elapsed    | 549       |\n",
      "|    total_timesteps | 62060     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1e+04     |\n",
      "|    critic_loss     | 1.65e+05  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 58957     |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 24        |\n",
      "|    fps             | 112       |\n",
      "|    time_elapsed    | 659       |\n",
      "|    total_timesteps | 74472     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 7.77e+03  |\n",
      "|    critic_loss     | 1.07e+05  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 71369     |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "day: 3102, episode: 190\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1019754.19\n",
      "total_reward: 19754.19\n",
      "total_cost: 1150.29\n",
      "total_trades: 58900\n",
      "Sharpe: 1.186\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 28        |\n",
      "|    fps             | 112       |\n",
      "|    time_elapsed    | 769       |\n",
      "|    total_timesteps | 86884     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 6.06e+03  |\n",
      "|    critic_loss     | 5.8e+04   |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 83781     |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 32        |\n",
      "|    fps             | 112       |\n",
      "|    time_elapsed    | 878       |\n",
      "|    total_timesteps | 99296     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 4.72e+03  |\n",
      "|    critic_loss     | 3.99e+04  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 96193     |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 36        |\n",
      "|    fps             | 113       |\n",
      "|    time_elapsed    | 987       |\n",
      "|    total_timesteps | 111708    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 3.74e+03  |\n",
      "|    critic_loss     | 2.36e+04  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 108605    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "day: 3102, episode: 200\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1019754.19\n",
      "total_reward: 19754.19\n",
      "total_cost: 1150.29\n",
      "total_trades: 58900\n",
      "Sharpe: 1.186\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 40        |\n",
      "|    fps             | 113       |\n",
      "|    time_elapsed    | 1097      |\n",
      "|    total_timesteps | 124120    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 3.05e+03  |\n",
      "|    critic_loss     | 2.02e+04  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 121017    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 44        |\n",
      "|    fps             | 110       |\n",
      "|    time_elapsed    | 1238      |\n",
      "|    total_timesteps | 136532    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.42e+03  |\n",
      "|    critic_loss     | 1.38e+04  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 133429    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "day: 3102, episode: 210\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1019754.19\n",
      "total_reward: 19754.19\n",
      "total_cost: 1150.29\n",
      "total_trades: 58900\n",
      "Sharpe: 1.186\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 48        |\n",
      "|    fps             | 110       |\n",
      "|    time_elapsed    | 1347      |\n",
      "|    total_timesteps | 148944    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.93e+03  |\n",
      "|    critic_loss     | 8.17e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 145841    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 52        |\n",
      "|    fps             | 110       |\n",
      "|    time_elapsed    | 1456      |\n",
      "|    total_timesteps | 161356    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.59e+03  |\n",
      "|    critic_loss     | 6.95e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 158253    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 56        |\n",
      "|    fps             | 110       |\n",
      "|    time_elapsed    | 1566      |\n",
      "|    total_timesteps | 173768    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.29e+03  |\n",
      "|    critic_loss     | 4.94e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 170665    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "day: 3102, episode: 220\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1019754.19\n",
      "total_reward: 19754.19\n",
      "total_cost: 1150.29\n",
      "total_trades: 58900\n",
      "Sharpe: 1.186\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 60        |\n",
      "|    fps             | 68        |\n",
      "|    time_elapsed    | 2706      |\n",
      "|    total_timesteps | 186180    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.07e+03  |\n",
      "|    critic_loss     | 4.36e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 183077    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 64        |\n",
      "|    fps             | 70        |\n",
      "|    time_elapsed    | 2817      |\n",
      "|    total_timesteps | 198592    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 896       |\n",
      "|    critic_loss     | 3.72e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 195489    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "day: 3102, episode: 230\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1019754.19\n",
      "total_reward: 19754.19\n",
      "total_cost: 1150.29\n",
      "total_trades: 58900\n",
      "Sharpe: 1.186\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 68        |\n",
      "|    fps             | 72        |\n",
      "|    time_elapsed    | 2928      |\n",
      "|    total_timesteps | 211004    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 771       |\n",
      "|    critic_loss     | 3.92e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 207901    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 72        |\n",
      "|    fps             | 73        |\n",
      "|    time_elapsed    | 3039      |\n",
      "|    total_timesteps | 223416    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 685       |\n",
      "|    critic_loss     | 3.41e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 220313    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 76        |\n",
      "|    fps             | 74        |\n",
      "|    time_elapsed    | 3148      |\n",
      "|    total_timesteps | 235828    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 634       |\n",
      "|    critic_loss     | 2.71e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 232725    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "day: 3102, episode: 240\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1019754.19\n",
      "total_reward: 19754.19\n",
      "total_cost: 1150.29\n",
      "total_trades: 58900\n",
      "Sharpe: 1.186\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 80        |\n",
      "|    fps             | 76        |\n",
      "|    time_elapsed    | 3258      |\n",
      "|    total_timesteps | 248240    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 629       |\n",
      "|    critic_loss     | 2.64e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 245137    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 84        |\n",
      "|    fps             | 53        |\n",
      "|    time_elapsed    | 4862      |\n",
      "|    total_timesteps | 260652    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 571       |\n",
      "|    critic_loss     | 2.32e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 257549    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "day: 3102, episode: 250\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1019754.19\n",
      "total_reward: 19754.19\n",
      "total_cost: 1150.29\n",
      "total_trades: 58900\n",
      "Sharpe: 1.186\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 88        |\n",
      "|    fps             | 51        |\n",
      "|    time_elapsed    | 5335      |\n",
      "|    total_timesteps | 273064    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 560       |\n",
      "|    critic_loss     | 2.27e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 269961    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 92        |\n",
      "|    fps             | 51        |\n",
      "|    time_elapsed    | 5528      |\n",
      "|    total_timesteps | 285476    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 559       |\n",
      "|    critic_loss     | 2.21e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 282373    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 96        |\n",
      "|    fps             | 52        |\n",
      "|    time_elapsed    | 5639      |\n",
      "|    total_timesteps | 297888    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 584       |\n",
      "|    critic_loss     | 2.68e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 294785    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "day: 3102, episode: 260\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1019754.19\n",
      "total_reward: 19754.19\n",
      "total_cost: 1150.29\n",
      "total_trades: 58900\n",
      "Sharpe: 1.186\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 100       |\n",
      "|    fps             | 53        |\n",
      "|    time_elapsed    | 5750      |\n",
      "|    total_timesteps | 310300    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 595       |\n",
      "|    critic_loss     | 2.87e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 307197    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 104       |\n",
      "|    fps             | 55        |\n",
      "|    time_elapsed    | 5859      |\n",
      "|    total_timesteps | 322712    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 665       |\n",
      "|    critic_loss     | 3.87e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 319609    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "day: 3102, episode: 270\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1019754.19\n",
      "total_reward: 19754.19\n",
      "total_cost: 1150.29\n",
      "total_trades: 58900\n",
      "Sharpe: 1.186\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 108       |\n",
      "|    fps             | 48        |\n",
      "|    time_elapsed    | 6879      |\n",
      "|    total_timesteps | 335124    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 701       |\n",
      "|    critic_loss     | 3.29e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 332021    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 112       |\n",
      "|    fps             | 35        |\n",
      "|    time_elapsed    | 9857      |\n",
      "|    total_timesteps | 347536    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 676       |\n",
      "|    critic_loss     | 4.36e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 344433    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 116       |\n",
      "|    fps             | 31        |\n",
      "|    time_elapsed    | 11450     |\n",
      "|    total_timesteps | 359948    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 970       |\n",
      "|    critic_loss     | 9.38e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 356845    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "day: 3102, episode: 280\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1019754.19\n",
      "total_reward: 19754.19\n",
      "total_cost: 1150.29\n",
      "total_trades: 58900\n",
      "Sharpe: 1.186\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 120       |\n",
      "|    fps             | 32        |\n",
      "|    time_elapsed    | 11559     |\n",
      "|    total_timesteps | 372360    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 804       |\n",
      "|    critic_loss     | 3.46e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 369257    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 124       |\n",
      "|    fps             | 30        |\n",
      "|    time_elapsed    | 12608     |\n",
      "|    total_timesteps | 384772    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 880       |\n",
      "|    critic_loss     | 7.33e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 381669    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "day: 3102, episode: 290\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1019754.19\n",
      "total_reward: 19754.19\n",
      "total_cost: 1150.29\n",
      "total_trades: 58900\n",
      "Sharpe: 1.186\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 128       |\n",
      "|    fps             | 26        |\n",
      "|    time_elapsed    | 15138     |\n",
      "|    total_timesteps | 397184    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.14e+03  |\n",
      "|    critic_loss     | 2e+04     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 394081    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 132       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 17241     |\n",
      "|    total_timesteps | 409596    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.34e+03  |\n",
      "|    critic_loss     | 2.98e+04  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 406493    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 136       |\n",
      "|    fps             | 21        |\n",
      "|    time_elapsed    | 19344     |\n",
      "|    total_timesteps | 422008    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.2e+03   |\n",
      "|    critic_loss     | 2.56e+04  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 418905    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "day: 3102, episode: 300\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1019754.19\n",
      "total_reward: 19754.19\n",
      "total_cost: 1150.29\n",
      "total_trades: 58900\n",
      "Sharpe: 1.186\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 140       |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 19452     |\n",
      "|    total_timesteps | 434420    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.05e+03  |\n",
      "|    critic_loss     | 1.74e+04  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 431317    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 144       |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 19560     |\n",
      "|    total_timesteps | 446832    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 863       |\n",
      "|    critic_loss     | 1.26e+04  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 443729    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "day: 3102, episode: 310\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1019754.19\n",
      "total_reward: 19754.19\n",
      "total_cost: 1150.29\n",
      "total_trades: 58900\n",
      "Sharpe: 1.186\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 148       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 19669     |\n",
      "|    total_timesteps | 459244    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 813       |\n",
      "|    critic_loss     | 1.09e+04  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 456141    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 152       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 19905     |\n",
      "|    total_timesteps | 471656    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 859       |\n",
      "|    critic_loss     | 1.52e+04  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 468553    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 156       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 20014     |\n",
      "|    total_timesteps | 484068    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 686       |\n",
      "|    critic_loss     | 9.96e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 480965    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n",
      "day: 3102, episode: 320\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1019754.19\n",
      "total_reward: 19754.19\n",
      "total_cost: 1150.29\n",
      "total_trades: 58900\n",
      "Sharpe: 1.186\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 160       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 20123     |\n",
      "|    total_timesteps | 496480    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 589       |\n",
      "|    critic_loss     | 7.44e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 493377    |\n",
      "|    reward          | 18.409077 |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_td3 = agent.train_model(model=model_td3, \n",
    "                             tb_log_name='td3',\n",
    "                             total_timesteps=500000) if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "OkJV6V_mv2hw"
   },
   "outputs": [],
   "source": [
    "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dr49PotrfG01"
   },
   "source": [
    "### Agent 5: SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwOhVjqRkCdM"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "SAC_PARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"learning_starts\": 100,\n",
    "    \"ent_coef\": \"auto_0.1\",\n",
    "}\n",
    "\n",
    "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
    "\n",
    "if if_using_sac:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/sac'\n",
    "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_sac.set_logger(new_logger_sac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K8RSdKCckJyH"
   },
   "outputs": [],
   "source": [
    "trained_sac = agent.train_model(model=model_sac, \n",
    "                             tb_log_name='sac',\n",
    "                             total_timesteps=70000) if if_using_sac else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_SpZoQgPv7GO"
   },
   "outputs": [],
   "source": [
    "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgGm3dQZfRks"
   },
   "source": [
    "## Save the trained agent\n",
    "Trained agents should have already been saved in the \"trained_models\" drectory after you run the code blocks above.\n",
    "\n",
    "For Colab users, the zip files should be at \"./trained_models\" or \"/content/trained_models\".\n",
    "\n",
    "For users running on your local environment, the zip files should be at \"./trained_models\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MRiOtrywfAo1",
    "_gDkU-j-fCmZ",
    "3Zpv4S0-fDBv",
    "Dr49PotrfG01"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
