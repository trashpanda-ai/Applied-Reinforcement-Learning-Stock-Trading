@inproceedings{kour2014real,
  title={Real-time segmentation of on-line handwritten arabic script},
  author={Kour, George and Saabne, Raid},
  booktitle={Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference on},
  pages={417--422},
  year={2014},
  organization={IEEE}
}

@inproceedings{kour2014fast,
  title={Fast classification of handwritten on-line Arabic characters},
  author={Kour, George and Saabne, Raid},
  booktitle={Soft Computing and Pattern Recognition (SoCPaR), 2014 6th International Conference of},
  pages={312--318},
  year={2014},
  organization={IEEE},
  doi={10.1109/SOCPAR.2014.7008025}
}

@article{hadash2018estimate,
  title={Estimate and Replace: A Novel Approach to Integrating Deep Neural Networks with Existing Applications},
  author={Hadash, Guy and Kermany, Einat and Carmeli, Boaz and Lavi, Ofer and Kour, George and Jacovi, Alon},
  journal={arXiv preprint arXiv:1804.09028},
  year={2018}
}

@article{liu2020finrl,
  title={FinRL: A deep reinforcement learning library for automated stock trading in quantitative finance},
  author={Liu, Xiao-Yang and Yang, Hongyang and Chen, Qian and Zhang, Runjia and Yang, Liuqing and Xiao, Bowen and Wang, Christina Dan},
  journal={arXiv preprint arXiv:2011.09607},
  year={2020}
}

@article{liu2022finrlMeta,
  title={FinRL-Meta: Market environments and benchmarks for data-driven financial reinforcement learning},
  author={Liu, Xiao-Yang and Xia, Ziyi and Rui, Jingyang and Gao, Jiechao and Yang, Hongyang and Zhu, Ming and Wang, Christina and Wang, Zhaoran and Guo, Jian},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={1835--1849},
  year={2022}
}
% Turbulence index
@article{kritzman2010skulls,
  title={Skulls, financial turbulence, and risk management},
  author={Kritzman, Mark and Li, Yuanzhen},
  journal={Financial Analysts Journal},
  volume={66},
  number={5},
  pages={30--41},
  year={2010},
  publisher={Taylor \& Francis}
}

% volume
@article{gallant1992stock,
  title={Stock prices and volume},
  author={Gallant, A Ronald and Rossi, Peter E and Tauchen, George},
  journal={The Review of Financial Studies},
  volume={5},
  number={2},
  pages={199--242},
  year={1992},
  publisher={Oxford University Press}
}

% volume
@article{campbell1993trading,
  title={Trading volume and serial correlation in stock returns},
  author={Campbell, John Y and Grossman, Sanford J and Wang, Jiang},
  journal={The Quarterly Journal of Economics},
  volume={108},
  number={4},
  pages={905--939},
  year={1993},
  publisher={MIT Press}
}

% Introduction Psychologe
@article{statman2002lottery,
  title={Lottery players/stock traders},
  author={Statman, Meir},
  journal={Financial Analysts Journal},
  volume={58},
  number={1},
  pages={14--21},
  year={2002},
  publisher={Taylor \& Francis}
}

@article{Sobol_sensitivity,
author = {Sobol′, I.M},
year = {2001},
month = {02},
pages = {271-280},
title = {Global sensitivity indices for nonlinear mathematical models and their Monte Carlo estimates},
volume = {55},
journal = {Mathematics and Computers in Simulation},
doi = {10.1016/S0378-4754(00)00270-6}
}

@article{Variance_more1,
author = {Tarantola, Stefano and Gatelli, Debora and Mara, Thierry},
year = {2006},
month = {06},
pages = {717-727},
title = {Random balance designs for the estimation of first order global sensitivity indices},
volume = {91},
journal = {Reliability Engineering & System Safety},
doi = {10.1016/j.ress.2005.06.003}
}

@article{Variance_more2,
author = {Saltelli, Andrea and Tarantola, Stefano and Chan, K.},
year = {2012},
month = {03},
pages = {},
title = {A Quantitative Model-Independent Method for Global Sensitivity Analysis of Model Output},
volume = {41},
journal = {Technometrics},
doi = {10.1080/00401706.1999.10485594}
}

@ARTICLE{GlobalSensitivityOverview,
  author={Stein, Bas Van and Raponi, Elena and Sadeghi, Zahra and Bouman, Niek and Van Ham, Roeland C. H. J. and Bäck, Thomas},
  journal={IEEE Access}, 
  title={A Comparison of Global Sensitivity Analysis Methods for Explainable AI With an Application in Genomic Prediction}, 
  year={2022},
  volume={10},
  number={},
  pages={103364-103381},
  doi={10.1109/ACCESS.2022.3210175}}

@InProceedings{PERTURBATION1,
author="Zeiler, Matthew D.
and Fergus, Rob",
editor="Fleet, David
and Pajdla, Tomas
and Schiele, Bernt
and Tuytelaars, Tinne",
title="Visualizing and Understanding Convolutional Networks",
booktitle="Computer Vision -- ECCV 2014",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="818--833",
abstract="Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.",
isbn="978-3-319-10590-1"
}

@article{PERTURBATION2,
  author       = {Luisa M. Zintgraf and
                  Taco S. Cohen and
                  Tameem Adel and
                  Max Welling},
  title        = {Visualizing Deep Neural Network Decisions: Prediction Difference Analysis},
  journal      = {CoRR},
  volume       = {abs/1702.04595},
  year         = {2017},
  url          = {http://arxiv.org/abs/1702.04595},
  eprinttype    = {arXiv},
  eprint       = {1702.04595},
  timestamp    = {Mon, 13 Aug 2018 16:47:17 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/ZintgrafCAW17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{PERTURBATION,
    title={Towards better understanding of gradient-based attribution methods for Deep Neural Networks},
    author={Marco Ancona and Enea Ceolini and Cengiz Öztireli and Markus Gross},
    year={2017},
    eprint={1711.06104},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{gradients_tutorial,
  author       = {Ian E. Nielsen and
                  Dimah Dera and
                  Ghulam Rasool and
                  Nidhal Bouaynaya and
                  Ravi Prakash Ramachandran},
  title        = {Robust Explainability: {A} Tutorial on Gradient-Based Attribution
                  Methods for Deep Neural Networks},
  journal      = {CoRR},
  volume       = {abs/2107.11400},
  year         = {2021},
  url          = {https://arxiv.org/abs/2107.11400},
  eprinttype    = {arXiv},
  eprint       = {2107.11400},
  timestamp    = {Thu, 29 Jul 2021 16:14:15 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2107-11400.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{gradients_first,
  author       = {Avanti Shrikumar and
                  Peyton Greenside and
                  Anna Shcherbina and
                  Anshul Kundaje},
  title        = {Not Just a Black Box: Learning Important Features Through Propagating
                  Activation Differences},
  journal      = {CoRR},
  volume       = {abs/1605.01713},
  year         = {2016},
  url          = {http://arxiv.org/abs/1605.01713},
  eprinttype    = {arXiv},
  eprint       = {1605.01713},
  timestamp    = {Mon, 13 Aug 2018 16:48:14 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/ShrikumarGSK16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{gradients_IntergratedGradients,
  title = 	 {Axiomatic Attribution for Deep Networks},
  author =       {Mukund Sundararajan and Ankur Taly and Qiqi Yan},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {3319--3328},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/sundararajan17a.html},
  abstract = 	 {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms—Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.}
}

@article{gradients_Taylor,
title = {Explaining nonlinear classification decisions with deep Taylor decomposition},
journal = {Pattern Recognition},
volume = {65},
pages = {211-222},
year = {2017},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2016.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0031320316303582},
author = {Grégoire Montavon and Sebastian Lapuschkin and Alexander Binder and Wojciech Samek and Klaus-Robert Müller},
keywords = {Deep neural networks, Heatmapping, Taylor decomposition, Relevance propagation, Image recognition},
abstract = {Nonlinear methods such as Deep Neural Networks (DNNs) are the gold standard for various challenging machine learning problems such as image recognition. Although these methods perform impressively well, they have a significant disadvantage, the lack of transparency, limiting the interpretability of the solution and thus the scope of application in practice. Especially DNNs act as black boxes due to their multilayer nonlinear structure. In this paper we introduce a novel methodology for interpreting generic multilayer neural networks by decomposing the network classification decision into contributions of its input elements. Although our focus is on image classification, the method is applicable to a broad set of input data, learning tasks and network architectures. Our method called deep Taylor decomposition efficiently utilizes the structure of the network by backpropagating the explanations from the output to the input layer. We evaluate the proposed method empirically on the MNIST and ILSVRC data sets.}
}


@misc{gradients_Saliency_Maps,
      title={Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps}, 
      author={Karen Simonyan and Andrea Vedaldi and Andrew Zisserman},
      year={2014},
      eprint={1312.6034},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{FeatureAblation, title={Efficient Parameter Importance Analysis via Ablation with Surrogates}, volume={31}, url={https://ojs.aaai.org/index.php/AAAI/article/view/10657}, DOI={10.1609/aaai.v31i1.10657}, abstractNote={ &lt;p&gt; To achieve peak performance, it is often necessary to adjust the parameters of a given algorithm to the class of problem instances to be solved; this is known to be the case for popular solvers for a broad range of AI problems, including AI planning, propositional satisfiability (SAT) and answer set programming (ASP). To avoid tedious and often highly sub-optimal manual tuning of such parameters by means of ad-hoc methods, general-purpose algorithm configuration procedures can be used to automatically find performance-optimizing parameter settings. While impressive performance gains are often achieved in this manner, additional, potentially costly parameter importance analysis is required to gain insights into what parameter changes are most responsible for those improvements. Here, we show how the running time cost of ablation analysis, a well-known general-purpose approach for assessing parameter importance, can be reduced substantially by using regression models of algorithm performance constructed from data collected during the configuration process. In our experiments, we demonstrate speed-up factors between 33 and 14 727 for ablation analysis on various configuration scenarios from AI planning, SAT, ASP and mixed integer programming (MIP). &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Biedenkapp, Andre and Lindauer, Marius and Eggensperger, Katharina and Hutter, Frank and Fawcett, Chris and Hoos, Holger}, year={2017}, month={Feb.} }

@article{FeatureAblation_first,
author = {Fawcett, Chris and Hoos, Holger},
year = {2015},
month = {01},
pages = {},
title = {Analysing differences between algorithm configurations through ablation},
volume = {22},
journal = {Journal of Heuristics},
doi = {10.1007/s10732-014-9275-9}
}

@article{SensitivityRL,
author = {Naik, Dayakar and Kiran, Ravi},
year = {2021},
month = {10},
pages = {},
title = {A Novel Sensitivity-based Method for Feature Selection},
volume = {8},
journal = {Journal of Big Data},
doi = {10.1186/s40537-021-00515-w}
}

@misc{UncertantyRL,
      title={A Review of Uncertainty for Deep Reinforcement Learning}, 
      author={Owen Lockwood and Mei Si},
      year={2022},
      eprint={2208.09052},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{tensorflow2015whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@misc{IlonMuskTwitterWebpage,   
    title = {Das bedeutet Musks Kaufplan für Twitter-Aktionäre},   
    url = {https://www.wiwo.de/finanzen/boerse/twitter-uebernahme-das-bedeutet-musks-kaufplan-fuer-twitter-aktionaere/28280056.html},   
    author = {Julia Groth},   
    year = {2022},   
    note = {Accessed on September 2023} 
}

@ARTICLE{SocialMedia_stockmarket,
  
AUTHOR={Zhang, Hua and Chen, Yuanzhu and Rong, Wei and Wang, Jun and Tan, Jinghua},   
	 
TITLE={Effect of social media rumors on stock market volatility: A case of data mining in China},      
	
JOURNAL={Frontiers in Physics},      
	
VOLUME={10},           
	
YEAR={2022},      
	  
URL={https://www.frontiersin.org/articles/10.3389/fphy.2022.987799},       
	
DOI={10.3389/fphy.2022.987799},      
	
ISSN={2296-424X},   
   
ABSTRACT={The Stock Market is a typical complex network composed of investors, stocks, and market information. The abnormal fluctuation of the Stock Market has a strong effect on the economy of a country and even that of the world. Fueled by the herd effect of the increasingly abundant social media, Internet rumors, as an important source of market information and an exogenous financial risk, can lead to the collapse of investor confidence and the further propagation of financial risks, which can damage the financial system and even lead to social unrest. With additional availability of computing techniques, we attempt to uncover the media information effects in the stock market and seek to provide researchers with 1) a theoretical reference for a comprehensive understanding of such a complex network, 2) accurate prediction of future data, and 3) design of efficient and reliable risk intervention models. Based on the data of China’s Stock Market, this study uses machine learning to investigate social media rumors to reveal the interplay of social media rumors and stock market volatility. In this work, we find patterns from social media rumors from financial forums using machine learning, quantify social media rumors based on statistics, and analyze the mechanism of propagation and influence of social media rumors on stock market volatility using econometric models. The empirical results show that rumors play an important information transmission effect on stock market volatility and the constructed Internet Financial Forum Rumor Index is helpful to sense the potential impact of rumors, i.e., a significant lagged negative effect. These findings are of guidance for the optimization of the information environment, and can serve to promote the healthy and stable development of the stock market.}
}

@misc{FMP_documentation,   
    title = {Financial Modeling Prep API Documentation},   
    url = {https://site.financialmodelingprep.com/developer/docs/},   
    author = {FMP},   
    year = {2023},   
    note = {Accessed on September 2023} 
}

@misc{FMP_Sentiment_documentation,   
    title = {Social Sentiment API},   
    url = {https://site.financialmodelingprep.com/developer/docs/social-sentiment-api/},   
    author = {FMP},   
    year = {2023},   
    note = {Accessed on September 2023} 
}

@InProceedings{RL_for_autoTrading,
author="Bertoluzzo, Francesco
and Corazza, Marco",
editor="Bassis, Simone
and Esposito, Anna
and Morabito, Francesco Carlo",
title="Reinforcement Learning for Automated Financial Trading: Basics and Applications",
booktitle="Recent Advances of Neural Network Models and Applications",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="197--213",
abstract="The construction of automated financial trading systems (FTSs) is a subject of high interest for both the academic environment and the financial one due to the potential promises by self-learning methodologies. In this paper we consider Reinforcement Learning (RL) type algorithms, that is algorithms that real-time optimize their behavior in relation to the responses they get from the environment in which they operate, without the need for a supervisor. In particular, first we introduce the essential aspects of RL which are of interest for our purposes, second we present some original automatic FTSs based on differently configured RL-based algorithms, then we apply such FTSs to artificial and real time series of daily stock prices. Finally, we compare our FTSs with a classical one based on Technical Analysis indicators. All the results we achieve are generally quite satisfactory.",
isbn="978-3-319-04129-2"
}

@book{RLforFinance,
  title={Reinforcement Learning for Finance: Solve Problems in Finance with CNN and RNN Using the TensorFlow Library},
  author={Ahlawat, Samit},
  year={2022},
  publisher={Apress, Berkeley, CA},
  edition={1},
  pages={XV, 423},
  doi={https://doi.org/10.1007/978-1-4842-8835-1},
  isbn={978-1-4842-8835-1},
  note={1 b/w illustrations, 84 illustrations in colour}
}

@Book{SuttonBarto1998,
  author =       "Sutton, Richard S. and Barto, Andrew G.",
  title =        "Reinforcement Learning: An Introduction",
  publisher =    "MIT Press",
  year =         "1998",
  ISBN =         "0-262-19398-1",
  address =   "Cambridge, MA, USA",
  url = "http://www.cs.ualberta.ca/%7Esutton/book/ebook/the-book.html",
  bib2html_rescat = "Function Approximation, Partial Observability, Learning Methods, General RL, Applications",
}

@misc{Stockstats, 
    title={Stockstats package}, 
    url={https://github.com/jealous/stockstats}, 
    journal={GitHub}, 
    author={Jealous}, 
    note = {Accessed on 17.09.2023}
} 

@misc{ADX, 
    title={Average Directional Index (ADX): Definition and Formula}, 
    url={https://www.investopedia.com/terms/a/adx.asp}, 
    journal={Investopedia.com}, 
    author={Cory Mitchell}, 
    note = {Accessed on 17.09.2023}
} 
@misc{Labbe2020, 
    title={FilterPy -- Kalman and Bayesian Filters in Python}, 
    url={https://drive.google.com/file/d/0By_SW19c1BfhSVFzNHc0SjduNzg/view?resourcekey=0-41olC9ht9xE3wQe2zHZ45A}, 
    journal={GitHub.com}, 
    author={Labbe,  Roger R. Jr}, 
    year={2020},
    note = {Accessed on 17.09.2023}
} 



@inproceedings{Welch1994,
  title={Welch \& Bishop , An Introduction to the Kalman Filter},
  author={Greg Welch and Gary Bishop},
  year={1994},
  url={https://api.semanticscholar.org/CorpusID:9209711}
}

@article{Li2022,
  doi = {10.1155/2022/4698656},
  url = {https://doi.org/10.1155/2022/4698656},
  year = {2022},
  month = mar,
  publisher = {Hindawi Limited},
  volume = {2022},
  pages = {1--15},
  author = {Yawei Li and Peipei Liu and Ze Wang},
  editor = {Cristian Mateos},
  title = {Stock Trading Strategies Based on Deep Reinforcement Learning},
  journal = {Scientific Programming}
}

@book{Hull2021,
	address = {Boston},
	author = {Hull, John C.},
	date-added = {2022-05-19 15:58:51 +0200},
	date-modified = {2022-05-19 16:00:32 +0200},
	edition = {11th},
	publisher = {Pearson},
	title = {Options, Futures, and other Derivatives},
	year = {2021}}


@book{Joshi2008,
	author = {Joshi, Mark S.},
	date-added = {2022-06-11 09:04:10 +0200},
	date-modified = {2022-06-11 09:05:33 +0200},
	edition = {2nd},
	publisher = {Cambridge University Press},
	series = {Mathematics, Finance and Risk},
	title = {The Concepts and Practice of Mathematical Finance},
	year = {2008}}

@book{Russell2021,
	author = {Russell, Stuart Jonathan AND Norvig, Peter},
	date-added = {2021-12-03 17:39:55 +0100},
	date-modified = {2022-01-02 11:17:20 +0100},
	edition = {4th},
	publisher = {Pearson},
	title = {Artificial Intelligence -- A Modern Approach},
	year = {2021}}

@misc{CCI, 
    title={What Is the Commodity Channel Index (CCI)? How To Calculate}, 
    url={https://www.investopedia.com/terms/c/commoditychannelindex.asp}, 
    journal={Investopedia.com}, 
    author={Cory Mitchell}, 
    note = {Accessed on 17.09.2023}
} 


@misc{pyfolio, 
    title={pyfolio package}, 
    url={https://github.com/quantopian/pyfolio}, 
    journal={ Quantopian Inc}, 
    author={Quantopian}, 
    note = {Accessed on 17.09.2023}
} 


@book{Garita2021,
  doi = {10.1007/978-3-030-29141-9},
  url = {https://doi.org/10.1007/978-3-030-29141-9},
  year = {2021},
  publisher = {Springer International Publishing},
  author = {Mauricio Garita},
  title = {Applied Quantitative Finance}
}






@article{Kritzman2010,
author = {Mark Kritzman and Yuanzhen Li},
title = {Skulls, Financial Turbulence, and Risk Management},
journal = {Financial Analysts Journal},
volume = {66},
number = {5},
pages = {30-41},
year  = {2010},
publisher = {Routledge},
doi = {10.2469/faj.v66.n5.3},
URL = { 
    
        https://doi.org/10.2469/faj.v66.n5.3
    
},
eprint = { 
        https://doi.org/10.2469/faj.v66.n5.3
}

}



@book{Prado2018,
author = {de Prado, Marcos Lopez},
title = {Advances in Financial Machine Learning},
year = {2018},
isbn = {1119482089},
publisher = {Wiley Publishing},
edition = {1st},
abstract = {Machine learning (ML) is changing virtually every aspect of our lives. Today ML algorithms accomplish tasks that until recently only expert humans could perform. As it relates to finance, this is the most exciting time to adopt a disruptive technology that will transform how everyone invests for generations. Readers will learn how to structure Big data in a way that is amenable to ML algorithms; how to conduct research with ML algorithms on that data; how to use supercomputing methods; how to backtest your discoveries while avoiding false positives. The book addresses real-life problems faced by practitioners on a daily basis, and explains scientifically sound solutions using math, supported by code and examples. Readers become active users who can test the proposed solutions in their particular setting. Written by a recognized expert and portfolio manager, this book will equip investment professionals with the groundbreaking tools needed to succeed in modern finance.}
}

@article{EnsembleStategy,
  title={Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy},
  author={Yang, Hongyang and Liu, Xiao-Yang and Zhong, Shan and Walid, Anwar},
  year={2020},
  month={Sep},
  note={Available at SSRN: \url{https://ssrn.com/abstract=3690996} or \url{http://dx.doi.org/10.2139/ssrn.3690996}}
}


@misc{practicalDRL,
      title={Practical Deep Reinforcement Learning Approach for Stock Trading}, 
      author={Xiao-Yang Liu and Zhuoran Xiong and Shan Zhong and Hongyang Yang and Anwar Walid},
      year={2022},
      eprint={1811.07522},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@INPROCEEDINGS{AutoStockTrain,
  author={Chen, Lin and Gao, Qiang},
  booktitle={2019 IEEE 10th International Conference on Software Engineering and Service Science (ICSESS)}, 
  title={Application of Deep Reinforcement Learning on Automated Stock Trading}, 
  year={2019},
  volume={},
  number={},
  pages={29-33},
  doi={10.1109/ICSESS47205.2019.9040728}}

@online{alphavantage,
    author    = "Alpha Vantage",
    title     = "Alpha Vantage - Free APIs for Realtime \& Historical Stock, Forex (FX), Cryptocurrency Data, Technical Analysis, Charting and More!",
    year      = 2023,
    url       = "https://www.alphavantage.co"
}

@Article{Shen2020,
author={Shen, Jingyi
and Shafiq, M. Omair},
title={Short-term stock market price trend prediction using a comprehensive deep learning system},
journal={Journal of Big Data},
year={2020},
month={Aug},
day={28},
volume={7},
number={1},
pages={66},
abstract={In the era of big data, deep learning for predicting stock market prices and trends has become even more popular than before. We collected 2 years of data from Chinese stock market and proposed a comprehensive customization of feature engineering and deep learning-based model for predicting price trend of stock markets. The proposed solution is comprehensive as it includes pre-processing of the stock market dataset, utilization of multiple feature engineering techniques, combined with a customized deep learning based system for stock market price trend prediction. We conducted comprehensive evaluations on frequently used machine learning models and conclude that our proposed solution outperforms due to the comprehensive feature engineering that we built. The system achieves overall high accuracy for stock market trend prediction. With the detailed design and evaluation of prediction term lengths, feature engineering, and data pre-processing methods, this work contributes to the stock analysis research community both in the financial and technical domains.},
issn={2196-1115},
doi={10.1186/s40537-020-00333-6},
url={https://doi.org/10.1186/s40537-020-00333-6}
}

@article{VERMA2007231,
title = {Noise trading and stock market volatility},
journal = {Journal of Multinational Financial Management},
volume = {17},
number = {3},
pages = {231-243},
year = {2007},
issn = {1042-444X},
doi = {https://doi.org/10.1016/j.mulfin.2006.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S1042444X06000806},
author = {Rahul Verma and Priti Verma},
keywords = {Volatility, Investor sentiment},
abstract = {We investigate the relative effects of fundamental and noise trading on the formation of conditional volatility. We find significant positive (negative) effects of investor sentiments on stock returns (volatilities) for both individual and institutional investors. There are greater positive effects of rational sentiments on stock returns than irrational sentiments. Conversely, there are significant (insignificant) negative effects of irrational (rational) sentiments on volatility. Also, we find asymmetric (symmetric) spillover effects of irrational (rational) bullish and bearish sentiments on the stock market. Evidence in favor of irrational sentiments is consistent with the view that investor error is a significant determinant of stock volatilities.}
}

@article{doi:10.1198/073500106000000071,
author = {Peter R Hansen and Asger Lunde},
title = {Realized Variance and Market Microstructure Noise},
journal = {Journal of Business \& Economic Statistics},
volume = {24},
number = {2},
pages = {127-161},
year  = {2006},
publisher = {Taylor & Francis},
doi = {10.1198/073500106000000071},
URL = { 
        https://doi.org/10.1198/073500106000000071
},
eprint = { 
        https://doi.org/10.1198/073500106000000071
}
}

@misc{mnih2016asynchronous,
      title={Asynchronous Methods for Deep Reinforcement Learning}, 
      author={Volodymyr Mnih and Adrià Puigdomènech Badia and Mehdi Mirza and Alex Graves and Timothy P. Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
      year={2016},
      eprint={1602.01783},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{schulman2017proximal,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{haarnoja2018soft,
      title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}, 
      author={Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine},
      year={2018},
      eprint={1801.01290},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{fujimoto2018addressing,
      title={Addressing Function Approximation Error in Actor-Critic Methods}, 
      author={Scott Fujimoto and Herke van Hoof and David Meger},
      year={2018},
      eprint={1802.09477},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{lillicrap2019continuous,
      title={Continuous control with deep reinforcement learning}, 
      author={Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
      year={2019},
      eprint={1509.02971},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{stable-baselines3,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {268},
  pages   = {1-8},
  url     = {http://jmlr.org/papers/v22/20-1364.html}
}